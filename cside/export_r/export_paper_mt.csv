#,key,Title,Link,Abstract,Preview
1,paper_1,"Mohammed N.M., Niazi M., Alshayeb M., Mahmood S.",,"There is an increase use of security driven approaches to support software development activities, such as requirements, design and implementation. The objective of this paper is to identify the existing software security approaches used in the software development lifecycle (SDLC). In order to meet our goal, we conducted a systematic mapping study to identify the primary studies on the use of software security techniques in SDLC. In total, we selected and categorized 118 primary studies. After analyzing the selected studies, we identified 52 security approaches and we categorized them in to five main categories, namely, secure requirements modeling, vulnerability identification, adaption and mitigation, software security focused process, extended UML-based secure modeling profiles, non UML-based secure modeling notations. The results show that the most frequently used approaches are static analysis and dynamic analysis that provide security checks in the coding phase. In addition, our results show that many studies in this review considered security checks around the coding stage of software development. This work will assist software development organizations in better understanding the existing software security approaches used in the software development lifecycle. It can also provide researchers with a firm basis on which to develop new software security approaches. © 2016 Elsevier B.V.","<b>Authors:</b><br/>Mohammed N.M., Niazi M., Alshayeb M., Mahmood S. <br/><b>Key words:</b><br/>Empirical study, Software development life cycle, Software security, Systematic mapping study"
2,paper_2,"Reddivari S., Asaithambi A., Niu N., Wang W., Xu L.D., Cheng J.-R.C.",,"The requirements engineering (RE) processes have become a key in developing and deploying enterprise information system (EIS) for organisations and corporations in various fields and industrial sectors. Ethnography is a contextual method allowing scientific description of the stakeholders, their needs and their organisational customs. Despite the recognition in the RE literature that ethnography could be helpful, the actual leverage of the method has been limited and ad hoc. To overcome the problems, we report in this paper a systematic mapping study where the relevant literature is examined. Building on the literature review, we further identify key parameters, their variations and their connections. The improved understanding about the role of ethnography in EIS RE is then presented in a consolidated model, and the guidelines of how to apply ethnography are organised by the key factors uncovered. Our study can direct researchers towards thorough understanding about the role that ethnography plays in EIS RE, and more importantly, to help practitioners better integrate contextually rich and ecologically valid methods in their daily practices. © 2015 Informa UK Limited, trading as Taylor & Francis Group.","<b>Authors:</b><br/>Reddivari S., Asaithambi A., Niu N., Wang W., Xu L.D., Cheng J.-R.C. <br/><b>Key words:</b><br/>contextual methods, enterprise information systems, ethnography, requirements engineering"
3,paper_3,"Ahmad A., Babar M.A.",,"Context Several research efforts have been targeted to support architecture centric development and evolution of software for robotic systems for the last two decades. Objective We aimed to systematically identify and classify the existing solutions, research progress and directions that influence architecture-driven modeling, development and evolution of robotic software. Research Method We have used Systematic Mapping Study (SMS) method for identifying and analyzing 56 peer-reviewed papers. Our review has (i) taxonomically classified the existing research and (ii) systematically mapped the solutions, frameworks, notations and evaluation methods to highlight the role of software architecture in robotic systems. Results and Conclusions We have identified eight themes that support architectural solutions to enable (i) operations, (ii) evolution and (iii) development specific activities of robotic software. The research in this area has progressed from object-oriented to component-based and now to service-driven robotics representing different architectural models that emerged overtime. An emerging solution is cloud robotics that exploits the foundations of service-driven architectures to support an interconnected web of robots. The results of this SMS facilitate knowledge transfer  benefiting researchers and practitioners  focused on exploiting software architecture to model, develop and evolve robotic systems. © 2016 Elsevier Inc.","<b>Authors:</b><br/>Ahmad A., Babar M.A. <br/><b>Key words:</b><br/>Evidence-based software engineering, Robotic systems, Software architecture, Software architecture for robotics, Systematic mapping study"
4,paper_4,"Montalvillo L., Díaz O.",,"CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products). CONCLUSION. Analyses of the results indicate that Solution proposals are the most common type of contribution (31%). Regarding the evolution activity, Implement change (43%) and Analyze and plan change (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included. © 2016 Elsevier Inc.","<b>Authors:</b><br/>Montalvillo L., Díaz O. <br/><b>Key words:</b><br/>Evolution, Software product lines, Systematic mapping study"
5,paper_5,"Garousi V., Mäntylä M.V.",,"Context Any newcomer or industrial practitioner is likely to experience difficulties in digesting large volumes of knowledge in software testing. In an ideal world, all knowledge used in industry, education and research should be based on high-quality evidence. Since no decision should be made based on a single study, secondary studies become essential in presenting the evidence. According to our search, over 101 secondary studies have been published in the area of software testing since 1994. With this high number of secondary studies, it is important to conduct a review in this area to provide an overview of the research landscape in this area. Objective The goal of this study is to systematically map (classify) the secondary studies in software testing. We propose that tertiary studies can serve as summarizing indexes which facilitate finding the most relevant information from secondary studies and thus supporting evidence-based decision making in any given area of software engineering. Our research questions (RQs) investigate: (1) Software-testing-specific areas, (2) Types of RQs investigated, (3) Numbers and Trends, and (4) Citations of the secondary studies. Method To conduct the tertiary study, we use the systematic-mapping approach. Additionally, we contrast the testing topics to the number of Google hits to address a general popularity of a testing topic and study the most popular papers in terms of citations. We furthermore demonstrate the practicality and usefulness of our results by mapping them to ISTQB foundation syllabus and to SWEBOK to provide implications for practitioners, testing educators, and researchers. Results After a systematic search and voting process, our study pool included 101 secondary studies in the area of software testing between 1994 and 2015. Among our results are the following: (1) In terms of number of secondary studies, model-based approach is the most popular testing method, web services are the most popular system under test (SUT), while regression testing is the most popular testing phase, (2) The quality of secondary studies, as measured by a criteria set established in the community, is slowly increasing as the years go by, and (3) Analysis of research questions, raised and studied in the pool of secondary studies, showed that there is a lack of causality and relationship type of research questions, a situation which needs to be improved if we, as a community, want to advance as a scientific field. (4) Among secondary studies, we found that regular surveys receive significantly more citations than SMs (p = 0.009) and SLRs (p = 0.014). Conclusion Despite the large number of secondary studies, we found that many important areas of software testing currently lack secondary studies, e.g., test management, role of product risk in testing, human factors in software testing, beta-testing (A/B-testing), exploratory testing, testability, test stopping criteria, and test-environment development. Having secondary studies in those areas is important for satisfying industrial and educational needs in software testing. On the other hand, education material of ISTQB foundation syllabus and SWEBOK could benefit from the inclusion of the latest research topics, namely search-based testing, use of cloud-computing for testing and symbolic execution. © 2016 Elsevier B.V.","<b>Authors:</b><br/>Garousi V., Mäntylä M.V. <br/><b>Key words:</b><br/>Secondary studies, Software testing, Surveys, Systematic literature reviews, Systematic mapping, Tertiary study"
6,paper_6,"Keszei A.P., Berkels B., Deserno T.M.",,"We catalogue available software solutions for non-rigid image registration to support scientists in selecting suitable tools for specific medical registration purposes. Registration tools were identified using non-systematic search in Pubmed, Web of Science, IEEE Xplore® Digital Library, Google Scholar, and through references in identified sources (n = 22). Exclusions are due to unavailability or inappropriateness. The remaining (n = 18) tools were classified by (i) access and technology, (ii) interfaces and application, (iii) living community, (iv) supported file formats, and (v) types of registration methodologies emphasizing the similarity measures implemented. Out of the 18 tools, (i) 12 are open source, 8 are released under a permissive free license, which imposes the least restrictions on the use and further development of the tool, 8 provide graphical processing unit (GPU) support, (ii) 7 are built on software platforms, 5 were developed for brain image registration, (iii) 6 are under active development but only 3 have had their last update in 2015 or 2016, (iv) 16 support the Analyze format, while 7 file formats can be read with only one of the tools, and (v) 6 provide multiple registration methods and 6 provide landmark-based registration methods. Based on open source, licensing, GPU support, active community, several file formats, algorithms, and similarity measures, the tools Elastics and Plastimatch are chosen for the platform ITK and without platform requirements, respectively. Researchers in medical image analysis already have a large choice of registration tools freely available. However, the most recently published algorithms may not be included in the tools, yet. © 2016 Society for Imaging Informatics in Medicine","<b>Authors:</b><br/>Keszei A.P., Berkels B., Deserno T.M. <br/><b>Key words:</b><br/>Image alignment, Image analysis, Image registration, Open-source software, Public domain software, Software tool"
7,paper_7,"Saint-Louis P., Lapalme J.",,"The number of publications, along with the organization of new conferences are a couple of the relevant elements that usually indicate the progress of an area of study over the years. This is definitely true in the case of the Enterprise Architecture (EA) discipline, which went from having its first journal article published in 1989 to over two hundred published articles by 2015. But in spite of this evolution, EA is still suffering from a considerable lack of common understanding. It has become very important to investigate the current state of affairs concerning the EA discipline through its relevant publications in order to shed some light on this challenge. 171 journal papers published between 1990 and 2015 were systematically selected and examined in order to accomplish this investigation. The quantitative and qualitative findings of this examination show that EA is a young discipline which raises a growing interest in recent years. This examination also confirms the lack of common understanding in EA, which can be observed in the different descriptions of the term «enterprise architecture,» and in the diversity of perspective with regards to the whole discipline. Several issues related to this lack has been reported, such as multidisciplinary issue, language issue, structure of research and mode of observation issues. The major issue concerns the absence of enough research to shed some light on this challenge. In addition to this investigation, helpful directions for future research in this area was proposed. © 2016 IEEE.","<b>Authors:</b><br/>Saint-Louis P., Lapalme J. <br/><b>Key words:</b><br/>agreed definition, common terminology, common understanding, Enterprise architecture, fragmented literature, literature, schools of thought, shared meaning, state of the art, systematic mapping study"
8,paper_8,"Wohlin C.",,"Empirical studies of different kinds are nowadays regularly published in software engineering journals and conferences. Many empirical studies have been published, but are this sufficient? Individual studies are important, b ut the actual potential in relation to evidence-based software engineering [1] is not fully exploited. As a discipline we have to be able to go further to make our individual studies more useful. Other research should be able to leverage on the studies and industry should be able to make informed decisions based on the empirical research. There are several challenges related to making individual empirical studies useful in a broader context. Anyone having conducted a systematic literature review [2] has most likely experienced the problem of being able to synthesize the relevant studies. In all too many cases, we end up with a systematic mapping study [3], or in the best case something on the borderline between a review and a mapping study. This illustrates the need to write for synthesis [4], and in particular including sufficient contextual information to allow for synthesis [4]. Evidence-based software engineering [1] through the use of systematic literature studies (reviews and maps) has emerged. Methodological support and guidelines (e.g. [2], [3], [6] and [7]) for conducting systematic literature studies have been formulated and they should be carefully followed. However, more is needed! We still need to improve! The keynote is focused on the needs for the future as seen by the presenter. Synthesis has proven hard, and improvements are needed when it comes to both primary studies and secondary studies. It has been shown that the reliability of secondary studies can be challenged [8]. However, if we do manage to publish high quality primary studies, and we truly manage to conduct strong systematic literature reviews, we have a good basis for both building theories in software engineering and to enable industry to make informed decisions using scientific evidence. Unfortunately, this is not the situation today. Theories are mostly based on our own research, as exemplified by [9]. This is fine, but much more can be done if we can easier leverage on the research done by others to build theories. Furthermore, industry is often making decision related to processes, methods, techniques and tools before we manage to obtain sufficient evidence for recommendations. The points made above are highlighted using personal experiences from conducting systematic literature studies, collaborating with industry and research on developing an empirically based software engineering theory.","<b>Authors:</b><br/>Wohlin C. <br/><b>Key words:</b><br/>Empirical research methods, synthesis, systematic reviews"
9,paper_9,"Goulão M., Amaral V., Mernik M.",,"Model-driven engineering (MDE) is believed to have a significant impact in software quality. However, researchers and practitioners may have a hard time locating consolidated evidence on this impact, as the available information is scattered in several different publications. Our goal is to aggregate consolidated findings on quality in MDE, facilitating the work of researchers and practitioners in learning about the coverage and main findings of existing work as well as identifying relatively unexplored niches of research that need further attention. We performed a tertiary study on quality in MDE, in order to gain a better understanding of its most prominent findings and existing challenges, as reported in the literature. We identified 22 systematic literature reviews and mapping studies and the most relevant quality attributes addressed by each of those studies, in the context of MDE. Maintainability is clearly the most often studied and reported quality attribute impacted by MDE. Eighty out of 83 research questions in the selected secondary studies have a structure that is more often associated with mapping existing research than with answering more concrete research questions (e.g., comparing two alternative MDE approaches with respect to their impact on a specific quality attribute). We briefly outline the main contributions of each of the selected literature reviews. In the collected studies, we observed a broad coverage of software product quality, although frequently accompanied by notes on how much more empirical research is needed to further validate existing claims. Relatively, little attention seems to be devoted to the impact of MDE on the quality in use of products developed using MDE. © 2016, Springer Science+Business Media New York.","<b>Authors:</b><br/>Goulão M., Amaral V., Mernik M. <br/><b>Key words:</b><br/>Model-driven engineering, Quality, Tertiary study"
10,paper_10,"Garousi V., Mäntylä M.V.",,"Context Many organizations see software test automation as a solution to decrease testing costs and to reduce cycle time in software development. However, establishment of automated testing may fail if test automation is not applied in the right time, right context and with the appropriate approach. Objective The decisions on when and what to automate is important since wrong decisions can lead to disappointments and major wrong expenditures (resources and efforts). To support decision making on when and what to automate, researchers and practitioners have proposed various guidelines, heuristics and factors since the early days of test automation technologies. As the number of such sources has increased, it is important to systematically categorize the current state-of-The-art and -practice, and to provide a synthesized overview. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study on when and what to automate in software testing. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our MLR and its results are based on 78 sources, 52 of which were grey literature and 26 were formally published sources. We used the qualitative analysis (coding) to classify the factors affecting the when- And what-to-automate questions to five groups: (1) Software Under Test (SUT)-related factors, (2) test-related factors, (3) test-tool-related factors, (4) human and organizational factors, and (5) cross-cutting and other factors. The most frequent individual factors were: need for regression testing (44 sources), economic factors (43), and maturity of SUT (39). Conclusion We show that current decision-support in software test automation provides reasonable advice for industry, and as a practical outcome of this research we have summarized it as a checklist that can be used by practitioners. However, we recommend developing systematic empirically-validated decision-support approaches as the existing advice is often unsystematic and based on weak empirical evidence. © 2016 Elsevier B.V.","<b>Authors:</b><br/>Garousi V., Mäntylä M.V. <br/><b>Key words:</b><br/>Decision support, Multivocal literature review, Software test automation, Systematic literature review, Systematic Mapping study, What to automate, When to automate"
11,paper_11,"Ambreen T., Ikram N., Usman M., Niazi M.",,"Requirements engineering (RE) being a foundation of software development has gained a great recognition in the recent era of prevailing software industry. A number of journals and conferences have published a great amount of RE research in terms of various tools, techniques, methods, and frameworks, with a variety of processes applicable in different software development domains. The plethora of empirical RE research needs to be synthesized to identify trends and future research directions. To represent a state-of-the-art of requirements engineering, along with various trends and opportunities of empirical RE research, we conducted a systematic mapping study to synthesize the empirical work done in RE. We used four major databases IEEE, ScienceDirect, SpringerLink and ACM and Identified 270 primary studies till the year 2012. An analysis of the data extracted from primary studies shows that the empirical research work in RE is on the increase since the year 2000. The requirements elicitation with 22 % of the total studies, requirements analysis with 19 % and RE process with 17 % are the major focus areas of empirical RE research. Non-functional requirements were found to be the most researched emerging area. The empirical work in the sub-area of requirements validation and verification is little and has a decreasing trend. The majority of the studies (50 %) used a case study research method followed by experiments (28 %), whereas the experience reports are few (6 %). A common trend in almost all RE sub-areas is about proposing new interventions. The leading intervention types are guidelines, techniques and processes. The interest in RE empirical research is on the rise as whole. However, requirements validation and verification area, despite its recognized importance, lacks empirical research at present. Furthermore, requirements evolution and privacy requirements also have little empirical research. These RE sub-areas need the attention of researchers for more empirical research. At present, the focus of empirical RE research is more about proposing new interventions. In future, there is a need to replicate existing studies as well to evaluate the RE interventions in more real contexts and scenarios. The practitioners involvement in RE empirical research needs to be increased so that they share their experiences of using different RE interventions and also inform us about the current requirements-related challenges and issues that they face in their work. © 2016 Springer-Verlag London","<b>Authors:</b><br/>Ambreen T., Ikram N., Usman M., Niazi M. <br/><b>Key words:</b><br/>Evidence-based software engineering, Mapping study, Requirements engineering, Systematic review"
12,paper_12,"Galster M., Weyns D.",,"Context: Empirical research helps gain well-founded insights about phenomena. Furthermore, empirical research creates evidence for the validity of research results. Objective: We aim at assessing the state-of-practice of empirical research in software architecture. Method: We conducted a comprehensive survey based on the systematic mapping method. We included all full technical research papers published at major software architecture conferences between 1999 and 2015. Results: 17% of papers report empirical work. The number of empirical studies in software architecture has started to increase in 2005. Looking at the number of papers, empirical studies are about equally frequently used to a) evaluate newly proposed approaches and b) to explore and describe phenomena to better understand software architecture practice. Case studies and experiments are the most frequently used empirical methods. Almost half of empirical studies involve human participants. The majority of these studies involve professionals rather than students. Conclusions: Our findings are meant to stimulate researchers in the community to think about their expectations and standards of empirical research. Our results indicate that software architecture has become a more mature domain with regards to applying empirical research. However, we also found issues in research practices that could be improved (e.g., when describing study objectives and acknowledging limitations). © 2016 IEEE.","<b>Authors:</b><br/>Galster M., Weyns D. <br/><b>Key words:</b><br/>empirical research, software architecture, state-of-practice"
13,paper_13,"Leal F., Musicante M.A.",,"Cloud computing became a reality, and many companies are now moving their data-centers to the cloud. A concept that is often linked with cloud computing is Infrastructure as a Service (IaaS): the computational infrastructure of a company can now be seen as a monthly cost instead of a number of different factors. Recently, a large number of organizations started to replace their relational databases with hybrid solutions (NoSQL DBs, Search Engines, ORDBs). These changes are motivated by (i) performance improvements on the overall performance of the applications and (ii) inability to a RDBMS to provide the same performance of a hybrid solution given a fixed monthly infrastructure cost. However, not always the companies can exactly measure beforehand the future impact on the performance on their services by making this sort of technological changes (replace RDBMS by another solution). The goal of this systematic mapping study is to investigate the use of Service-Level-Agreements (SLAs) on database-transitioning scenarios and to verify how SLAs can be used in this processes. © 2015 IEEE.","<b>Authors:</b><br/>Leal F., Musicante M.A. <br/><b>Key words:</b><br/>NoSQL, Systematic Mapping, Transition to Cloud"
14,paper_14,"Carvalho R.M., de Castro Andrade R.M., de Oliveira K.M., de Sousa Santos I., Bezerra C.I.M.",,"The advent of ubiquitous systems places even more focus on users, since these systems must support their daily activities in such a transparent way that does not disturb them. Thus, much more attention should be provided to humancomputer interaction (HCI) and, as a consequence, to its quality. Dealing with quality issues implies first the identification of the quality characteristics that should be achieved and, then, which software measures should be used to evaluate them in a target system. Therefore, this work aims to identify what quality characteristics and measures have been used for the HCI evaluation of ubiquitous systems. In order to achieve our goal, we performed a large literature review, using a systematic mapping study, and we present our results in this paper. We identified 41 pertinent papers that were deeply analyzed to extract quality characteristics and software measures. We found 186 quality characteristics, but since there were divergences on their definitions and duplicated characteristics, an analysis of synonyms by peer review based on the equivalence of definitions was also done. This analysis allowed us to define a final suitable set composed of 27 quality characteristics, where 21 are generic to any system but are particularized for ubiquitous applications and 6 are specific for this domain. We also found 218 citations of measures associated with the characteristics, although the majority of them are simple definitions with no detail about their measurement functions. Our results provide not only an overview of this area to guide researchers in directing their efforts but also it can help practitioners in evaluating ubiquitous systems using these measures. © 2016 Springer Science+Business Media New York","<b>Authors:</b><br/>Carvalho R.M., de Castro Andrade R.M., de Oliveira K.M., de Sousa Santos I., Bezerra C.I.M. <br/><b>Key words:</b><br/>Humancomputer interaction, Quality characteristics, Quality model, Software measures, Systematic mapping study, Ubiquitous systems"
15,paper_15,"Parreira P.A., Penteado R.A.D.",,"Context: obtaining accurate information about the problem domain for which the software is being developed is one of most important and difficult activity of the Requirements Engineering (RE). One of the strategies to deal with problem domain concepts and their relationships is the usage of domain ontologies. However, there are not studies in the literature that describe which ontology-based approaches exist and how they have been applied in the RE context. Goal: this paper aims to identify, understand and catalog existing primary studies in the literature regarding to this subject. Method: for this, a Systematic Mapping was conducted and sixty-seven studies were analyzed and cataloged. Some results are: (i) there are five main ways of using ontologies in the context of RE and they are related to 'Requirements Elicitation', 'Requirements Analysis', 'Requirements Verification', 'Conflict Identification and Analysis' and 'Unification among requirements formalisms', (ii) there are few evaluation studies about this subject, among others. © 2015 IEEE.","<b>Authors:</b><br/>Parreira P.A., Penteado R.A.D. <br/><b>Key words:</b><br/>Domain Ontologies, Literature Review, Requirements Engineering, Software Engineering, Systematic Mapping"
16,paper_16,"Zein S., Salleh N., Grundy J.",,"The importance of mobile application specific testing techniques and methods has been attracting much attention of software engineers over the past few years. This is due to the fact that mobile applications are different than traditional web and desktop applications, and more and more they are moving to being used in critical domains. Mobile applications require a different approach to application quality and dependability and require an effective testing approach to build high quality and more reliable software. We performed a systematic mapping study to categorize and to structure the research evidence that has been published in the area of mobile application testing techniques and challenges that they have reported. Seventy nine (79) empirical studies are mapped to a classification schema. Several research gaps are identified and specific key testing issues for practitioners are identified: there is a need for eliciting testing requirements early during development process, the need to conduct research in real-world development environments, specific testing techniques targeting application life-cycle conformance and mobile services testing, and comparative studies for security and usability testing. © 2016 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Zein S., Salleh N., Grundy J. <br/><b>Key words:</b><br/>Mobile application testing, Software testing, Systematic mapping"
17,paper_17,"García S., Romero O., Raventós R.",,"Decision support systems (DSS) provide a unified analytical view of business data to better support decision-making processes. Such systems have shown a high level of user satisfaction and return on investment. However, several surveys stress the high failure rate of DSS projects. This problem results from setting the wrong requirements by approaching DSS in the same way as operational systems, whereas a specific approach is needed. Although this is well-known, there is still a surprising gap on how to address requirements engineering (RE) for DSS. To overcome this problem, we conducted a systematic mapping study to identify and classify the literature on DSS from an RE perspective. Twenty-seven primary studies that addressed the main stages of RE were selected, mapped, and classified into 39 models, 27 techniques, and 54 items of guidance. We have also identified a gap in the literature on how to design the DSS main constructs (typically, the data warehouse and data flows) in a methodological manner from the business needs. We believe this study will help practitioners better address the RE stages of DSS projects. © 2016 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>García S., Romero O., Raventós R. <br/><b>Key words:</b><br/>Business intelligence, Decision support systems, Requirements engineering"
18,paper_18,"Asaadi E., Wilke D.N., Heyns P.S., Kok S.",,"Material parameter identification is a technique that is used to calibrate material models, often a precursor to perform an industrial analysis. Conventional material parameter identification methods estimate the material parameters for a material model by solving an optimisation problem. An alternative but lesser-known approach, called a direct inverse map, directly maps the measured response to the parameters of a material model. In this study we investigate the potential pitfalls of the well-known stochastic noise and lesser-known model errors when constructing direct inverse maps. We show how to address these problems, explaining in particular the importance of projecting the measured response onto the domain of the simulated responses before mapping it to the material parameters. This paper concludes by proposing partial least squares regression as an elegant and computationally efficient approach to address stochastic and systematic (model) errors. This paper also gives insight into the nature of the inverse problem under consideration. © 2016 Springer-Verlag Berlin Heidelberg","<b>Authors:</b><br/>Asaadi E., Wilke D.N., Heyns P.S., Kok S. <br/><b>Key words:</b><br/>Inverse mapping, Inverse problem, Material parameter identification, Partial least squares regression, Principal component analysis, Radial basis function approximation"
19,paper_19,"Rahman M.M.",,"When images are described with visual words based on vector quantization of low-level color, texture, and edge-related visual features of image regions, it is usually referred as bag-of-visual words (BoVW)-based presentation. Although it has proved to be effective for image representation similar to document representation in text retrieval, the hard image encoding approach based on one-to-one mapping of regions to visual words is not expressive enough to characterize the image contents with higher level semantics and prone to quantization error. Each word is considered independent of all the words in this model. However, it is found that the words are related and their similarity of occurrence in documents can reflect the underlying semantic relations between them. To consider this, a soft image representation scheme is proposed by spreading each regions membership values through a local fuzzy membership function in a neighborhood to all the words in a codebook generated by self-organizing map (SOM). The topology preserving property of the SOM map is exploited to generate a local membership function. A systematic evaluation of retrieval results of the proposed soft representation on two different image (natural photographic and medical) collections has shown significant improvement in precision at different recall levels when compared to different low-level and BoVW-based feature that consider only probability of occurrence (or presence/absence) of a word. © 2015, Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Rahman M.M. <br/><b>Key words:</b><br/>Clustering, Content-based image retrieval, Fuzzy system, Self-organizing map, Soft image annotation"
20,paper_20,"Nguyen T., Colman A., Han J.",,"A customizable web service is a service that enables service consumers to dynamically determine variants of the service they receive. Provisioning customizable services helps to efficiently address functional variability in customer requirements. However, this is challenging due to: i) the complexity in deriving the right subset of service capabilities for a service variant and ii) the existence of a large number of variants and their dependencies. We propose a feature-based framework to tackle this challenge. In our framework, a feature model is used to capture functional variability in customer requirements at a high-level of abstraction and to provide customers with a much simpler way to customize an atomic service. A service engineering process is designed to facilitate the systematic identification and implementation of variability during service development, and to maintain the mapping between variabilities at the feature modeling level and the service implementation level. We define a generative middleware that supports service deployment and exploits the mapping to enable runtime service customization. A large scale case study based on the Amazon web services is used for evaluation. In addition to addressing the challenge in provisioning customizable services, our experiments show that the generative middleware helps to reduce runtime resource consumption. © 2008-2012 IEEE.","<b>Authors:</b><br/>Nguyen T., Colman A., Han J. <br/><b>Key words:</b><br/>Feature modeling, Service customization, Service middleware, Web services"
21,paper_21,"Nascimento A.M., Silveira D.S.D.",,"Studies involving social media have focused on analyzing how companies can propagate information or how the social media tools can affect user behavior. Few studies have paid attention to User-Generated Content and its application to the improvement of business processes, even though there has been an increase in reported cases in recent years. Through a systematic mapping study of the Ebsco ® database, we looked for articles that addressed this issue of user-generated content since its first appearance of up to 2015. In a first view, we confirmed the paucity of research related to this theme, even with the large increase of social media studies. As findings, we identified some important research trends towards public organizations using the content created by citizens to improve public services, as well as companies using social media tools to improve innovation processes. On the other hand, we identified gaps in the research, such as in journalism, engaging readers as information providers, and social media, being used for the improvement of the e-learning process. © 2016 Elsevier Ltd.","<b>Authors:</b><br/>Nascimento A.M., Silveira D.S.D. <br/><b>Key words:</b><br/>Business process improvement, Social media, Systematic mapping study, User-created content, User-generated content"
22,paper_22,"Jia C., Cai Y., Yu Y.T., Tse T.H.",,"A common type of study used by researchers to map out the landscape of a research topic is known as mapping study. Such a study typically begins with an exploratory search on the possible ideas of the research topic, which is often done in an unsystematic manner. Hence, the activity of formulating research questions in mapping studies is ill-defined, rendering it difficult for researchers who are new to the topic. There is a need to guide them kicking off a mapping study of an unfamiliar domain. This paper proposes a 5W+1H pattern to help investigators systematically examine a generic set of dimensions in a mapping study toward the formulation of research questions before identifying, reading, and analyzing sufficient articles of the topic. We have validated the feasibility of our proposal by conducting a case study of a mapping study on cloud software testing, that is, software testing for and on cloud computing platforms. The case study reveals that the 5W+1H pattern can lead investigators to define a set of systematic, generic, and complementary research questions, enabling them to kick off and expedite the mapping study process in a well-defined manner. We also share our experiences and lessons learned from our case study on the use of the 5W+1H pattern in mapping studies. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Jia C., Cai Y., Yu Y.T., Tse T.H. <br/><b>Key words:</b><br/>5W+1H pattern, Cloud software testing, Systematic mapping study"
23,paper_23,"Wnuk K., Kollu R.K.",,"Context: Requirements scoping is one of the key activities in requirements management but also a major risk for project management. Continuously changing scope may create a congestion state in handling the requirements inflow which causes negative consequences, e.g. delays or scope creep. Objectives: In this paper, we look at requirements scoping literature outside Software Product Line (SPL) by exploring the current literature on the phenomenon, summarizing publication trends, performing thematic analysis and analyzing the strength of the evidence in the light of rigor and relevance assessment. Method: We run a Systematic Mapping Study (SMS) using snowballing procedure, supported by a database search for the start set identification, and identified 21 primary studies and 2 secondary studies. Results: The research interest in this area steadily increases and includes mainly case studies, validation or evaluation studies. The results were categorized into four themes: definitions, negative effects associated with scoping, challenges and identified methods/tools. The identified scope management techniques are also matched against the identified requirements scoping challenges. © 2016 ACM.","<b>Authors:</b><br/>Wnuk K., Kollu R.K. <br/><b>Key words:</b><br/>Requirements scoping, Snowballing, Systematic mapping study"
24,paper_24,"Souag A., Mazo R., Salinesi C., Comyn-Wattiau I.",,"Security is a concern that must be taken into consideration starting from the early stages of system development. Over the last two decades, researchers and engineers have developed a considerable number of methods for security requirements engineering. Some of them rely on the (re)use of security knowledge. Despite some existing surveys about security requirements engineering, there is not yet any reference for researchers and practitioners that presents in a systematic way the existing proposals, techniques, and tools related to security knowledge reuse in security requirements engineering. The aim of this paper is to fill this gap by looking into drawing a picture of the literature on knowledge and reuse in security requirements engineering. The questions we address are related to methods, techniques, modeling frameworks, and tools for and by reuse in security requirements engineering. We address these questions through a systematic mapping study. The mapping study was a literature review conducted with the goal of identifying, analyzing, and categorizing state-of-the-art research on our topic. This mapping study analyzes more than thirty approaches, covering 20 years of research in security requirements engineering. The contributions can be summarized as follows: (1) A framework was defined for analyzing and comparing the different proposals as well as categorizing future contributions related to knowledge reuse and security requirements engineering, (2) the different forms of knowledge representation and reuse were identified, and (3) previous surveys were updated. We conclude that most methods should introduce more reusable knowledge to manage security requirements. © 2015, Springer-Verlag London.","<b>Authors:</b><br/>Souag A., Mazo R., Salinesi C., Comyn-Wattiau I. <br/><b>Key words:</b><br/>Knowledge, Ontologies, Patterns, Reusability, Security requirements, Templates"
25,paper_25,"Balaid A., Abd Rozan M.Z., Hikmi S.N., Memon J.",,"Context: Nowadays the concept of knowledge mapping has attracted increased attention from scientists in a variety of academic disciplines and professional practice areas. Among the most important attributes of a knowledge map is its ability to increase communication and share common practices across an entire organisation. However, despite being a promising area for research, the knowledge maps community lacks a widespread understanding of the current state of the art. Objective: The objective of this article is to explore the world of knowledge mapping by reviewing and analysing the current state of research and providing an overview of knowledge mapping's concepts, benefits, techniques, classifications and methodologies, which are precisely reviewed, and their features are highlighted. In addition, we offer directions for future research. Method: Based on the systematic literature review method this study collects, synthesises, and analyses numerous articles on a variety of topics closely related to a knowledge map published from January 2000 to December 2013 on six electronic databases by following a pre-defined review protocol. The articles have been retrieved through a combination of automatic and manual search, hence extensive quantitative and qualitative results of the research are provided. Results: From the review study, we identified 132 articles addressing knowledge maps that have been reviewed in order to extract relevant information on a set of research questions. We found a generally increasing level of activity during this 5-year period. We noted that while existing research covers a large number of studies on some disciplines, such as systems and tools development, it contains very few studies on other disciplines, such as knowledge maps adoption. To aid this situation, we offer directions for future research. Conclusions: The results demonstrated that a knowledge map is an imperative strategy for increasing organisations' effectiveness. In addition, there is a need for more knowledge maps research. © 2016 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Balaid A., Abd Rozan M.Z., Hikmi S.N., Memon J. <br/><b>Key words:</b><br/>Knowledge management, Knowledge maps, Systematic literature review"
26,paper_26,"Jabbari R., Ali N.B., Petersen K., Tanveer B.",,"Context: DevOps, the combination of Development and Operations, is a new way of thinking in the software engineering domain that recently received much attention. Given that DevOps is a new term and novel concept recently introduced, no common understanding of what it entails has been achieved yet. Consequently, definitions of DevOps often only represent a part that is relevant to the concept. Objective:This study aims to characterize DevOps by exploring central components of DevOps definitions reported in the literature, specifying practices explicitly proposed for DevOps and investigating the similarities and differences between DevOps and other existing methods in software engineering. Method: A systematic mapping study was conducted that used six electronic databases: IEEE, ACM, Inspec, Scopus, Wiley Online Library and Web of Science. Result: 44 studies have been selected that report a definition of DevOps, 15 studies explicitly stating DevOps practices, and 15 studies stating how DevOps is related to other existing methods. Papers in some cases stated a combination of a definition, practices, and relations to other methods, the total number of primary studies was 49. Conclusion: We proposed a definition for DevOps which may overcome inconsistencies over the various existing definitions of individual research studies. In addition, the practices explicitly proposed for DevOps have been presented as well as the relation to other software development methods. © 2016 ACM.","<b>Authors:</b><br/>Jabbari R., Ali N.B., Petersen K., Tanveer B. <br/><b>Key words:</b><br/>DevOps definition, DevOps practice, Software development method"
27,paper_27,"Souza D.M., Felizardo K.R., Barbosa E.F.",,"The benefits of using assessment tools for programming assignments have been widely discussed in computing education. However, as both researchers and instructors are unaware of the characteristics of existing tools, they are either not used or are reimplemented. This paper presents the results of a study conducted to collect and evaluate evidence about tools that assist in the assessment of programming assignments. To achieve our goal, we performed a systematic literature review since it provides an objective procedure for identifying the quantity of existing research related to a research question. The results identified subjects in the development of new assessment tools that researchers could better investigate and characteristics of assessment tools that could help instructors make selections for their programming courses. © 2016 IEEE.","<b>Authors:</b><br/>Souza D.M., Felizardo K.R., Barbosa E.F. <br/><b>Key words:</b><br/>Assessment tools, Mapping study, Programming assignments"
28,paper_28,"Giray G., Tüzün E., Tekinerdogan B., Macit Y.",,"The Essence framework has been recently defined as a basis for modeling various kinds of software development methods. The framework includes the necessary concepts to instantiate the software development methods. In this way a new method can be better understood, learned and compared with other methods. In practice, it is not straightforward to model a given software development method using the Essence framework. In this paper we provide a systematic approach for mapping methods to the elements of the Essence framework. To illustrate our approach, we use the mapping of the Nexus, a scaled agile approach, to the Essence framework. We report on the lessons learned and provide our conclusions. © 2016 ACM.","<b>Authors:</b><br/>Giray G., Tüzün E., Tekinerdogan B., Macit Y. <br/><b>Key words:</b><br/>Essence framework, Nexus framework, Software development method"
29,paper_29,"Thakurta R.",,"The importance of prioritizing requirements stems from the fact that not all requirements can usually be met with available time and resource constraints. Efficient and trustworthy methods for prioritizing requirements are therefore in high demand. In this article, we present results of a systematic mapping study in order to appreciate the different considerations that have influenced prioritization of software requirements, identify the various types of artifacts proposed toward prioritizing software requirements, and examine certain characterizations of these artifacts. The results emphasize the heightened attention the domain of requirement prioritization has received in recent years. On the basis of this study, we are able to provide the following inferences regarding possible future research trajectories in software requirement prioritization artifacts: (1) focus on frameworks and tools, (2) emphasis on specialization, and (3) proposition of theory-based artifacts. Additional research possibilities are also pointed out at the end and are expected to stimulate further research on the topic. © 2016 Springer-Verlag London","<b>Authors:</b><br/>Thakurta R. <br/><b>Key words:</b><br/>Content analysis, Requirement prioritization, Software engineering, Systematic mapping study"
30,paper_30,"Roberto R., Lima J.P., Teichrieb V.",,"Tracking is an important task that is used for several applications, such as navigation assistance and augmented reality. The improvement and popularization of mobile devices in recent years allowed these applications to be executed on such devices. Thus, several tracking techniques proposed lately take into consideration the benefits and limitations of handheld devices. Therefore, the goal of this work is to perform a systematic mapping in order to provide trends and classification regarding the recent publications in the area of tracking for mobile devices. This study collected 2276 papers from three scientific databases using an open-source crawler, from which 360 were selected to be classified according to four properties: tracking type, degree of freedom, tracking platform and research type. The analysis of these data resulted in a map of the research field, which was presented under three perspectives: the distribution and trends over time of each classification property and the relationship between them. Besides the visual map, the full list of classified papers is available through an open-source web-based catalog. The results showed that the number of publications is increasing every year, which shows a growing interest in this field. Moreover, most works use the device's sensors for tracking in location-based applications and almost all of them calculate a 2D or 2D + ? pose. There are also several papers about vision-based techniques to compute the device's pose and in the majority of them a full 6D pose is computed. Beyond that, there is a clear preference for systems that calculate the pose locally on the device and only a few use a remote server to assist in this task. Moreover, more than 92% of all papers propose a new technique or use existing ones to create a solution. © 2016 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Roberto R., Lima J.P., Teichrieb V. <br/><b>Key words:</b><br/>Mobile device, Systematic mapping, Tracking"
31,paper_31,"Liu D., Duan J., Shi H.",,"An improved fast simultaneous localization and mapping (FastSLAM) algorithm based on the strong tracking square root central difference Kalman filter (STSRCDKF) with adaptive partial systematic resampling is proposed in this paper to solve the large-scale simultaneous localization and mapping (SLAM) problem for unmanned intelligent vehicle. In the proposed algorithm, STSRCDKF is composed of a strong tracking filter and a square root central difference Kalman filter. STSRCDKF is used to design an adaptive adjusting proposal distribution of the particle filter and to estimate the Gaussian densities of the landmarks. Moreover, an adaptive partial systematic resampling operation is carried out to reduce the degree of particle degeneracy and maintain the diversity of particles. The performance of the proposed algorithm is compared with that of central difference FastSLAM and FastSLAM2.0, the simulation results based on the simulator and two benchmark data sets verify that the proposed algorithm has better adaptability and robustness to respond with time-varying measurement noise. In addition, it reduces computational cost and improves state estimation accuracy and consistency. Furthermore, the validity of the proposed algorithm is verified by the experimental result in campus test site of Beijing University of Technology.","<b>Authors:</b><br/>Liu D., Duan J., Shi H. <br/><b>Key words:</b><br/>"
32,paper_32,"Lu Q., Zhu L., Zhang H., Wu D., Li Z., Xu X.",,"MapReduce has become the standard model for supporting big data analytics. In particular, MapReduce job optimization has been widely considered to be crucial in the implementations of big data analytics. However, there is still a lack of guidelines especially for practitioners to understand how the MapReduce jobs can be optimized. This paper aims to systematic identify and taxonomically classify the existing work on job optimization. We conducted a mapping study on 47 selected papers that were published between 2004 and 2014. We classified and compared the selected papers based on a 5WH-based characterization framework. This study generates a knowledge base of current job optimization solutions and also identifies a set of research gaps and opportunities. This study concludes that job optimization is still in an early stage of maturity. More attentions need to be paid to the cross-data center, cluster or rack job optimization to improve communication efficiency. © 2015 IEEE.","<b>Authors:</b><br/>Lu Q., Zhu L., Zhang H., Wu D., Li Z., Xu X. <br/><b>Key words:</b><br/>big data, job optimization, mapping study, MapReduce, systematic literature review"
33,paper_33,"Hassan M.M., Shah S.M.A., Afzal W., Andler S.F., Lindström B., Blom M.",,"In most of the research on software testability, functional correctness of the software has been the focus while the evidence regarding testability and non-functional properties such as performance is sporadic. The objective of this study is to present the current state-of-the-art related to issues of importance, types and domains of software under test, types of research, contribution types and design evaluation methods concerning testability and software performance. We find that observability, controllability and testing effort are the main testability issues while timeliness and response time (i.e., time constraints) are the main performance issues in focus. The primary studies in the area use diverse types of software under test within different domains, with realtime systems as being a dominant domain. The researchers have proposed many different methods in the area, however these methods lack implementation in practice. © 2016 ACM.","<b>Authors:</b><br/>Hassan M.M., Shah S.M.A., Afzal W., Andler S.F., Lindström B., Blom M. <br/><b>Key words:</b><br/>Software performance, Systematic mapping study, Testability"
34,paper_34,"De Farias M.A.F., Colaço M., Jr., Mendonça M., Novais R., Da Silva Carvalho L.P., Spínola R.O.",,"Background: Software repositories provide large amount of data encompassing software changes throughout its evolution. Those repositories can be effectively used to extract and analyze pertinent information and derive conclusions related to the software history or its current snapshot. Objective: This work aims to investigate recent studies on Mining Software Repositories (MSR) approaches collecting evidences about software analysis goals (purpose, focus, and object of analysis), data sources, evaluation methods, tools, and how the area is evolving. Method: A systematic mapping study was performed to identify and analyze research on mining software repositories by analyzing five editions of Working Conference on Mining Software Repositories - the main conference on this area. Results: MSR approaches have been used for many different goals, mainly for comprehension of defects, analysis of the contribution and behavior of developers, and software evolution comprehension. Besides, some gaps were identified with respect to their goals, focus, and data source type (e.g. lack of usage of comments to identify smells, refactoring, and issues of software quality). Regarding the evaluation method, our analysis pointed out to an extensive usage of some types of empirical evaluation. Conclusion: Studies of the MSR have focused on different goals, however there are still many research opportunities to be explored and issues associated with MSR that should be considered. © 2016 ACM.","<b>Authors:</b><br/>De Farias M.A.F., Colaço M., Jr., Mendonça M., Novais R., Da Silva Carvalho L.P., Spínola R.O. <br/><b>Key words:</b><br/>Empirical software engineering, Mining software repository, Secondary study, Systematic mapping study"
35,paper_35,"Vilela J., Gonçalves E., Holanda A., Figueiredo B., Castro J.",,"Context: The activities related to Requirements engineering (RE) are some of the most important steps in software development, since the requirements describe what will be provided in a software system in order to fulfill the stakeholders' needs. In this context, the ACM Symposium on Applied Computing (SAC) has been a primary gathering forum for many RE activities. When studying a research area, it is important to identify the most active groups, topics, the research trends and so forth. Objective: This study aims to investigate how the SAC RE-Track is evolving, by analyzing the papers published in its 8 previous editions. Method: We adopted a research strategy that combines scoping study and systematic review good practices. Results: We investigated the most active countries, institutions and authors, the main topics discussed, the types of the contributions, the conferences and journals that have most referenced SAC RE-Track papers, the phases of the RE process supported by the contributions, the publications with the greatest impact, and the trends in RE. Conclusions: We found 79 papers over the 8 previous SAC RE-Track editions, which were analyzed and discussed. © 2016 ACM.","<b>Authors:</b><br/>Vilela J., Gonçalves E., Holanda A., Figueiredo B., Castro J. <br/><b>Key words:</b><br/>Relevance, Requirements engineering, Retrospective, SAC, Scoping study, Symposium on Applied Computing, Systematic mapping study, Trends"
36,paper_36,"Lima P., Vilela J., Gonçalves E., Pimentel J., Holanda A., Castro J., Alencar F., Lencastre M.",,"Since its first proposal in the nineties, the i? framework has been used to requirements specification in many domains, such as healthcare, telecommunication, and air traffic control. After the modeling of different examples and case studies, it has been observed that i? models become dramatically more difficult to understand and analyze as they grow larger. This issue has led us to investigate scalability in the context of the i? framework, by means of a systematic mapping study. A total of 119 papers were analyzed, in order to understand how scalability is perceived by the i? research community, which proposals have considered this topic, and what open issues still need to be addressed. We found that scalability issues are indeed perceived as relevant and that further work is still required, even though many potential solutions have already been proposed. This study can be a starting point for researchers aiming to further advance the treatment of scalability in social goal models.","<b>Authors:</b><br/>Lima P., Vilela J., Gonçalves E., Pimentel J., Holanda A., Castro J., Alencar F., Lencastre M. <br/><b>Key words:</b><br/>Goal Models, I-Star, Scalability, Systematic Mapping study"
37,paper_37,"Brito M.A.S., Santos M.P., Souza S.R.S., Souza P.S.L.",,"Different techniques have been proposed to test software artifacts and source code. Integration testing, however, has not been sufficiently applied to all types of systems that require high-quality evaluation. This paper summarizes approaches proposed to test data generation for integration testing identifying gaps and opportunities for further research. We conducted a systematic mapping study in order to discover how data generation for integration testing is supported by existing approaches. A subjective evaluation of the studies was also carried out to evaluate them in terms of quality. As a result of the systematic mapping we found 38 primary studies, published between 1996 and 2015. Most of them focus on specification-based coverage, and to a lesser extent, code coverage. The following trends were identified: increased use of UML models and concern regarding automated test data generation. Gaps were found in the use of gray-box approaches, mutation testing, data generation tools, and experimental validation studies. The study's findings enable researchers to attain an overview of existing approaches and identify points that require further attention. Future research work should look at cost and effectiveness of such approaches.","<b>Authors:</b><br/>Brito M.A.S., Santos M.P., Souza S.R.S., Souza P.S.L. <br/><b>Key words:</b><br/>"
38,paper_38,"Silva F.A., Zaicaner G., Quesado E., Dornelas M., Silva B., Maciel P.",,"Mobile cloud computing (MCC) integrates mobile computing and cloud computing aiming to extend the capabilities of mobile devices through offloading techniques. In MCC, many controlled experiments have been performed using mobile applications as benchmarks. Usually, these applications are used to validate proposed algorithms, architectures or frameworks. The task of choosing a specific benchmark to evaluate MCC proposals is difficult because there is no standard applications list. This paper presents a systematic mapping study for benchmarks used in MCC research. Taking 5Â months of work, we have read 763 papers from MCC field. We catalogued the applications and characterized them considering three facets: category (e.g., games, imaging tools), evaluated resource (e.g., time, energy), and platform (e.g., Android, iPhone). The mapping study evidences research gaps and research trends. Providing a list of downloadable standardized benchmarks, this work can aid better choices to guide more reliable research studies since the same application could be used for different scientific purposes. Â© 2016, Springer Science+Business Media New York.","<b>Authors:</b><br/>Silva F.A., Zaicaner G., Quesado E., Dornelas M., Silva B., Maciel P. <br/><b>Key words:</b><br/>Application, Benchmark, Cloud computing, Mobile computing"
39,paper_39,"Sutherland C.J., Luxton-Reilly A., Plimmer B.",,"A variety of different approaches have been used to add digital ink annotations to text-based documents. While the majority of research in this field has focused on annotation support for static documents, a small number of studies have investigated support for documents in which the underlying content is changed. Although the approaches used to annotate static documents have been relatively successful, the annotation of dynamic text documents poses significant challenges which remain largely unsolved. However, it is difficult to clearly identify the successful techniques and the remaining challenges since there has not yet been a comprehensive review of digital ink annotation research. This paper reports the results of a systematic mapping study of existing work, and presents a taxonomy categorizing digital ink annotation research. © 2015 Elsevier Ltd.","<b>Authors:</b><br/>Sutherland C.J., Luxton-Reilly A., Plimmer B. <br/><b>Key words:</b><br/>Dynamic digital documents, Freeform ink annotation"
40,paper_40,"Silva P., Noël R., Gallego M., Matalonga S., Astudillo H.",,"The effective building of secure software systems has been addressed by security experts and software development experts through several techniques for identifing and mitigating security threats. Many techniques had been theoretically developed, however, for most of these proposals there is few empirical evidence of its application in building secure software systems. A systematic mapping has been conducted to cover the existent technologies for identification and mitigation of security threats. A total of 10 different techniques covering threats identification and 8 covering the mitigation of threats were found. All the initiatives were integrated to at least one activity of the Software Development Lifecycle (SDLC), while 7 show signs of being adopted in the industry. The mapping found only 15 studies that covered 11 different iniatiatives. Only two techniques presented scientific evidence of its results through controlled experiments, while others selected studies presented informal case studies or examples.","<b>Authors:</b><br/>Silva P., Noël R., Gallego M., Matalonga S., Astudillo H. <br/><b>Key words:</b><br/>Architectural tactics, Secure software development, Security patterns, Security threats, Systematic mapping"
41,paper_41,"Munir H., Wnuk K., Runeson P.",,"Open innovation (OI) means that innovation is fostered by using both external and internal influences in the innovation process. In software engineering (SE), OI has existed for decades, while we currently see a faster and broader move towards OI in SE. We therefore survey research on how OI takes place and contributes to innovation in SE. This study aims to synthesize the research knowledge on OI in the SE domain. We launched a systematic mapping study and conducted a thematic analysis of the results. Moreover, we analyzed the strength of the evidence in the light of a rigor and relevance assessment of the research. We identified 33 publications, divided into 9 themes related to OI. 17/33 studies fall in the highrigor/highrelevance category, suggesting the results are highly industry relevant. The research indicates that start-ups have higher tendency to opt for OI compared to incumbents. The evidence also suggests that firms assimilating knowledge into their internal R&D activities, have higher likelihood of gaining financial advantages. We concluded that OI should be adopted as a complementary approach to facilitate internal innovation and not to substitute it. Further research is advised on situated OI strategies and the interplay between OI and agile practices. © 2015, Springer Science+Business Media New York.","<b>Authors:</b><br/>Munir H., Wnuk K., Runeson P. <br/><b>Key words:</b><br/>Collective innovation, Collective invention, Literature review, Open innovation, Research agenda, Software engineering, User innovation"
42,paper_42,"Neiva F.W., David J.M.N., Braga R., Campos F.",,"Context: Many researchers have argued that providing interoperability support only considering the format and meaning (i.e. syntax and semantic) of data exchange is not enough to achieve complete, effective and meaningful collaboration. Pragmatic interoperability has been highlighted as a key requirement to enhance collaboration. However, fulfilling this requirement is not a trivial task and there is a lack of works discussing solutions to achieve this level of interoperability. Objectives: The aim of this study is to present a systematic review and mapping of the literature in order to identify, analyse and classify the published solutions to achieve pragmatic interoperability. Method: To conduct a systematic review and mapping in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: Our study identified 13 papers reporting pragmatic interoperability computational solutions. The first paper in our set of selected papers was published in 2004, the main strategies used to address pragmatic interoperability issues were service discovery, composition and/or selection and ontologies. The application domain of the identified solutions was mainly e-business. In addition, most of the identified solutions were software architectures. Conclusion: Mature proposals addressing pragmatic interoperability are still rare in the literature. Although many works have discussed the importance of pragmatic interoperability, it is necessary that researchers report solutions that implement and evaluate pragmatic interoperability in order to make progress in this area. © 2016 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Neiva F.W., David J.M.N., Braga R., Campos F. <br/><b>Key words:</b><br/>Collaboration, Collaborative systems, Groupware, Interoperability, Pragmatic interoperability"
43,paper_43,"Heradio R., Perez-Morago H., Fernandez-Amoros D., Javier Cabrerizo F., Herrera-Viedma E.",,"Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis. Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL, (ii) work on systematic software reuse has been essential for the development of the area, and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Heradio R., Perez-Morago H., Fernandez-Amoros D., Javier Cabrerizo F., Herrera-Viedma E. <br/><b>Key words:</b><br/>Bibliometrics, Performance analysis, Science mapping, Software product lines"
44,paper_44,"Lélis C.A.S., Araújo M.A.P., David J.M.N., Carneiro G.F.",,"[Background] Reputation systems have attracted the attention of researchers when it comes to collaborative systems. In the context of collaborative software maintenance, systems of this type are employed to facilitate the collection, aggregation and distribution of reputation information about a participant. GiveMe Infra is an infrastructure that supports collaborative software maintenance performed both by co-located and geographically distributed teams. In this last case, reputation is one of the factors that influence collaboration. Despite this recognized relevance, to the best of our knowledge, there is a shortage of tools providing reputation functionalities in the context of collaborative software maintenance. [Objective] However, GiveMe Infra needs to identify and correlate metrics, measures, criteria and factors (called parameters) that are used in defining the value of reputation of an entity. These parameters, used to determine the degree of reputation, can provide evidence of parameters to be used in the context of software maintenance and evolution. [Method] In order to achieve this goal, a systematic mapping was performed. Both the established protocol and the process adopted during the mapping are shown in this article. The parameters identified, as well as how to apply them in the context of software maintenance are demonstrated through an analysis scenario. [Results] Our goal has been achieved since the systematic mapping allowed the identification of the parameters used in defining reputation and even parameters that would not allow a correlation with the context of collaborative software maintenance. The contribution of this research is threefold, in its investigation carried out, the list of identified parameters and also the application in the collaborative software maintenance context. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Lélis C.A.S., Araújo M.A.P., David J.M.N., Carneiro G.F. <br/><b>Key words:</b><br/>Collaborative software maintenance, Metrics, Reputation, Software evolution"
45,paper_45,"Axelsson J., Skoglund M.",,"Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Axelsson J., Skoglund M. <br/><b>Key words:</b><br/>Quality, Software ecosystems, Testing, Verification"
46,paper_46,"Diener M., Cruz E.H.M., Alves M.A.Z., Navaux P.O.A.",,"Optimizing the communication behavior of parallel applications has emerged as an important topic in parallel processing. In shared memory architectures, threads communicate implicitly through memory accesses to shared memory areas. The communication behavior can be improved by mapping threads that communicate a lot to processing units that are close to each other in the memory hierarchy, such that they can benefit from shared caches and faster interconnections. An important aspect of such a communication-aware thread mapping is the accurate and efficient detection of communication in shared memory. Previous work used impromptu definitions, without an evaluation of the complexities of different communication types. In this paper, we perform an in-depth, systematic evaluation of communication in shared memory, focusing on its architectural effects. We present an efficient way to detect communication, which is orders of magnitude faster than a cache simulator, while maintaining a high accuracy. © 2016 IEEE.","<b>Authors:</b><br/>Diener M., Cruz E.H.M., Alves M.A.Z., Navaux P.O.A. <br/><b>Key words:</b><br/>Cache hierarchy, Communication, Interconnections, Thread mapping"
47,paper_47,"Kienberger J., Saad C., Kuntz S., Bauer B.",,"As the automotive industry seeks to include more and more features in its vehicles while simultaneously attempting to reduce the number of ""Electronic Control Units"" (ECUs) that execute the corresponding embedded software, the necessary policy shift towards multi-core technology is in full swing. In order to eventually exploit the extra processing power, there is much additional effort needed for coping with the tremendously increased complexity of such systems. This is largely due to the elaborate parallelization process (partitioning, mapping and scheduling software parts as tasks on different cores) that results in a combinatorial explosion and thus spans a vast search space. Mastering this challenge requires innovative methods and appropriate tools that are specifically designed for the creation of embedded multi-core applications or the migration of legacy software [16]. On the basis of the concept presented in [25], we use the results of its data dependency analysis performed on an ""AUTOSAR""1 model (AUTOSAR system descriptions) to determine advantageous partitions as well as initial task-to-core mappings. Afterwards, the extracted information serves as input for the simulation within an embedded multi-core timing tool suite. Here, the initial solution is evaluated with respect to the fulfillment of basic timing requirements and metrics like cross-core communication rates, average latencies or core workloads. A subsequent optimization process improves the initial solution and enables a comparative assessment. In order to demonstrate the benefit of this approach, we apply it to two models - A fictional mid-sized and a real-life complex one - and show the advantage compared to a parallelization process without preceding dependency analysis and initial partition/mapping suggestions. Copyright © 2016 ACM.","<b>Authors:</b><br/>Kienberger J., Saad C., Kuntz S., Bauer B. <br/><b>Key words:</b><br/>AUTOSAR, Data dependency analysis, Deployment simulation, Keywords multi-core, Mapping optimization, Migration, Semi-automated parallelization, Systematic partitioning, Timing validation"
48,paper_48,"Paz F., Pow-Sang J.A.",,"Usability is currently one of the most important aspects of software quality. Developers are aware that if a software product does not meet user's expectations regarding usability, the entire project might fail. For this reason, several usability evaluation methods have emerged and nowadays are employed from early phases of the software development process. However, the literature reports a wide range of techniques for this purpose. In this way, the choice of a suitable method for a particular scenario has become a difficult decision. Through a systematic mapping review about the use of usability evaluation methods in software development contexts, we have identified the most commonly used techniques. A total of 228 case studies were selected for this review. We found that (1) Questionnaire, (2) User Testing and (3) Heuristic Evaluation were the most frequent. This study is intended to be a guide for scholars in this area. © 2015 IEEE.","<b>Authors:</b><br/>Paz F., Pow-Sang J.A. <br/><b>Key words:</b><br/>evaluation methods, human-computer interaction, systematic mapping review, usability, user-centered design"
49,paper_49,"Bano M.",,"Ambiguity in natural language requirements has long been recognized as an inevitable challenge in requirements engineering (RE). Various initiatives have been taken by RE researchers to address the challenges of ambiguity. In this paper the results of a mapping study are presented that focus on the application of Natural Language Processing (NLP) techniques for addressing ambiguity in requirements. Systematic review of the literature resulted in 174 studies on the subject published during 1995 to 2015, and out of these only 28 are empirically evaluated studies that were selected. From of the resulting set of papers, 81% have focused on detecting ambiguity, whereas 4% and 5% are focusing on reducing and removing ambiguity respectively. Addressing syntactic, semantic, and lexical ambiguities has attracted more attention than other types. In spite of all the research efforts, there is a lack of empirical evaluation of NLP tools and techniques for addressing ambiguity in requirements. The results have pointed out some gaps in empirical results and have raised questions the designing of an analytical framework for research in this field. © 2015 IEEE.","<b>Authors:</b><br/>Bano M. <br/><b>Key words:</b><br/>Ambiguity, NLP, Requirements, Systematic Mapping Study"
50,paper_50,"Nayebi M., Ruhe G., Mota R.C., Mufti M.",,"Software project management is a decision intensive process. Success or failure of the project is highly dependent on these decisions. Analytical techniques and tools can support project managers throughout the software project life cycle by increasing the predictability and chance of success in these projects. In this paper, we report the results of a systematic mapping study within which we investigate the usage of different types of analytics for software project management. We analyze the accessibility of the data as well as the degree of validation reported in the 115 studies selected for final analysis. This resulted in a picture of the status quo (Where are we?) of analytics in software project management. From comparing this status quo with the results of an industrial survey on the industrial needs of different types of analysis, we propose an agenda on future work (Where do we go?). © 2015 IEEE.","<b>Authors:</b><br/>Nayebi M., Ruhe G., Mota R.C., Mufti M. <br/><b>Key words:</b><br/>Analytical project management, Analytical technique, data analysis, systematic mapping"
51,paper_51,"Kosar T., Bohra S., Mernik M.",,"Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs), based on an automatic search including primary studies from journals, conferences, and workshops during the period from 2006 until 2012. Objective: The main objective of the described work was to perform an SMS on DSLs to better understand the DSL research field, identify research trends, and any possible open issues. The set of research questions was inspired by a DSL survey paper published in 2005. Method: We conducted a SMS over 5 stages: defining research questions, conducting the search, screening, classifying, and data extraction. Our SMS included 1153 candidate primary studies from the ISI Web of Science and ACM Digital Library, 390 primary studies were classified after screening. Results: This SMS discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the DSL community focuses more on the development of new techniques/methods rather than investigating the integrations of DSLs with other software engineering processes or measuring the effectiveness of DSL approaches. Furthermore, there is a clear lack of evaluation research. Amongst different DSL development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years. Top cited papers and venues are mentioned, as well as identifying the more active institutions carrying DSL research. Conclusion: The statistical findings regarding research questions paint an interesting picture about the mainstreams of the DSL community, as well as open issues where researchers can improve their research in their future work. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Kosar T., Bohra S., Mernik M. <br/><b>Key words:</b><br/>Domain-Specific Languages, Systematic Mapping Study, Systematic Review"
52,paper_52,"Dusse F., Júnior P.S., Alves A.T., Novais R., Vieira V., Mendonça M.",,"Background: Emergency management (EM) refers to the ability to deal with emergency tasks in different phases and iterations. To do this, each task requires many and different types of information coming from several sources related to the incident. As people working in an emergency situation are generally under stress and have to make quick and effective decisions, they need to assimilate the received information in an easy and intuitive way. Information visualization (InfoVis) is the study of visual representations of abstract data to reinforce human cognition to understand these data through 2D computer screens. It is frequently used to analyze and understand the huge amount of multidimensional data produced in an emergency. Objective: This study analyzes how researchers use information visualization tools to improve emergency management. Our general objective is to map the area examining both the scientific community and the contributions that have been published in the literature, aiming to provide information, such as: understanding how the area is structured, common practices in existing works, and research gaps. Methods: A systematic mapping study was conducted to analyze the available information visualization tools and their applications in EM activities. A thorough search was carried out and a formal selection process was applied to gather all relevant articles on the subject. Selected primary studies were classified and analyzed with respect to their metadata and to answer eight research questions related to our mapping goal. In total, 196 studies were analyzed in depth. Results: The mapping study identified the most common visualization techniques applied in emergency management, the common environments and phases where they are applied, identifying gaps and also possible trends in the subject. We found out that particular issues concerning emergency management are not fully covered by existing visualization approaches, and when covered, existing literature provides only partial solutions. Conclusion: Our results provide a deep analysis on the application of InfoVis in the EM area, supporting researchers and developers in EM systems with insightful information on trending techniques in use, command practices and existing research gaps. We expect that these findings can support them on proposing new approaches to solve open problems in the area. © 2015 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Dusse F., Júnior P.S., Alves A.T., Novais R., Vieira V., Mendonça M. <br/><b>Key words:</b><br/>Emergency management, Information visualization, Systematic mapping study"
53,paper_53,"González-Ladrón-de-Guevara F., Fernández-Diego M., Lokan C.",,"The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>González-Ladrón-de-Guevara F., Fernández-Diego M., Lokan C. <br/><b>Key words:</b><br/>ISBSG data field, Software effort estimation, Systematic mapping study"
54,paper_54,"DeSmet A., Van Cleemput K., Bastiaensens S., Poels K., Vandebosch H., Malliet S., Verloigne M., Vanwolleghem G., Mertens L., Cardon G., De Bourdeaudhuij I.",,"Introduction The Intervention Mapping Protocol (IMP) was applied to the design of a serious game against cyberbullying among adolescents (12-14y). Method The IMP comprises 6 predefined steps. A systematic review assessed the cyberbullying problem and associated health risks (Step 1). Surveys and focus groups collected information on behavior and its determinants from adolescents (surveys, n = 1979 and n = 453, focus groups, n = 69), parents (surveys, n = 48 and n = 323) and educators (survey, n = 451) (Step 1, 2). Meta-analyses analyzed effective methods for cyberbullying programs and serious games (Step 3). A survey (n = 530) and focus groups (n = 69 adolescents, n = 8 adolescents) assessed preferences and program material appreciation (Step 4). Planned activities for step 5 (implementation) and step 6 (effectiveness) are reported. Results Targeting positive bystander behavior (defending, reporting and comforting) was chosen as a viable approach to reduce cyberbullying. Bystander behavior differed by context and was predicted most by positive outcome expectations for the victims. Adolescents valued educator and parental support. Predictors for educator behavior and parental support are described. Serious game design was based on effective change methods and features, and took stakeholder and user preferences into account. Conclusion Findings may aid professionals in evidence- and theory-based design of cyberbullying interventions and serious games. © 2015 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>DeSmet A., Van Cleemput K., Bastiaensens S., Poels K., Vandebosch H., Malliet S., Verloigne M., Vanwolleghem G., Mertens L., Cardon G., De Bourdeaudhuij I. <br/><b>Key words:</b><br/>Adolescence, Bystanders, Cyberbullying, Intervention Mapping, Serious game"
55,paper_55,"Barn B.S., Clark T., Ali A., Arif R.",,"Systematic mapping studies are an important research method and have been used in software engineering to provide an overview of a research area by a process of classification and counting of the outputs in a particular area. They have also been used to examine the outputs found in specific publication outlets. In this paper, we report on the results of a systematic mapping study conducted to review the entire publication output of the Indian Software Engineering Conference (ISEC) series. We use the outputs of the study to present visual depictions of the nature of Indian Software Engineering academic research from 2008-2015. A second contribution of the work reports on comparison of the ISEC series with that of the pre-eminent international conference in software engineering (ICSE). We contextualise the results within the wider picture of the national Indian IT community. © 2016 ACM.","<b>Authors:</b><br/>Barn B.S., Clark T., Ali A., Arif R. <br/><b>Key words:</b><br/>Indian it industry, ISEC, Systematic mapping study"
56,paper_56,"Ibrahim S.N.K.A., Harun J.",,"Computer-supported collaborative learning is one of the most promising innovations to improve teaching and learning with the help of modern information and communication technology. Constructing knowledge via online collaborative and social learning has been realized as one of the ways to nurture students' higher order thinking skills. In the collaborative environment, it normally involves processes of evidence and argumentation which is refers to making convincing claims backed up by assure evidence and broad understanding of various aspects of an issue. When dealing with argumentation, knowledge is shared or transmitted among learners as they work towards common learning goals, for example, a shared understanding of the subject at hand or a solution to a problem. Mainly, learners are not passive input receiver but are active in their process of knowledge acquisition as they participate in discussions, search for information, and exchange opinions with their peers. In collaborative argumentation, knowledge is constructed and shared among peers, not owned by one particular learner after obtaining it from the learning activities, course materials or instructor. However, how to analyze argumentative knowledge construction process in online collaborative learning to confirm that it is really reflects students' higher order thinking skills? Thus, the aim of this paper is to provide an overview of argumentative knowledge construction analysis frameworks and identifying gaps that merit future investigation. For this purpose, we conducted a systematic mapping study on the argumentative knowledge construction analysis frameworks to answer that question. The main results are summarized in this paper. © 2015 IEEE.","<b>Authors:</b><br/>Ibrahim S.N.K.A., Harun J. <br/><b>Key words:</b><br/>argumentative knowledge construction, collaborative learning, CSCL, frameworks, higher order thinking skills, HOTS, systematic mapping"
57,paper_57,"Banaeianjahromi N., Smolander K.",,"Purpose  Constant changes in the environment seem to have become the biggest challenge of a modern enterprise, which emphasizes the constant need to integrate the enterprise into its changing environment. Aiming at eliminating the integration challenges, EA is proposed as a solution. The purpose of this paper is to survey and analyse the available literature on determining the role of EA in EI and also to identify gaps and state-of-the-art in research. Design/methodology/approach  This paper presents a systematic mapping study that found 50 papers in the intersection of EA and EI, these papers were surveyed, analysed, and classified with respect to research focus, research method, and paper type. Findings  Based on the analyses of the final 50 articles, the authors realized that EA framework is the dominating research focus of these studies. Evaluation research is recognized as the most common paper type in this area. However, Experience paper was a rare paper type in this research domain. Constructive research and Case study/multiple case studies are widely applied as the research method. Survey, Delphi study and Grounded theory are the least employed research methods. The conclusion was that there is a need for empirical research in this area. After analysing the articles based on their publication year, the authors also noticed a significant growth between 2004 and 2010. After 2010 the number of publications had a downward trend. Originality/value  To the knowledge of the authors, this study is the first systematic literature study regarding the role of EA in EI. There are several systematic literature reviews about the EA or EI separately but none of them has addressed the specific realm of the research. Hence, the goal of this study is to provide a map of existing literature to enable improvement of the practice with the known research results and to identify gaps for future research. © 2016, © Emerald Group Publishing Limited.","<b>Authors:</b><br/>Banaeianjahromi N., Smolander K. <br/><b>Key words:</b><br/>Enterprise architecture, Enterprise integration, Systematic mapping study"
58,paper_58,"Merchan V.",,"The characteristics of a value-based quality system as part of the state of the art of Government Information Technology (GoIT), are a challenge for those who study these topics and propose the existence of a systematic and comprehensive mechanisms for quality assessment. In order to identify and classify the existing of empirical studies in the field of a value-based quality assessment for GoIT, a systematic mapping was proposed for studies that guide the development of this work. The method to be used to perform a specific systematic mapping, is used to reduce of biased results. Therefore, as a result of this research, the steps taken for mapping are described. From the 220 papers identified with different academic search engines, 14 were selected for compliance, and classified according to their type and contribution. The studies show that value-based quality evaluation in the context of GoIT seems to be an area that provides opportunities for further investigation and assessment for quality criteria focused on value. © 2015 IEEE.","<b>Authors:</b><br/>Merchan V. <br/><b>Key words:</b><br/>Information technology governance, Quality assessment, Systematic mapping, Value-based"
59,paper_59,"Asghar I., Cang S., Yu H.",,"This paper presents a systematic mapping study based on literature and industrial survey related to assistive technologies for people with dementia. The world population of the ageing people is increasing both in developed and developing countries, so does the number of people with dementia. Consequently assistive technologies are getting much importance from academic researchers and industry as an aid for people with dementia. Assistive technologies are helping people with dementia to perform activities which otherwise are not possible for them without external help. Systematic mapping studies are popular in other fields like software engineering, yet this methodology is mostly ignored in assistive technology research. Literature survey indicates that there is no systematic mapping study conducted so far on assistive technologies for people with dementia. Thus, we are motivated to conduct this systematic mapping study on assistive technologies for people with dementia. As per the study nature, we carried out a thorough literature and industrial survey. The results indicates that current available assistive technologies can be classified into five major types i.e. robotics, health monitoring, prompts and reminders, communication and software. There is rich literature available on first three categories while communication and software based assistive technologies need more attention, whereas the industrial focus is mostly on health monitoring and software based assistive technologies. The mapping study results emphasize the need for industry to invest more efforts in communication and software based assistive technologies. © 2015 IEEE.","<b>Authors:</b><br/>Asghar I., Cang S., Yu H. <br/><b>Key words:</b><br/>assistive technologies, dementia, systematic mapping study, wellbeing of elderly"
60,paper_60,"Cravero A., Sepulveda S.",,"The OLAP cubes allow to visualizing information from a data warehouse to be analyzed by business executives. An interesting research topic is the OLAP design because they must be created according to the need of users, the context, how to process queries, among other research topics. This paper presents a systematic mapping study of the main research topics for designing OLAP. © 2016 IEEE.","<b>Authors:</b><br/>Cravero A., Sepulveda S. <br/><b>Key words:</b><br/>designing, methodologies, OLAP, systematic mapping, techniques, tools"
61,paper_61,"Abbaspour Asadollah S., Sundmark D., Eldh S., Hansson H., Afzal W.",,"Debuggingthe process of identifying, localizing and fixing bugsis a key activity in software development. Due to issues such as non-determinism and difficulties of reproducing failures, debugging concurrent software is significantly more challenging than debugging sequential software. A number of methods, models and tools for debugging concurrent and multicore software have been proposed, but the body of work partially lacks a common terminology and a more recent view of the problems to solve. This suggests the need for a classification, and an up-to-date comprehensive overview of the area. This paper presents the results of a systematic mapping study in the field of debugging of concurrent and multicore software in the last decade (20052014).The study is guided by two objectives: (1) to summarize the recent publication trends and (2) to clarify current research gaps in the field. Through a multi-stage selection process, we identified 145 relevant papers. Based on these, we summarize the publication trend in the field by showing distribution of publications with respect to year, publication venues, representation of academia and industry, and active research institutes. We also identify research gaps in the field based on attributes such as types of concurrency bugs, types of debugging processes, types of research and research contributions. The main observations from the study are that during the years 20052014: (1) there is no focal conference or venue to publish papers in this area, hence, a large variety of conferences and journal venues (90) are used to publish relevant papers in this area, (2) in terms of publication contribution, academia was more active in this area than industry, (3) most publications in the field address the data race bug, (4) bug identification is the most common stage of debugging addressed by articles in the period, (5) there are six types of research approaches found, with solution proposals being the most common one, and (6) the published papers essentially focus on four different types of contributions, with methods being the most common type. We can further conclude that there are still quite a number of aspects that are not sufficiently covered in the field, most notably including (1) exploring correction and fixing bugs in terms of debugging process, (2) order violation, suspension and starvation in terms of concurrency bugs, (3) validation and evaluation research in the matter of research type, (4) metric in terms of research contribution. It is clear that the concurrent, parallel and multicore software community needs broader studies in debugging. This systematic mapping study can help direct such efforts. © 2016 Springer Science+Business Media New York","<b>Authors:</b><br/>Abbaspour Asadollah S., Sundmark D., Eldh S., Hansson H., Afzal W. <br/><b>Key words:</b><br/>Bugs, Concurrent, Debugging process, Failure, Fault, Multicore, Parallel, Systematic mapping study"
62,paper_62,"Pahl C., Jamshidi P.",,"Microservices have recently emerged as an architectural style, addressing how to build, manage, and evolve architectures out of small, self-contained units. Particularly in the cloud, the microservices architecture approach seems to be an ideal complementation of container technology at the PaaS level However, there is currently no secondary study to consolidate this research. We aim here to identify, taxonomically classify and systematically compare the existing research body on microservices and their application in the cloud. We have conducted a systematic mapping study of 21 selected studies, published over the last two years until end of 2015 since the emergence of the microservices pattern. We classified and compared the selected studies based on a characterization framework. This results in a discussion of the agreed and emerging concerns within the microservices architectural style, positioning it within a continuous development context, but also moving it closer to cloud and container technology. Copyright © 2016 by SCITEPRESS-Science and Technology Publications, Lda. All rights reserved.","<b>Authors:</b><br/>Pahl C., Jamshidi P. <br/><b>Key words:</b><br/>Cloud, Container, Mircoservices, Systematic Literature Review, Systematic Mapping Study"
63,paper_63,"Brink C., Heisig P., Wackermann F.",,"A product line (PL) supports and simplifies the development process of (software) systems by reusing assets. As systems are subjected to frequent alterations, the implementation of this changes can be a complex and error-prone task. For this reason a change impact analysis (CIA) systematically identifies locations that are affected by a change. While both approaches (PL and CIA) per se are often discussed in literature, the combination of them is still a challenge. This paper gives a comprehensive overview of literature, which addresses the integration of PL and CIA concepts. Furthermore, we classify our results to outline both, the current research stage as well as gaps. Therefore, we conducted a systematic mapping study incorporating 165 papers. While most of the papers have their background within Software Product Lines (SPLs) (44.2%) or PLs (5.5%), CIA in the combination with Multi Product Lines (2.4%) or Product Families (PFs) (1.8%) is sparsely addressed in literature. The results show that CIA for SPLs has been partially addressed yet, whereas the consideration of different disciplines (PFs) is insufficiently covered. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Brink C., Heisig P., Wackermann F. <br/><b>Key words:</b><br/>Change impact analysis, Embedded systems, Hardware/Software, Product family, Product line, Systematic mapping study"
64,paper_64,"Idri A., Hosni M., Abran A.",,"Ensemble methods have been used recently for prediction in data mining area in order to overcome the weaknesses of single estimation techniques. This approach consists on combining more than one single technique to predict a dependent variable and has attracted the attention of the software development effort estimation (SDEE) community. An ensemble effort estimation (EEE) technique combines several existing single/classical models. In this study, a systematic mapping study was carried out to identify the papers based on EEE techniques published in the period 2000-2015 and classified them according to five classification criteria: research type, research approach, EEE type, single models used to construct EEE techniques, and rule used the combine single estimates into an EEE technique. Publication channels and trends were also identified. Within the 16 studies selected, homogeneous EEE techniques were the most investigated. Furthermore, the machine learning single models were the most frequently employed to construct EEE techniques and two types of combiner (linear and non-linear) have been used to get the prediction value of an ensemble. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","<b>Authors:</b><br/>Idri A., Hosni M., Abran A. <br/><b>Key words:</b><br/>Ensemble effort estimation, Software development effort estimation, Systematic mapping study"
65,paper_65,"Alves N.S.R., Mendes T.S., De Mendonça M.G., Spinola R.O., Shull F., Seaman C.",,"Context: The technical debt metaphor describes the effect of immature artifacts on software maintenance that bring a short-term benefit to the project in terms of increased productivity and lower cost, but that may have to be paid off with interest later. Much research has been performed to propose mechanisms to identify debt and decide the most appropriate moment to pay it off. It is important to investigate the current state of the art in order to provide both researchers and practitioners with information that enables further research activities as well as technical debt management in practice. Objective: This paper has the following goals: to characterize the types of technical debt, identify indicators that can be used to find technical debt, identify management strategies, understand the maturity level of each proposal, and identify what visualization techniques have been proposed to support technical debt identification and management activities. Method: A systematic mapping study was performed based on a set of three research questions. In total, 100 studies, dated from 2010 to 2014, were evaluated. Results: We proposed an initial taxonomy of technical debt types, created a list of indicators that have been proposed to identify technical debt, identified the existing management strategies, and analyzed the current state of art on technical debt, identifying topics where new research efforts can be invested. Conclusion: The results of this mapping study can help to identify points that still require further investigation in technical debt research. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Alves N.S.R., Mendes T.S., De Mendonça M.G., Spinola R.O., Shull F., Seaman C. <br/><b>Key words:</b><br/>Software engineering, Software maintenance, Systematic mapping, Technical debt"
66,paper_66,"Pinto V.H.S.C., Luz H.J.F., Oliveira R.R., Souza P.S.L., Souza S.R.S.",,"Background: SaaS (Software as a Service) is a services delivery model in Cloud Computing whose applications are remotely hosted by the service provider and available to customers on demand over the Internet. Multi-tenant Architecture (MTA) is an organizational pattern for SaaS that enables a single instance of an application to be hosted on the same hardware and accessed by multiple customers, so-called tenants, with the aim of lowering costs. Tenants are able to configure the system according to their particular needs. Objective: This research aims at the obtaining an overview of the challenges and research opportunities in MTA context for SaaS through a Systematic Mapping Study. Results: Eighty nine primary studies were selected for discussions on advances and opportunities for further investigations. The results showed the relevancy of MTA and pointed out the main research trends for next years in this topic.","<b>Authors:</b><br/>Pinto V.H.S.C., Luz H.J.F., Oliveira R.R., Souza P.S.L., Souza S.R.S. <br/><b>Key words:</b><br/>Cloud computing, Multi-tenant architecture, Software as a service, Systematic mapping study"
67,paper_67,"Ribeiro L.F., De Farias M.A.F., Mendonça M., Spínola R.O.",,"The term Technical Debt (TD) is used to describe the debt that a development team incurs when it takes shortcuts in the software development process, but that may increase the complexity and maintenance cost in the long-term. If a development team does not manage TD, this debt can cause significant long-term problems such as high maintenance costs. An important goal of the management of the debt is to evaluate the appropriate time to pay a TD item and to effectively apply decision-making criteria to balance the shortterm benefits against long-term costs. However, although there are different studies that have proposed strategies for the management of TD, decision criteria are often discussed in the background and, sometimes, they are not even mentioned. Thus, the purpose of this work is to identify, by performing a systematic mapping study of the literature, decision-making criteria that have been proposed to support the management of TD. We identified 14 decision-making criteria that can be used by development teams to prioritize the payment of TD items and a list of types of debt related to the criteria. In addition, the results show possible gaps where further research may be performed. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","<b>Authors:</b><br/>Ribeiro L.F., De Farias M.A.F., Mendonça M., Spínola R.O. <br/><b>Key words:</b><br/>Decision-making criteria, Software maintenance, Systematic mapping, Technical Debt, Technical debt management"
68,paper_68,"De Vargas Agilar E., De Almeida R.B., Canedo E.D.",,"Legacy system modernization has gained increasing attention from both researchers and practitioners, mainly due to the need of maintaining legacy systems towards business needs and technology advances. In this way, a set of techniques, tools and terms related to software modernization have been proposed- although they have not been consolidated yet. This hinders the characterization of real modernization scenarios according to the existing literature. This paper synthesize the existing contributions related to software modernization by means of a mapping study that characterizes the main results in terms of proposed processes, techniques, and tools. As one of our main findings, we report a lack of empirical studies trying to understand the benefits of using the existing approaches for software modernization.","<b>Authors:</b><br/>De Vargas Agilar E., De Almeida R.B., Canedo E.D. <br/><b>Key words:</b><br/>Legacy systems, Mapping studies in software engineering, Software modernization"
69,paper_69,"Berntsen K.R., Olsen M.R., Limbu N., Tran A.T., Colomo-Palacios R.",,"Information Technology (IT) has become a key element in our everyday life, and one of humanitys current challenges is to conserve the environment and attain a sustainable IT development. Therefore, it has become increasingly important how environmentally friendly a software product is during its life cycle and the effects on the environment related to the development, exercise, maintenance and disposal of the software product. The purpose of this study is to outline recent development of frameworks and guidelines in sustainable software engineering. A systematic mapping was conducted which focuses on practices and models that are being used or proposed in this regard. The results reveal different types of models and different criteria for evaluating sustainability properties. In addition, the study indicates an increase of interest in this field in recent years whereas results suggest a handful of prominent authors and venues publishing research within the scope of sustainable software engineering. © Springer International Publishing AG 2017.","<b>Authors:</b><br/>Berntsen K.R., Olsen M.R., Limbu N., Tran A.T., Colomo-Palacios R. <br/><b>Key words:</b><br/>Sustainable software engineering, Systematic mapping"
70,paper_70,"García-Mireles G.A.",,"Sustainability is a main concern in our current society. One of the aspects that play an important role in supporting sustainable development is Information Technology (IT). Both software behavior and the way it is developed impact the amount of energy consumption. Thus, this paper aims to present the most recent approaches to address sustainability from a software process improvement perspective. A systematic mapping study was conducted to identify the latest efforts made in the IT field to improve sustainability. As a result, seven primary papers with initial ideas about how sustainability can be integrated into software processes were found. The lack of both proposals and empirical data suggests that further research on the topic is needed. © Springer International Publishing AG 2017.","<b>Authors:</b><br/>García-Mireles G.A. <br/><b>Key words:</b><br/>Environmental sustainability, Green software, Software process improvement"
71,paper_71,"de Carvalho M.F., Gasparini I., Hounsell M.S.",,"This work surveys the Brazilian scientific production regarding Digital Games (DG) for Math Literacy. Math literacy is divided into three stages: fundamental, numbering and operations. Through a systematic literature mapping, 16.483 papers from all the main events and journals in the computing and informatics in education areas over the last decade were analyzed. Among the few (7) DG found all comply to the features of a Serious Game (SG), most focused on the four basic operations, especially on the multiplication operation, none focused on the fundamentals or numbering, they were meant for children of the first cycle of elementary school, the participation of educators occurred in 5 but in only 2 of them the participation occurred in the designing phase of the game. The majority of SG were evaluated against usability issues rather than learning aspects. The results indicate that Brazilian research on DG for Math Literacy is yet to mature and there is a remarkable absence of SG developments for the initial stages of Math Literacy by the Brazilian scientific community. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>de Carvalho M.F., Gasparini I., Hounsell M.S. <br/><b>Key words:</b><br/>Educational games, Math literacy, Numeracy, Serious games, Systematic literature mapping"
72,paper_72,"Yang C., Liang P., Avgeriou P.",,"Context Combining software architecture and agile development has received significant attention in recent years. However, there exists no comprehensive overview of the state of research on the architecture-agility combination. Objective This work aims to analyze the combination of architecture and agile methods for the purpose of exploration and analysis with respect to architecting activities and approaches, agile methods and practices, costs, benefits, challenges, factors, tools, and lessons learned concerning the combination. Method A systematic mapping study (SMS) was conducted, covering the literature on the architecture-agility combination published between February 2001 and January 2014. Results Fifty-four studies were finally included in this SMS. Some of the highlights: (1) a significant difference exists in the proportion of various architecting activities, agile methods, and agile practices employed in the combination. (2) none of the architecting approaches has been widely used in the combination. (3) there is a lack of description and analysis regarding the costs and failure stories of the combination. (4) twenty challenges, twenty-nine factors, and twenty-five lessons learned were identified. Conclusions The results of this SMS help the software engineering community to reflect on the past thirteen years of research and practice on the architecture-agility combination with a number of implications. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Yang C., Liang P., Avgeriou P. <br/><b>Key words:</b><br/>Agile development, Architecting approach, Software architecture"
73,paper_73,"Idri A., Abnane I., Abran A.",,"Missing Values (MV) present a serious problem facing research in software engineering (SE) which is mainly based on statistical and/or data mining analysis of SE data. Therefore, various techniques have been developed to deal adequately with MV. In this paper, a systematic mapping study was carried out to summarize the existing techniques dealing with MV in SE datasets and to classify the selected studies according to six classification criteria: research type, research approach, MV technique, MV type, data types and MV objective. Publication channels and trends were also identified. As results, 35 papers concerning MV treatments of SE data were selected. This study shows an increasing interest in machine learning (ML) techniques especially the K-nearest neighbor algorithm (KNN) to deal with MV in SE datasets and found that most of the MV techniques are used to serve software development effort estimation techniques. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Idri A., Abnane I., Abran A. <br/><b>Key words:</b><br/>Missing values, Software engineering, Systematic mapping study"
74,paper_74,"Batot E., Sahraoui H., Syriani E., Molins P., Sboui W.",,"As a contribution to the adoption of the Model-Driven Engineering (MDE) paradigm, the research community has proposed concrete model transformation solutions for the MDE infrastructure and for domain-specific problems. However, as the adoption increases and with the advent of the new initiatives for the creation of repositories, it is legitimate to question whether proposals for concrete transformation problems can be still considered as research contributions or if they respond to a practical/technical work. In this paper, we report on a systematic mapping study that aims at understanding the trends and characteristics of concrete model transformations published in the past decade. Our study shows that the number of papers with, as main contribution, a concrete transformation solution, is not as high as expected. This number increased to reach a peak in 2010 and is decreasing since then. Our results also include a characterization and an analysis of the published proposals following a rigorous classification scheme. © Copyright 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.","<b>Authors:</b><br/>Batot E., Sahraoui H., Syriani E., Molins P., Sboui W. <br/><b>Key words:</b><br/>Model Transformation, Software Engineering, Systematic Mapping"
75,paper_75,"Paz F., Pow-Sang J.A.",,"Given that usability is one of the most important aspects of software quality, several methods have been developed in order to establish techniques capable of evaluating this attribute from early phases of the software development process. However, the choice of the most appropriate method for a particular scenario is still a difficult decision, due to the existence of a vast number of approaches that are described in the literature for this purpose. Therefore, a systematic mapping review was conducted in order to identify the most commonly used usability evaluation techniques in software developments. A total of 1169 studies were identified, of which only 215 studies were selected for this review. According to the analysis, most cases studies establish the use of usability questionnaires as assessment tool. In addition, health informatics and Web applications are the software domain and type of application that are frequently reported in these evaluations. This work has allowed to reach promising results in this area. It is intended to be a guide for specialists to support the choice of the most suitable method for a particular scenario. © 2016 SERSC.","<b>Authors:</b><br/>Paz F., Pow-Sang J.A. <br/><b>Key words:</b><br/>Human-computer interaction, Software development process, Systematic mapping review, Usability evaluation methods, User-centered design"
76,paper_76,"Deniz G., Durdu P.O.",,"Brain Computer Interface (BCI) is an emerging research area which has been studied over thirty years extensively in the fields of clinical neurophysiology and neuroscience however it is now recognized as an interdisciplinary field involving neurobiology, psychology, engineering, mathematics and computer science. HCI community has begun to deal with BCI and substantial publications have been made in HCI related journals and conferences in recent years. In the scope of this research we conducted a systematic mapping study with the articles published in proceedings of HCI International Conferences to reveal the trends, general aims of the articles, which signal recording modality were used, for what kind of practical applications were developed, which feature types and classification algorithms were used and what are the nationalities and institutes of contributing authors in the scope of HCI International conference. The aim was to give insight to the researchers who were studying or would like to study in the area of BCI. According to the results of the mapping, research type of BCI articles were mainly in the category of emerging applications not related to communication and control. General aim of the articles was determined as practical applications of BCI technologies first and then development or improvement of methodologies for signal processing issues. Visual P300 was the mostly applied BCI paradigms and SSVEP and motor imagery followed next. Most of the articles had reported EEG as a signal acquisition method, ICA or bandpass filter as a preprocessing or filtering method, ERP as a feature type and LDA and LR as a classification algorithm method. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Deniz G., Durdu P.O. <br/><b>Key words:</b><br/>BCI, BCI research trends, Brain - computer interface, HCI International, Systematic mapping study"
77,paper_77,"Vale T., Crnkovic I., De Almeida E.S., Silveira Neto P.A.D.M., Cavalcanti Y.C., Meira S.R.D.L.",,"The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Vale T., Crnkovic I., De Almeida E.S., Silveira Neto P.A.D.M., Cavalcanti Y.C., Meira S.R.D.L. <br/><b>Key words:</b><br/>Component-based software development, Component-based software engineering, Software component, Systematic mapping study"
78,paper_78,"Magües D.A., Castro J.W., Acuña S.T.",,"Context: Over the last decade there has been a growing interest in the integration of agile software development process (ASDP) and user-centred design (UCD). However, there are no papers that study which usability techniques related to requirements engineering are being adopted in the ASDP, and there are no formalized proposals for their adoption. Objective: Identify which techniques related to requirements engineering activities are being adopted in the ASDP and determine how they are being adopted. Method: We have conducted a systematic mapping study (SMS) to retrieve the literature reporting the application of usability techniques in the ASDP. We analysed these techniques using a catalogue of techniques compiled by software engineering researchers. We then determined the manner in which the techniques that are being used in the ASDP were adopted. Results: The agile community is very much interested in adopting usability techniques. The most used techniques are Personas, contextual inquiry and prototyping. Conclusions: This research offers an overview of the adoption of usability techniques related to requirements engineering in ASDPs and reports how they are being adopted. We found that some of the techniques are being adapted for adoption.","<b>Authors:</b><br/>Magües D.A., Castro J.W., Acuña S.T. <br/><b>Key words:</b><br/>Agile software development, Systematic mapping study, Usability, Usability techniques, User-centred design"
79,paper_79,"Pettersson O., Andersson J.",,"Software ecosystems is one promising strategy for organizations to find new market segments, new innovative value propositions creating new value streams. However, understanding internal and external actors, resources and relationships that could be leveraged in a SECO is critical for their strategic decisions. The consequence of mistakes may be costly failures that can force an organization to move out of a market. This paper describes a systematic mapping study that targets description of software ecosystems. Our conjecture is that adequate description support leads to modeling, which will improve information and in turn strategic decisions. The survey searches existing literature for description techniques and their application for comprehensive description. The study identifies and maps 63 primary studies out of 937 candidates according to their degree of modeling support and several other important aspects for SECO description. The analysis indicates that no approach fully supports comprehensive SECO descriptions, supporting domain specific and view specific modeling of ecosystem concerns. The analysis is used to highlight areas for a future research agenda. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Pettersson O., Andersson J. <br/><b>Key words:</b><br/>Domain specific, Mapping study, Software ecosystems, View"
80,paper_80,"Ozakinci R., Tarhan A.",,"Software quality is the set of inherent characteristics that are built into a software product throughout software development process. An important indicator of software quality is the trend of software defects in the life-cycle. The models of software defect prediction and software reliability provide the opportunity for practitioners to observe the defectiveness distribution of their products in development and operation. However, reported studies are mostly focused on coding or testing stages. Though this is reasonable due to executable nature of the product, it prevents practitioners from taking the advantages (such as cost reduction) of identifying and predicting software defects earlier in the life-cycle. This paper, therefore, provides an overview of the trend of early software defect prediction studies as retrieved by a systematic mapping of the literature, and elaborates on the methods, attributes, and metrics of the studies that comprise software process data in the defect prediction. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Ozakinci R., Tarhan A. <br/><b>Key words:</b><br/>Defect prediction, Early, Prediction model, Process metrics, Software defect, Software quality, Software reliability, Systematic mapping"
81,paper_81,"Souza Neto P.A., Vargas-Solar G., Da Costa U.S., Musicante M.A.",,"Context The development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. Objective This paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. Method Our analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. Results This paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented), (ii) contribution (methodology, system, middleware), (iii) software process phase, (iv) technique or mathematical model used for expressing NFR, and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. Conclusion This systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping-service-based-app-nfr. © 2015 Elsevier B.V.","<b>Authors:</b><br/>Souza Neto P.A., Vargas-Solar G., Da Costa U.S., Musicante M.A. <br/><b>Key words:</b><br/>Non-functional requirements, Service-based software process, Systematic mapping"
82,paper_82,"Paula D., Cormican K.",,"[No abstract available]","<b>Authors:</b><br/>Paula D., Cormican K. <br/><b>Key words:</b><br/>Design research, Design thinking, Empirical study, Mapping study"
83,paper_83,"Souza E., Vitório D., Castro D., Oliveira A.L.I., Gusmão C.",,"The growth of social media and user-generated content (UGC) on the Internet provides a huge quantity of information that allows discovering the experiences, opinions, and feelings of users or customers. Opinion Mining (OM) is a sub-field of text mining in which the main task is to extract opinions from UGC. Given that Portuguese is one of the most common spoken languages in the world, and it is also the second most frequent on Twitter, the goal of this work is to plot the landscape of current studies that relates the application of OM for Portuguese. A systematic mapping review (SMR) method was applied to search, select and to extract data from the included studies. Manual and automated searches retrieved 6075 studies up to year 2014, from which 25 articles were included. Almost 70 % of all approaches focus on the Brazilian Portuguese variant. Naïve Bayes and Support Vector Machine were the main classifiers and SentiLex-PT was the most used lexical resource. Portugal and Brazil are the main contributors in processing the Portuguese language. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Souza E., Vitório D., Castro D., Oliveira A.L.I., Gusmão C. <br/><b>Key words:</b><br/>Opinion mining, Portuguese language, Sentiment analysis, Text classification, Text mining"
84,paper_84,"Arkin E., Tekinerdogan B., Imre K.M.",,"The need for high-performance computing together with the increasing trend from single processor to parallel computer architectures has leveraged the adoption of parallel computing. To benefit from parallel computing power, usually parallel algorithms are defined that can be mapped and executed on parallel computing platforms. In general, different alternative mappings can be defined each with different performance. For small computing platforms with a limited number of processing nodes, the mapping process can be carried out manually. However, for large-scale parallel computing platforms in which hundreds of thousands of processing nodes are applied, the number of possible mapping alternatives increases dramatically, and the mapping process becomes intractable for the human engineer. To assist the parallel computing engineer, we provide a systematic approach to derive feasible mapping alternatives of parallel algorithms to parallel computing platforms. The approach includes activities for modeling the parallel algorithm and parallel computing platform, generation of feasible mapping alternatives, generation of the deployment code, and finally the deployment of the generated code to the nodes. We evaluate our approach for deriving feasible mapping alternatives for four well-known parallel algorithms. The evaluation is based on both simulations and real executions of the generated mapping alternatives. © 2016 John Wiley & Sons, Ltd.","<b>Authors:</b><br/>Arkin E., Tekinerdogan B., Imre K.M. <br/><b>Key words:</b><br/>Architecture viewpoints, Model-driven software development, Parallel computing, Tool support"
85,paper_85,"Pereira K.S., Feitosa E., Conte T.",,"The objective of this paper to summarize the current knowledge in Human Computer Interaction (HCI) and Information Security (IS) areas regarding the classification of the end-user profile and present a new taxonomy to classify risk end-user profile in interaction with the computing environment in the information security perspective. A systematic mapping study was performed to assess the taxonomy of end-users. From an initial set of 105 papers based on string search, we conducted and selected a total of 21 papers. After the full reading of these 21 papers, only 02 papers were selected and 01 new paper were manually added. The results obtained allowed us to identify gap profiles of end-users related to the risk they cause to the computing environment. Thus, we propose a taxonomy to classify risk end-user profile in interaction with the computing environment. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Pereira K.S., Feitosa E., Conte T. <br/><b>Key words:</b><br/>Computing environment, Risk end-user, Taxonomy"
86,paper_86,"Souza E., Castro D., Vitório D., Teles I., Oliveira A.L.I., Gusmão C.",,"Unstructured data accounts for more than 80% of enterprise data and is growing at an annual exponential rate of 60%. Text mining refers to the process of discovering new, previously unknown and potentially useful information from a variety of unstructured data including user-generated text content (UGTC). Given that Portuguese language is one of the most common languages in the world, and it is also the second most frequent language on Twitter, the goal of this work is to plot the landscape of current studies that relates the application of text mining to UGTC in the Portuguese language. The systematic mapping review method was applied to search, select, and to extract data from the included studies. Our manual and automated searches retrieved 6075 studies up to year 2014, from which 35 were included in the study. Text classification concentrates 79% of all text mining tasks, having the Naïve Bayes as the main classifier and Twitter as the main data source. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Souza E., Castro D., Vitório D., Teles I., Oliveira A.L.I., Gusmão C. <br/><b>Key words:</b><br/>Opinion mining, Portuguese language, Text classification, Text mining, User-generated content"
87,paper_87,"Bosse R., Soares A.V., Hounsell M.S.",,"Rehabilitation using video games is more motivating than standard rehabilitation. There are several games developed for balance rehabilitation however, they focus on different aspects, apply a variety of assessment and were developed using a myriad of approaches. In order to provide an overview about specially purposed games for balance rehabilitation, a systematic literature mapping was conducted to assess how they were developed, what software evaluation and clinical assessment were applied and the devices they used. A total of 514 studies were analyzed, but only 44 of them satisfied the inclusion and exclusion criteria. As a result, we verified that most studies used attachment based commercial devices, usability is the most used criteria to evaluate the game and questionnaires are used for it. There is no consensus on clinical assessment metrics and a remarkable lack of design methodology used. From the plethora of serious games found in the literature it is clear that much research need to be done but games can already be considered an acceptable approach for balance promotion. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Bosse R., Soares A.V., Hounsell M.S. <br/><b>Key words:</b><br/>Balance, Literature mapping, Rehabilitation, Serious games"
88,paper_88,"Soad G.W., Esteca A.M.N., Fioravanti M.L., Barbosa E.F.",,"Computer Supported Collaborative Learning (CSCL) has stood out as one of the most successful strategies to support the teaching and learning processes. Currently, a variety of CSCL systems for different knowledge areas and educational levels can be found. The evaluation of such systems is essential for both the selection of the most appropriate computational tool and improvements in the existing tools. Motivated by this scenario, we present a CSCL Systems Evaluation Catalog established from the results of a systematic mapping conducted in this domain. The main goal of the systematic mapping was to find the characteristics to be considered in this type of evaluation. The catalog adopts a three-level hierarchical structure, whose dimensions, characteristics and description of CSCL systems are defined and detailed. Copyright ISCA.","<b>Authors:</b><br/>Soad G.W., Esteca A.M.N., Fioravanti M.L., Barbosa E.F. <br/><b>Key words:</b><br/>CSCL systems, Evaluation catalog, Systematic mapping"
89,paper_89,"Söylemez M., Tarhan A.",,"Process assessment enables to identify strengths and weaknesses of selected processes in a specific domain typically by referencing process maturity/capability frameworks. Assessment findings are usually transformed into action-items for process improvement. In healthcare domain where hospitals offer high-risk services to patients every day in a complex, dynamic, and multidisciplinary environment, establishing process thinking and effective process management is increasingly demanded but not an easy task to accomplish. In this study, we investigate the maturity/capability frameworks that are proposed or used for assessing and improving the healthcare processes. We searched the studies reported between the years 2000 and 2015 in scientific digital libraries and identified 29 studies out of 958 initially retrieved in a systematic way. This study provides an analysis of six studies out of 29 with respect to a number of research questions regarding context, scope, time coverage, and results as well as research method and contribution. © Springer International Publishing Switzerland 2016.","<b>Authors:</b><br/>Söylemez M., Tarhan A. <br/><b>Key words:</b><br/>Capability framework, Healthcare, Healthcare process, Maturity model, Process assessment, Process capability, Process improvement, Process maturity, Systematic mapping"
90,paper_90,"Kersten T.P., Stallmann D., Tschirschwitz F.",,"For mapping of building interiors various 2D and 3D indoor surveying systems are available today. These systems essentially differ from each other by price and accuracy as well as by the effort required for fieldwork and post-processing. The Laboratory for Photogrammetry & Laser Scanning of HafenCity University (HCU) Hamburg has developed, as part of an industrial project, a lowcost indoor mapping system, which enables systematic inventory mapping of interior facilities with low staffing requirements and reduced, measurable expenditure of time and effort. The modelling and evaluation of the recorded data take place later in the office. The indoor mapping system of HCU Hamburg consists of the following components: laser range finder, panorama head (pan- Tilt-unit), single-board computer (Raspberry Pi) with digital camera and battery power supply. The camera is pre-calibrated in a photogrammetric test field under laboratory conditions. However, remaining systematic image errors are corrected simultaneously within the generation of the panorama image. Due to cost reasons the camera and laser range finder are not coaxially arranged on the panorama head. Therefore, eccentricity and alignment of the laser range finder against the camera must be determined in a system calibration. For the verification of the system accuracy and the system calibration, the laser points were determined from measurements with total stations. The differences to the reference were 4-5mm for individual coordinates.","<b>Authors:</b><br/>Kersten T.P., Stallmann D., Tschirschwitz F. <br/><b>Key words:</b><br/>3d, Calibration, Camera, Modelling, Panorama, Reconstruction, Sparse point cloud"
91,paper_91,"Frank S., Labonnote N.",,"The objective of this study is to provide an overview of current developments in monitoring technologies in buildings equipped with ambient assisted living technologies, as well as a short summary of the challenges facing the building industry and trends in future developments. A systematic mapping approach was used as a basis for describing the current state-of-the-art. An initial search of three databases provided more than 3000 articles, of which 610 were selected as relevant to the study once duplicates were removed and a screening process applied using inclusion and exclusion criteria based on titles and abstracts. Keywording was carried out in order to address research questions relevant to the study. © 2015 IEEE.","<b>Authors:</b><br/>Frank S., Labonnote N. <br/><b>Key words:</b><br/>acting, ambient assisted living, assistive technologies, monitoring, perceiving, systematic mapping approach"
92,paper_92,"Alferillo V., Inés M., Matturro G.",,"This paper presents the process and outcomes of a systematic literature mapping about the application of the Use Cases technique for software requirements specification in distributed software development environments. The purpose is to identify which methods, tools and methodologies are reported in literature as more frequently used and how the Use Case technique is applied when the project team is not collocated. © 2015 IEEE.","<b>Authors:</b><br/>Alferillo V., Inés M., Matturro G. <br/><b>Key words:</b><br/>distributed software development, requirements engineering, Systematic literature mapping, use cases"
93,paper_93,"Muthukumar R., Damodaran D.",,"Indian Software industries are uniquely placed world over as the software solution providers due to the advantage of high calibre, good English speaking technical manpower employers. The companies enjoy a trusted status among the western companies. However India is still lacking in becoming a world leader in providing quality software solution. The software industries demonstrate compliance to specific western standards implying a mere follower rather than a leader. Countries in the South East Asia, Eastern Europe and also South America pose serious threat to Indian Companies from becoming world leaders. Opinions are floating that most serious development cannot be carried out in India owing to less than mature Quality levels of both the outsourced as well as original software. The basic strategy to demonstrate software quality is to reduce the frequency of the defects and issues and evolving mitigating methods to weaken the severity experienced by the users. In this paper an attempt is made to improve the software quality by mapping the relational objects of the software. Functional defect data was gathered for an application Software and categorised into High, Medium and Low type of defects with underlying need for urgent attention of High category ones. The number of defects generated through test cases for each object (module) is normalised and treated to form relational clusters. Dendrogram is drawn and cluster members are arrived. Ward method of maxima and minima tabulated for each of the category of defects and the mean is found. Defects by test cases is found and graded as needing primary attention to that of less significant ones. Modules contributing to the highest defect density is identified and listed. These defects are systematically analysed and root causes identified. By placing the best mitigating practices the Software Quality can be enhanced. © 2015 IEEE.","<b>Authors:</b><br/>Muthukumar R., Damodaran D. <br/><b>Key words:</b><br/>"
94,paper_94,"Verdonck M., Gailly F., De Cesare S., Poels G.",,"Ontology-driven conceptual modeling (ODCM) is still a relatively new research domain in the field of information systems and there is still much discussion on how the research in ODCM should be performed and what the focus of this research should be. Therefore, this article aims to critically survey the existing literature in order to assess the kind of research that has been performed over the years, analyze the nature of the research contributions and establish its current state of the art by positioning, evaluating and interpreting relevant research to date that is related to ODCM. To understand and identify any gaps and research opportunities, our literature study is composed of both a systematic mapping study and a systematic review study. The mapping study aims at structuring and classifying the area that is being investigated in order to give a general overview of the research that has been performed in the field. A review study on the other hand is a more thorough and rigorous inquiry and provides recommendations based on the strength of the found evidence. Our results indicate that there are several research gaps that should be addressed and we further composed several research opportunities that are possible areas for future research. © 2015 - IOS Press and the authors. All rights reserved.","<b>Authors:</b><br/>Verdonck M., Gailly F., De Cesare S., Poels G. <br/><b>Key words:</b><br/>conceptual model, ontology, Ontology-driven conceptual modeling, systematic literature mapping, systematic literature study"
95,paper_95,"Britto R., Usman M.",,"Designing and assessing learning outcomes could be a challenging activity for any Software Engineering (SE) educator. To support the process of designing and assessing SE courses, educators have been applied the cognitive domain of Bloom's taxonomy. However, to the best of our knowledge, the evidence on the usage of Bloom's taxonomy in SE higher education has not yet been systematically aggregated or reviewed. Therefore, in this paper we report the state of the art on the usage of Bloom's taxonomy in SE education, identified by conducted a systematic mapping study. As a result of the performed systematic mapping study, 26 studies were deemed as relevant. The main findings from these studies are: i) Bloom's taxonomy has mostly been applied at undergraduate level for both design and assessment of software engineering courses, ii) software construction is the leading SE subarea in which Bloom's taxonomy has been applied. The results clearly point out the usefulness of Bloom's taxonomy in the SE education context. We intend to use the results from this systematic mapping study to develop a set of guidelines to support the usage of Bloom's taxonomy cognitive levels to design and assess SE courses. © 2015 IEEE.","<b>Authors:</b><br/>Britto R., Usman M. <br/><b>Key words:</b><br/>Context, Data mining, Education, Software, Software engineering, Systematics, Taxonomy"
96,paper_96,"Reveco I., Cravero A.",,"The enterprises cannot be competitive if their business and IT strategies are not aligned. In the last years, the alignment issue was addressed in several researches and numerous methods, approaches and techniques, using GORE were proposed. This paper proposes a systematic mapping study useful to evaluate differents alignment approaches, with the aim of discovering the objectives models used in the requirements engineering stage, serving as a starting point for future research in this area. © 2003-2012 IEEE.","<b>Authors:</b><br/>Reveco I., Cravero A. <br/><b>Key words:</b><br/>Alignment, GORE, Requirements, Systematic mapping"
97,paper_97,"Bezerra R.M.M., Da Silva F.Q.B., Santana A.M., Magalhaes C.V.C., Santos R.E.S.",,"Context: Current empirical research highlight the need for replications of empirical studies because replications plays an important role in the construction of scientific knowledge. Objective: Considering the importance of replications in the consolidation of the knowledge produced in the software engineering research, this study aims to update and extend the results produced in a previous mapping study seeking to discuss the current state of the replication work of empirical studies performed in software engineering research between 2011 and 2012. Method: We applied the systematic review method to search and select published papers, to extract, and synthesize data from reported replications. Results: This study analyzed more than 7,000 articles, from which 39 articles that published replications between 2011 and 2012 were selected. Data extracted from these studies were used to update the information about the replications work in software engineering. Conclusion: The number of replications increased significantly in the period, when compared to the previous mapping study. In particular, the percentage of external replications also increased, with respect to internal ones. However, several other limitations identified in the previous mapping studies are still observed in this new set of replications. © 2015 IEEE.","<b>Authors:</b><br/>Bezerra R.M.M., Da Silva F.Q.B., Santana A.M., Magalhaes C.V.C., Santos R.E.S. <br/><b>Key words:</b><br/>Empirical Software Engineering, Replications, Systematic Mapping Study"
98,paper_98,"Carroll C., Falessi D., Forney V., Frances A., Izurieta C., Seaman C.",,"Context: Software maintenance is important to keep existing software systems functional for organizations or users that depend on that software. Goal: We aim to identify the factors, i.e., software characteristics such as code complexity, leading to maintenance problems. Method: We present a Mapping Study (MS) on controlled experiments that investigated software characteristics related to defects during maintenance. Results: The search strategy identified 78 papers, of which 9 have been included in our study, dated from 1985 to 2013, after applying our inclusion and exclusion criteria. We extracted data from these papers to identify the research methods, and the independent, dependent, blocked, and measured variables. Conclusions: Our MS results point to a weak evidence on software factors causing defects during maintenance. Stronger evidence can be developed via more controlled experiments that address multiple independent variables and hold the software objects constant. © 2015 IEEE.","<b>Authors:</b><br/>Carroll C., Falessi D., Forney V., Frances A., Izurieta C., Seaman C. <br/><b>Key words:</b><br/>controlled experiments, defects, mapping study, Software maintenance, systematic literature review"
99,paper_99,"Pacheco-Gutierrez S., Stancu A., Mustafa M., Codres E., Codres B.",,"his article presents a novel systematic methodology for the detection of interest points in 3D point clouds and its corresponding descriptors by using the information of an RGB camera and a structured-light sensor. This is achieved by fusing Speeded-Up Robust Features (SURF) in the image space, and histograms that statistically represent the relationship of three dimensional geometric data around the interest points. The SURF algorithm is implemented over an image whose pixel coordinates have a direct corresponding 3D point, thus allowing the fusion of both approaches. By combining both methodologies, it is intent to define a set of interest points whose descriptors are able to maintain the intrinsic characteristics of its constituent parts such as repeatability, distinctiveness and robustness while remaining compact and fast to compute. The detected points will be use for both, localization and mapping of mobile robots in partially unknown environments. © 2015 IEEE.","<b>Authors:</b><br/>Pacheco-Gutierrez S., Stancu A., Mustafa M., Codres E., Codres B. <br/><b>Key words:</b><br/>Computer vision, Landmark Detection and Characterization, Robotics, SLAM"
100,paper_100,"Falcão L., Ferreira W., Borges A., Nepomuceno V., Soares S., Baldassare M.T.",,"Context: Researchers perform experiments to check their proposals under controlled conditions. Thus, experiments are an important category of empirical studies and are the classical approach for identifying cause-effect relationships. Goal: Quantitatively characterize and analyze the controlled experiments in software engineering published in journal and conference proceedings in the decade from 2003 to 2013. Method: We performed a systematic mapping study that includes all full papers published at EASE, ESEM and ESEJ. A total of 731 were selected. Results: We obtained 110 papers that report controlled experiments. In these experiments we obtained quantitative data about authors and institutions, subjects, tasks, environment, replication and threats to validity. Conclusions: The main contribution of this work is the amount of experiments published in the three main venues of Empirical Software Engineering between the years 2003 to 2013. And also how these experiments are being reported and executed. © 2015 IEEE.","<b>Authors:</b><br/>Falcão L., Ferreira W., Borges A., Nepomuceno V., Soares S., Baldassare M.T. <br/><b>Key words:</b><br/>Decision support systems, Software engineering, Systematics"
101,paper_101,"Lacerda A., De Queiroz R., Barbosa M.",,"The increase in the use of mobile devices and the large amount of sensitive information that they store make them valuable targets for cybercriminals. In this work, we use the systematic mapping methodology to identify what are the main security incidents targeting mobile devices and what are their main causes. Finally, this research aims to propose some guidelines to avoid or minimize the extent of the damages provoked by these incidents. © 2015 IEEE.","<b>Authors:</b><br/>Lacerda A., De Queiroz R., Barbosa M. <br/><b>Key words:</b><br/>digital identity theft, mobile malware behaviour, privacy, systematic mapping"
102,paper_102,"Gonçales L.J., Farias K., Scholl M., Veronéz M., De Oliveira T.C.",,"Context: Model comparison plays a central role in many software engineering activities. However, a comprehensive understanding about the state-of-the-art is still required. Goal: This paper aims at classifying and performing a thematic analysis of the current literature. Method: For this, we have followed well-established empirical guidelines to define and perform a systematic mapping study. Results: Some studies (14 out of 40) provide generic model comparison techniques, rather than specific ones for UML diagrams. Conclusion: Fine-grained techniques are still required to support ever-present and complex model comparison tasks during the evolution of design models. © 2015 World Scientific Publishing Company.","<b>Authors:</b><br/>Gonçales L.J., Farias K., Scholl M., Veronéz M., De Oliveira T.C. <br/><b>Key words:</b><br/>Mapping study, Model comparison, Model matching, Model similarity"
103,paper_103,"Parkkila J., Ikonen J., Porras J.",,"[No abstract available]","<b>Authors:</b><br/>Parkkila J., Ikonen J., Porras J. <br/><b>Key words:</b><br/>Connecting video games, Connecting virtual worlds, Interoperability, Mapping study, Systematic literature review"
104,paper_104,"Guedes G., Silva C., Soares M., Castro J.",,"Dynamic Software Product Lines (DSPLs) are SPLs in which the product configuration may occur at runtime. Over the last decade, DSPL has gained the interest of researchers as a way of modelling and developing dynamically adaptive systems. We have conducted a systematic mapping to discover how variability is modelled in DSPL approaches and which information is used to guide variability binding at runtime. This paper presents the results of our systematic mapping, which can be used to identify research trends and gaps for variability management in DSPLs. © 2015 IEEE.","<b>Authors:</b><br/>Guedes G., Silva C., Soares M., Castro J. <br/><b>Key words:</b><br/>dynamic software product lines, dynamically adaptive systems, Systematic mapping, variability management"
105,paper_105,"Paiva S.L.D.C., Simao A.D.S.",,"Context: The construction of complex systems has increased the adoption of technologies that aim at automating the testing activity. Model-Based Testing (MBT) has emerged as an approach to automate the generation of high-quality test suites from behavioural models. Input/Output Transition Systems(IOTSs) have been used in MBT because they are more expressive than other formalisms. Objective: This paper focuses on methods for test generation from IOTSs, aiming at synthesizing available knowledge and identifying gaps in the existing approaches. Method: A systematic mapping was conducted, in which 84 studies were evaluated and categorized in the taxonomy of MBT approaches. Results: The results indicate most of the reported approaches apply non-deterministic algorithms to test generation which do not employ measures of coverage or quality. This scenario underscores the importance of further research into this topic. Conclusion: The evidences indicate that the generation of complete test suites is guaranteed in theory without satisfying a certain test selection criterion. This result points out the need of additional investigation in this topic. © 2015 IEEE.","<b>Authors:</b><br/>Paiva S.L.D.C., Simao A.D.S. <br/><b>Key words:</b><br/>Input/Output Transition Systems, systematic mapping study, test generation"
106,paper_106,"Flemstrom D., Sundmark D., Afzal W.",,"Vertical test reuse refers to the reuse of test cases or other test artifacts over different integration levels in the software or system engineering process. Vertical test reuse has previously been proposed for reducing test effort and improving test effectiveness, particularly for embedded system development. The goal of this study is to provide an overview of the state of the art in the field of vertical test reuse for embedded system development. For this purpose, a systematic mapping study has been performed, identifying 11 papers on vertical test reuse for embedded systems. The primary result from the mapping is a classification of published work on vertical test reuse in the embedded system domain, covering motivations for reuse, reuse techniques, test levels and reusable test artifacts considered, and to what extent the effects of reuse have been evaluated. © 2015 IEEE.","<b>Authors:</b><br/>Flemstrom D., Sundmark D., Afzal W. <br/><b>Key words:</b><br/>embedded system, systematic mapping study, vertical test reuse"
107,paper_107,"Gomes P., Cavalcante E., Maia P., Batista T., Oliveira K.",,"Systems-of-systems (SoS) represent a class of systems resulted from the interaction among independent systems that cooperate to form a larger and more complex system aiming at accomplishing global missions. An inherent characteristic of SoS is the high heterogeneity of their constituent systems, which are distributed, independent, and developed with different technologies. In addition, SoS are highly dynamic, so that their constituents are often partially known or even unknown at design time. As a consequence, these constituent systems need to be discovered, selected, and composed at runtime towards identifying the proper arrangements that contribute to the accomplishment of the global missions of the SoS. In this paper, we present the results of a systematic mapping aimed to investigate the existing approaches to discover and compose constituent systems within an SoS. Besides providing an overview of the state of the art on these topics, we shed light on important issues to be addressed by future research towards a more effective development of SoS. © 2015 IEEE.","<b>Authors:</b><br/>Gomes P., Cavalcante E., Maia P., Batista T., Oliveira K. <br/><b>Key words:</b><br/>composition, discovery, search, systematic mapping, systems-of-systems"
108,paper_108,"Souza E.F., Falbo R.A., Vijaykumar N.L.",,"A mapping study provides a broad overview of a research area in order to determine whether there is research evidence on a particular topic. Results of a systematic mapping may identify suitable areas for performing future research. In this paper, we discuss our experience in using the findings of a mapping study on Knowledge Management (KM) in Software Testing for performing a real research project, which also applied other empirical approaches. The main goals of this paper are: (i) to reinforce the importance of a systematic mapping in the conduction of a research project by discussing a real case of such application, and (ii) to present the results of our survey on the most important aspects of KM when applied to software testing. © 2015 IEEE.","<b>Authors:</b><br/>Souza E.F., Falbo R.A., Vijaykumar N.L. <br/><b>Key words:</b><br/>Knowledge Management, Mapping Study, Software Testing, Survey, Systematic Mapping"
109,paper_109,"Vargas-Enriquez J., Garcia-Mundo L., Genero M., Piattini M.",,"Gamified software is currently very popular, and it is expected that it will be widely adopted over the coming years. The social impact of gamified software will probably be very high, and we therefore believe that the assessment and improvement of gamified software quality may be necessary. The aim of this paper is to present a systematic mapping study (SMS) carried out to discover the current state of the research on software gamification quality, in order to identify gaps that merit rigorous future investigation. This SMS allowed us to select 35 papers found in five digital libraries up to April 2014. This paper summarizes the main issues of the planning and the conducting of the SMS. The main results of the data synthesis are detailed and future work is also outlined. © 2015 IEEE.","<b>Authors:</b><br/>Vargas-Enriquez J., Garcia-Mundo L., Genero M., Piattini M. <br/><b>Key words:</b><br/>gamification, gamified software quality, ISO/IEC 25010, systematic mapping study"
110,paper_110,"Bayar S., Yurdakul A.",,"In this paper, we propose a mapping algorithm called particle filter mapping (PFMAP), PFMAP is able to map task nodes onto the cores of tile-based network-on-chip (NoC) architectures, such as regular, irregular, and custom 2-D or 3-D topologies. PFMAP is inspired from systematic resampling algorithm for particle filters, in which all particles can run parallel and independently from each other. Based upon the experimental results from applying PFMAP for various real life and synthetic applications onto the different topologies and architectures, the performance of the 2-D mesh architectures in terms of communication cost increased by up to 51% for irregular topologies, and by up to 31% for custom architectures. Similarly, total travel distance obtained by PFMAP is reduced by up to 45% for custom 2-D mesh architectures. In addition to these, average clock cycles per flit and total network power are reduced by up to 17% and 15% for regular 2-D mesh architectures, respectively. Finally, communication cost is diminished by up to 34% for 3-D regular NoC architectures. © 2015 IEEE.","<b>Authors:</b><br/>Bayar S., Yurdakul A. <br/><b>Key words:</b><br/>Communication system traffic, digital signal processing, greedy algorithms, multithreading, network-on-chip, parallel algorithms, routing, System-on-chip"
111,paper_111,"Milik A.",,"Many processes require controllers with an instant response (e.g. motor control, CNC machines). A high-performance PLC can be constructed with use of programmable logic devices. A lack of custom synthesis tools disables the use of standard languages widely accepted by automation designers. The paper presents the systematic process of a PLC program synthesis to hardware structure. An input PLC program is given according to the IEC61131-3 standard. The synthesis process has been developed for implementation of a program described with the LD and SFC languages. The essential idea of synthesis process is obtaining a massively parallel operating hardware structure that significantly reduces response processing time. The PLC program is translated into originally developed dedicated graph structure that enables a wide range of optimizations. In the next step, it is mapped into a hardware structure. In order to reduce resource requirements, a strategy with resource sharing is shown, which is an original extension of general mapping concepts. Modern FPGAs are equipped with arithmetic cores dedicated for signal processing, inspiring the development of the original DSP48 block mapping strategy. It attempts to utilize all features of the block in the pipelined calculation model. The considerations are summarized with the implementation result compared against standard PLC implementation, a mutual comparison of general hardware mapping, and with the use of DSP48 units. Â© 2016 Elsevier B.V.","<b>Authors:</b><br/>Milik A. <br/><b>Key words:</b><br/>DFG, DSP48, FPGA, High level synthesis, IL, LD, Logic synthesis, PLC, Reconfigurable hardware, SFC"
112,paper_112,"De Lima Fontao A., Dos Santos R.P., Dias-Neto A.C.",,"The development of mobile applications around a central software platform has been impacting the software industry. Software solutions are collaboratively built within a dynamic market, often requiring adaptation of software development processes. This trend has been broadly studied as Software Ecosystem (SECO)-in the mobile platform domain, named as Mobile Software Ecosystem (MSECO). In this paper, we share the results of a systematic mapping study on the MSECO field that analyzed 28 papers identified as relevant to answer our research questions. The results indicate the growing interest in this research field as well as the main software platforms and trending topics. We confirmed the three main MSECOs as Android (Google), iOS (Apple) and Windows Phone (Microsoft). We observed the more investigated area is mobile app development. Finally, we highlighted the main benefits: the attracting and supporting developers, then helping developers to learn and create content and the app store that allows selling and buying processes. © 2015 IEEE.","<b>Authors:</b><br/>De Lima Fontao A., Dos Santos R.P., Dias-Neto A.C. <br/><b>Key words:</b><br/>Mobile computing, Software ecosystem, Systematic mapping study"
113,paper_113,"Lopes A.M.Z., Pedro L.Z., Isotani S., Bittencourt I.I.",,"Nowadays, with the spread of massive open online courses (MOOC) there is a significant increase in the number of students learning through the Web. Consequently, there is a growing concern about the quality of the Web-based Educational Software (WES). Despite of various mechanisms to evaluate the quality of this software, most of the quality criteria are spread in different articles found in the literature. Therefore, this work aims to conduct a Systematic Literature Mapping to identify the criteria to assess the quality of WES, and classify them according to their contribution. A total of 78 studies were analyzed and classified into four categories of quality: pedagogical, technical, organizational and social. The results contribute to support the maintenance and improvement of the quality of the WES available to the community. © 2015 IEEE.","<b>Authors:</b><br/>Lopes A.M.Z., Pedro L.Z., Isotani S., Bittencourt I.I. <br/><b>Key words:</b><br/>Quality assessment, Semantic web, Web-based educational software"
114,paper_114,"Reis R.C.D., Rodriguez C.L., Lyra K.T., Jaques P.A., Bittencourt I.I., Isotani S.",,"Affective states such as personality trait, emotion and mood are critical elements that affect group learning. Nevertheless, few studies classify and analyze the research findings of the community to explicitly show the real impact and use of affective states in CSCL environments. Thus, this paper presents the development of a systematic mapping of the literature to answer three questions: (i) What are the affective states frequently used in CSCL environments? (ii) Were affective states in CSCL environment empirically evaluated? (iii) What are the types of environments that use affective states to support group learning? To answer these questions, seven digital libraries were queried and 31 studies were deeply analyzed. As main results, we verified that: (i) 54,84% of the studies deal with 'emotions' in CSCL environments, (ii) 51,61% of the studies were empirically evaluated, and (iii) LMS (38,71%) and E-Learning systems (22,58%) are the most common types of environments that use affective states to support collaborative learning. Finally, we also identified gaps and opportunities to conduct research on adaptive/intelligent CSCL systems to support better group formation and customization of group activities using affective states. © 2015 IEEE.","<b>Authors:</b><br/>Reis R.C.D., Rodriguez C.L., Lyra K.T., Jaques P.A., Bittencourt I.I., Isotani S. <br/><b>Key words:</b><br/>Affective computing, Collaborative learning"
115,paper_115,"Saleh M., Abel M.-H., Misseri V.",,"Digital ecosystem is a concept emerged from the natural existence of business ecosystem, which in turn is taken from the concept of biological ecological systems. In this paper we aim to review the literature in the field of digital ecosystem and investigate models that support the claim that collaboration systems and social media networks, to certain extent, could be considered as digital ecosystem. For this investigation we undertake a systematic mapping study to present a wide review of primary studies on digital ecosystems, collaboration systems, and social media networks. The systematic mapping is a research methodology that gives a visual summary of the results obtained from a systematic research process. This paper mapped what is currently known about digital ecosystem. The work presented in this paper aims at investigating the similarity between collaboration systems and digital ecosystems and guide efforts for future research intended to model collaboration system as a digital ecosystem. Finally, we present our own model of a collaboration system working as a digital ecosystem based on the knowledge obtained from our investigation. © 2015 IEEE.","<b>Authors:</b><br/>Saleh M., Abel M.-H., Misseri V. <br/><b>Key words:</b><br/>"
116,paper_116,"Melo S.M., Souza S.R.S., Silva R.A., Souza P.S.L.",,"The testing of concurrent programs is very complex due to the non-determinism present in those programs. They must be subjected to a systematic testing process that assists in the identification of defects and guarantees quality. Although testing tools have been proposed to support the concurrent program testing, to the best of our knowledge, no study that concentrates all testing tools to be used as a catalog for testers is available in the literature. This paper proposes a new classification for a set of testing tools for concurrent programs, regarding attributes, such as testing technique supported, programming language, and paradigm of development. The purpose is to provide a useful categorization guide that helps testing practitioners and researchers in the selection of testing tools for concurrent programs. A systematic mapping was conducted so that studies on testing tools for concurrent programs could be identified. As a main result, we provide a catalog with 116 testing tools appropriately selected and classified, among which the following techniques were identifies with higher support were Java and C/C++. Although a large number of tools have been categoirzed, most of them are academic and only few are available on a commercial scale. The classification proposed here can contribute to the state-of-the-art of testing tools for concurrent programs and also provides information for the exchange of knowledge between academy and industry. © 2015 ACM.","<b>Authors:</b><br/>Melo S.M., Souza S.R.S., Silva R.A., Souza P.S.L. <br/><b>Key words:</b><br/>Concurrent programs, Systematic mapping, Testing tools"
117,paper_117,"Kuhrmann M., Konopka C., Nellemann P., Diebold P., Münch J.",,"Software process improvement (SPI) is around for decades: frameworks are proposed, success factors are studied, and experiences have been reported. However, the sheer mass of concepts, approaches, and standards published over the years overwhelms practitioners as well as researchers. What is out there? Are there new emerging approaches? What are open issues? Still, we struggle to answer the question for what is the current state of SPI and related research? In this paper, we present initial results from a systematic mapping study to shed light on the field of SPI and to draw conclusions for future research directions. An analysis of 635 publications draws a big picture of SPI-related research of the past 25 years. Our study shows a high number of solution proposals, experience reports, and secondary studies, but only few theories. In particular, standard SPI models like CMMI and ISO/IEC 15504 are analyzed, enhanced, and evaluated for applicability, whereas these standards are critically discussed from the perspective of SPI in small-to-medium-sized companies, which leads to new specialized frameworks. Furthermore, we find a growing interest in success factors to aid companies in conducting SPI. © 2015 ACM.","<b>Authors:</b><br/>Kuhrmann M., Konopka C., Nellemann P., Diebold P., Münch J. <br/><b>Key words:</b><br/>Software process, Software process improvement, Systematic mapping study"
118,paper_118,"Jasinski M.G., De Oliveira H.C., Sartori D.F., Berkenbrock C.D.M.",,"Social networks are used by millions of users every day and become part of our lives. However, privacy and security issues have been raised since user's sensitive information are exposed by several manners. User's sensitive information disclosure is not solved yet by current social networks. Thus several researches have been done in order to understand implications and users awareness. Different approaches and techniques are applied on every study resulting a heterogeneous set of information. In order to better understand the current state of the literature about user's privacy disclosure and security we conducted a review of published papers on IEEE, ACM and Science Direct using a systematic mapping. We evaluated 143 papers and classified 35 under our defined criteria to consider only studies with experiment on user privacy and security disclosure. Results indicate an academic preference to conduct questionnaire approaches aiming to explore user's awareness. We also notice that most of studies are limited to small groups and that research community is basically restricted to USA, Europe and Asia. © 2015 IEEE.","<b>Authors:</b><br/>Jasinski M.G., De Oliveira H.C., Sartori D.F., Berkenbrock C.D.M. <br/><b>Key words:</b><br/>disclosure, privacy, profile, security, sensitive information, social network, systematic mapping, trust"
119,paper_119,"Rafique S., Humayun M., Hamid B., Abbas A., Akhtar M., Iqbal K.",,"Number of security vulnerabilities in web application has grown with the tremendous growth of web application in last two decades. As the domain of Web Applications is maturing, large number of empirical studies has been reported in web applications to address the solution of vulnerable web application. However, before advancing towards finding new approaches of web applications security vulnerability detection, there is a need to analyze and synthesize existing evidence based studies in web applications area. To do this, we have planned to conduct a systematic mapping study to view and report the state-of-the-art of empirical work in existing research of web applications. In this paper, we aimed at providing a description of mapping study for synthesizing the reported empirical research in the area of web applications security vulnerabilities detection approaches. The proposed solutions are mapped against: (1) the software development stages for which the solution has been proposed and (2) the web application vulnerabilities mapping according to OWASP Top 10 security vulnerabilities. To do this, existing literature has been surveyed using a systematic mapping study by phrasing two research questions. In the mapping study, a total of 41 studies dating from 1994 to 2014 were evaluated and mapped against the aforementioned categories. The outcome of this mapping study is current state-of-the-art of empirical research in web application area, strength and weaknesses of existing empirical work, best practices and possible directions for future research. © 2015 IEEE.","<b>Authors:</b><br/>Rafique S., Humayun M., Hamid B., Abbas A., Akhtar M., Iqbal K. <br/><b>Key words:</b><br/>security, State-of-the-art, Systematic mapping study, vulnerability, web application"
120,paper_120,"Idri A., Abnane I., Abran A.",,"Missing Values (MV) present a serious problem facing research in software engineering (SE) which is mainly based on statistical and/or data mining analysis of SE data. The simple method of dealing with MV is to ignore data with missing observations. This leads to losing valuable information and then obtaining biased results. Therefore, various techniques have been developed to deal adequately with MV, especially those based on imputation methods. In this paper, a systematic mapping study was carried out to summarize the existing techniques dealing with MV in SE datasets and to classify the selected studies according to six classification criteria: research type, research approach, MV technique, MV type, data types and MV objective. Publication channels and trends were also identified. As results, 35 papers concerning MV treatments of SE data were selected. This study shows an increasing interest in machine learning (ML) techniques especially the K-nearest neighbor algorithm (KNN) to deal with MV in SE datasets and found that most of the MV techniques are used to serve software development effort estimation techniques. © 2015 IEEE.","<b>Authors:</b><br/>Idri A., Abnane I., Abran A. <br/><b>Key words:</b><br/>Machine learning, Missing values, Software engineering data, Systematic mapping study"
121,paper_121,"Fernández-Sánchez C., Garbajosa J., Vidal C., Yagüe A.",,"Technical debt is a metaphor referring to the consequences of weak software development. Managing technical debt is necessary in order to keep it under control, and several techniques have been developed with the goal of accomplishing this. However, available techniques have grown disperse and managers lack guidance. This paper covers this gap by providing a systematic mapping of available techniques and methods for technical debt management, covering architectural debt, and identifying existing gaps that prevent to manage technical debt efficiently. © 2015 IEEE.","<b>Authors:</b><br/>Fernández-Sánchez C., Garbajosa J., Vidal C., Yagüe A. <br/><b>Key words:</b><br/>architectural technical debt, Software architecture, systematic mapping, technical debt, technical debt management, techniques, tools"
122,paper_122,"Cavalcante E., Pereira J., Alves M.P., Maia P., Moura R., Batista T., Delicato F.C., Pires P.F.",,"The Internet of Things (IoT) is a novel paradigm relying on the interaction of smart objects (things) with each other and with physical and/or virtual resources through the Internet. Despite the recent advances that have made IoT a reality, there are several challenges to be addressed towards exploiting its full potential and promoting tangible benefits to society, environment, economy, and individual citizens. Recently, Cloud Computing has been advocated as a promising approach to tackle some of the existing challenges in IoT while leveraging its adoption and bringing new opportunities. With the combination of IoT and Cloud Computing, the cloud becomes an intermediate layer between smart objects and applications that make use of data and resources provided by these objects. On the one hand, IoT can benefit from the almost unlimited resources of Cloud Computing to implement management and composition of services related to smart objects and their provided data. On the other hand, the cloud can benefit from IoT by broadening its operation scope to deal with real-world objects. In spite of this synergy, the literature still lacks of a broad, comprehensive overview on what has been investigated on the integration of IoT and Cloud Computing and what are the open issues to be addressed in future research and development. The goal of this work is to fill this gap by systematically collecting and analyzing studies available in the literature aiming to: (i) obtain a comprehensive understanding on the integration of IoT and Cloud Computing paradigms, (ii) provide an overview of the current state of research on this topic, and (iii) identify important gaps in the existing approaches as well as promising research directions. To achieve this goal, a systematic mapping study was performed covering papers recently published in journals, conferences, and workshops, available at five relevant electronic databases. As a result, 35 studies were selected presenting strategies and solutions on how to integrate IoT and Cloud Computing as well as scenarios, research challenges, and opportunities in this context. Besides confirming the increasing interest on the integration of IoT and Cloud Computing, this paper reports the main outcomes of the performed systematic mapping by both presenting an overview of the state of the art on the investigated topic and shedding light on important challenges and potential directions to future research. © 2016.","<b>Authors:</b><br/>Cavalcante E., Pereira J., Alves M.P., Maia P., Moura R., Batista T., Delicato F.C., Pires P.F. <br/><b>Key words:</b><br/>Cloud Computing, Internet of Things, IoT, Systematic mapping"
123,paper_123,"Maplesden D., Tempero E., Hosking J., Grundy J.C.",,"Performance is a crucial attribute for most software, making performance analysis an important software engineering task. The difficulty is that modern applications are challenging to analyse for performance. Many profiling techniques used in real-world software development struggle to provide useful results when applied to large-scale object-oriented applications. There is a substantial body of research into software performance generally but currently there exists no survey of this research that would help identify approaches useful for object-oriented software. To provide such a review we performed a systematic mapping study of empirical performance analysis approaches that are applicable to object-oriented software. Using keyword searches against leading software engineering research databases and manual searches of relevant venues we identified over 5,000 related articles published since January 2000. From these we systematically selected 253 applicable articles and categorised them according to ten facets that capture the intent, implementation and evaluation of the approaches. Our mapping study results allow us to highlight the main contributions of the existing literature and identify areas where there are interesting opportunities. We also find that, despite the research including approaches specifically aimed at object-oriented software, there are significant challenges in providing actionable feedback on the performance of large-scale object-oriented applications. © 1976-2012 IEEE.","<b>Authors:</b><br/>Maplesden D., Tempero E., Hosking J., Grundy J.C. <br/><b>Key words:</b><br/>object-oriented, performance, survey, Systematic review"
124,paper_124,"Niu Y., Wang Y.",,"Background: Most existing systems that identify protein-protein interaction (PPI) in literature make decisions solely on evidence within a single sentence and ignore the rich context of PPI descriptions in large corpora. Moreover, they often suffer from the heavy burden of manual annotation. Methods: To address these problems, a new relational-similarity (RS)-based approach exploiting context in large-scale text is proposed. A basic RS model is first established to make initial predictions. Then word similarity matrices that are sensitive to the PPI identification task are constructed using a corpus-based approach. Finally, a hybrid model is developed to integrate the word similarity model with the basic RS model. Results: The experimental results show that the basic RS model achieves F-scores much higher than a baseline of random guessing on interactions (from 50.6% to 75.0%) and non-interactions (from 49.4% to 74.2%). The hybrid model further improves F-score by about 2% on interactions and 3% on non-interactions. Conclusion: The experimental evaluations conducted with PPIs in well-known databases showed the effectiveness of our approach that explores context information in PPI identification. This investigation confirmed that within the framework of relational similarity, the word similarity model relieves the data sparseness problem in similarity calculation. © 2015 Elsevier B.V.","<b>Authors:</b><br/>Niu Y., Wang Y. <br/><b>Key words:</b><br/>Biomedical text mining, Protein-protein interaction, Relational similarity model, Word similarity model"
125,paper_125,"Knutas A., Hajikhani A., Salminen J., Ikonen J., Porras J.",,"There is an increasing number of scientific articles being published, which makes tracking the state of the art more time-consuming. There are software tools available to help with systematic mapping studies in a field of science, but most of these tools are closed source and involve several manual timeconsuming steps that could be automated further. We present an open solution as a cloud-based design for bibliographic analysis that makes the research method available for a wider audience. Copyright © 2015 ACM.","<b>Authors:</b><br/>Knutas A., Hajikhani A., Salminen J., Ikonen J., Porras J. <br/><b>Key words:</b><br/>Bibliometric analysis, Citation analysis, Cloud computing, Literature review, R, Saas, Social network analysis, Systematic mapping study"
126,paper_126,"Ikonen J., Ryhänen P., Parkkila J., Knutas A.",,"This study presents a concept for linking individual physical activities and video games together to build an ecosystem to motivate players to exercise. We used a systematic mapping study to establish current state of art and a user questionnaire to understand how players feel about digital rewards from physical exercises. In addition we implemented a prototype to demonstrate the applicability. The results suggest that combining games and physical activity trackers together is technologically feasible, and there is an audience who would be willing to exercise in order to receive rewards in games.","<b>Authors:</b><br/>Ikonen J., Ryhänen P., Parkkila J., Knutas A. <br/><b>Key words:</b><br/>Application federation, Exercise motivation, Physical activity tracking, Player questionnaire, Sports monitoring, Video games"
127,paper_127,"Farshchian B.A., Dahl Y.",,"Fall risk and fall-related injuries increase with age. With an aging population, we need to have a better understanding of what solutions can help us cope with agerelated falls. Ambient and ubiquitous fall technologies engage a large research community. We wanted to map research that has been done, technology that is developed and/or applied, current major research topics, and the current knowledge gaps. We employed the systematic mapping study approach. We searched systematically for available literature where modern ICT was developed or applied. A total of 1017 relevant abstracts were analyzed based on a number of criteria such as type of intervention (e.g., fall detection), type of technology (e.g., accelerometers), type of research contributions (e.g., proof of concepts, field trial results), focus of the solution (e.g., accuracy, privacy) etc. Our findings show that existing research is largely in a proof-of-concept phase. A large variety of technology is used. Component requirements are in focus, while system requirements related to real-world deployment are seldom addressed. The focus is on monitoring and data collection, while systems for empowering users are less frequent. Fall detection is by far the largest intervention type, while preventive interventions are less frequent. We have four recommendations based on our findings: (1) more research is needed to develop ICT-based preventive and corrective interventions, (2) more research is needed to develop ICT for empowering users, (3) more research is needed to integrate component technologies into future deployable service models, and (4) more research is needed to evaluate solutions in real-world settings. © Springer-Verlag London 2015.","<b>Authors:</b><br/>Farshchian B.A., Dahl Y. <br/><b>Key words:</b><br/>Age-related falls, Independent living, Literature survey, Pervasive computing, Seniors, Systematic mapping study, Ubiquitous computing"
128,paper_128,"Labonnote N., Høyland K.",,"Objective: To provide an overview of the state of research concerning smart technologies that support independent living in buildings, as well as a deeper literature review of the specific challenges for the building industry. Methods: A systematic mapping methodology was applied in this study to provide basis knowledge on the state of the art. The initial search on three databases gave 3485 articles. Of these, 610 articles were finally selected as relevant to the study after removing the duplicates and screening by applying inclusion and exclusion criteria based on titles and abstracts. Keywording was performed in order to answer the research questions of the study. Results: Results from the systematic mapping technology allow the detection of almost mature technologies, as well as promising technologies and research gaps. Results from the deeper literature review have identified challenges and new trends specific to the building industry. Conclusions: Efforts are still needed for the building industry in order to contribute significantly to meeting societal needs. Further research: Research fields that would benefit from further research are highlighted. © 2015 Taylor & Francis","<b>Authors:</b><br/>Labonnote N., Høyland K. <br/><b>Key words:</b><br/>intelligent building, ubiquitous computing"
129,paper_129,"Sepúlveda S., Bustamante M., Cravero A.",,"Background: Given the level of technological pro- gress made today is that civic and government institutions seek to replicate the success of systems like the financial sector and banking in electronic voting systems. Objective: To make an analysis for the developed systems to conduct electronic voting from the Requirements Engineering perspective. Method: We use the methodology of systematic mapping study. Initially, we collect 240 publications and finally selecting 60 works. The Non Functional Requirements selected were Security, Performance efficiency, Reliability and Operability, which were chosen from several publications suggesting that this group was critical to the successful adoption of electronic voting systems. Results: The results show that 49 % of the proposals considered Security feature the most relevant, which is consistent with the literature. The 23% of publications addressing Performance Efficiency, revealing that the main problem today is to build a model of electronic voting that is safe and can be widely implemented dedicating a reasonable amount of resources. Conclusions: A set of recommendations was generated. Given the complexity of e-vote systems, these must be addressed holistically. © 2015 IEEE.","<b>Authors:</b><br/>Sepúlveda S., Bustamante M., Cravero A. <br/><b>Key words:</b><br/>electronic voting, Non Functional Requirements, systematic mapping study"
130,paper_130,"Cravero A., Sepulveda S.",,"The Goal-Oriented Requirements Engineering, known as GORE, is the first step in the development process of systems Data Warehouses (DW). It is based on the identification of the goal of the business that wants to analyze the executive, which enables developers to obtain the information requirements of DW through a set of transformations from the goal modeling in specifications model. This paper provides an overview of how some development proposals DW used GORE to elicit, negotiate, specify and validate information requirements. The methodology is the systematic mapping studies. © 2003-2012 IEEE.","<b>Authors:</b><br/>Cravero A., Sepulveda S. <br/><b>Key words:</b><br/>Data Warehouse, GORE, requirements, systematic mapping"
131,paper_131,"Borges A., Ferreira W., Barreiros E., Almeida A., Fonseca L., Teixeira E., Silva D., Alencar A., Soares S.",,"Context: Empirical studies are gaining recognition in the Software Engineering (SE) research community, allowing improved quality of research and accelerating the adoption of new technologies in the software market. However, empirical studies in this area are still limited. In order to foster empirical research in SE, it is essential to understand the resources available to aid these studies. Goal: Identify support mechanisms (methodology, tool, guideline, process, etc.) used to conduct empirical studies in the Empirical Software Engineering (ESE) community. Method: We performed a systematic mapping study that included all full papers published at EASE, ESEM and ESEJ since their first editions. Were selected 891 studies between 1996 and 2013. Results: A total of 375 support mechanisms were identified. We provide the full list of mechanisms and the strategies that uses them. Despite this, we identified a high number of studies that do not cite any mechanism to support their empirical strategies: 433 studies (48%). Experiment is the strategy that has more resources to support their activities. And guideline was the most used type of mechanism. Moreover we observed that the most mechanisms used as reference to empirical studies are not specific to SE area. And some mechanisms were used only in specific activities of empirical research, such as statistical and qualitative data analysis. Experiment and case studies are the strategies most applied. Conclusions: The use of empirical methods in SE has increased over the years. Despite this, many studies did not apply these methods and do not cite any resource to guide their research. Therefore, the list of support mechanisms, where and how they were applied is a major asset to the SE community. Such asset can encourage empirical studies aiding the choice regarding which strategies and mechanisms to use in a research, as well as pointing out examples where they were used, mainly to novice researchers. We also identified new perspectives and gaps that foster other research for the improvement of empirical research in this area. Copyright 2015 ACM.","<b>Authors:</b><br/>Borges A., Ferreira W., Barreiros E., Almeida A., Fonseca L., Teixeira E., Silva D., Alencar A., Soares S. <br/><b>Key words:</b><br/>Empirical software engineering, Empirical strategies, Support mechanisms, Systematic mapping study"
132,paper_132,"Stevanetic S., Zdun U.",,"The main idea of software architecture is to concentrate on the \big picture"" of a software system. In the context of object-oriented software systems higher-level architectural structures or views above the level of classes are frequently used to capture the \big picture"" of the system. One of the critical aspects of these higher-level views is understand- ability, as one of their main purposes is to enable design- ers to abstract away fine-grained details. In this article we present a systematic mapping study on software metrics re- lated to the understandability concepts of such higher-level software structures with regard to their relations to the sys- tem implementation. In our systematic mapping study, we started from 3951 studies obtained using an electronic search in the four digital libraries from ACM, IEEE, Scopus, and Springer. After applying our inclusion/exclusion criteria as well as the snowballing technique we selected 268 studies for in-depth study. From those, we selected 25 studies that con- tain relevant metrics. We classify the identified studies and metrics with regard to the measured artefacts, attributes, quality characteristics, and representation model used for the metrics definitions. Additionally, we present the assess- ment of the maturity level of the identified studies. Over- all, there is a lack of maturity in the studies. We discuss possible techniques how to mitigate the identified problems. From the academic point of view we believe that our study is a good starting point for future studies aiming at improv- ing the existing works. From a practitioner's point of view, the results of our study can be used as a catalogue and an indication of the maturity of the existing research results. Copyright 2015 ACM.","<b>Authors:</b><br/>Stevanetic S., Zdun U. <br/><b>Key words:</b><br/>"
133,paper_133,"Santos A.R., De Oliveira R.P., De Almeida E.S.",,"Context. Software Product Lines (SPL) has become one of the most prominents way to promote the systematic reuse of software artifacts. Like any other piece of software, with the SPL aging, it becomes necessary to manage their evolution. However, in this process, engineers might introduce divergences among the SPL artifacts. Thus, a number of initiatives address the management of such inconsistencies. Objective. In this paper, we mapped the existing approaches to inconsistency management within SPL. Method. We used the systematic mapping study methodology. Results. We classified and performed a characterization of the approaches found, which we mangaged to arrange in three main categories. Most papers selected proposed new methods as solution research. Besides, there is still a need for validation and evaluation studies. Conclusion. We identified a lack of support for a number of activities of consistency assurance. For instance, no paper addressed the tracking of findings, decisions, and actions, as well as, few papers describing either the handling or a management policy for identified inconsistencies. Copyright 2015 ACM.","<b>Authors:</b><br/>Santos A.R., De Oliveira R.P., De Almeida E.S. <br/><b>Key words:</b><br/>Consistency checking, Literature review, Mapping study, Software product line engineering"
134,paper_134,"Ouhbi S., Idri A., Fernández-Alemán J.L., Toval A.",,"Requirements engineering (RE) has attracted a great deal of attention from researchers and practitioners in recent years. Requirements engineering education (REE) is therefore an important undertaking if the field is to have professionals who are capable of successfully accomplishing software projects. This increasing interest demands that academia should provide software engineering students with a solid foundation in the subject matter. This paper aims to identify and to present the current research on REE that is available at present, and to select useful approaches and needs for future research. A systematic mapping study was therefore performed to classify the selected studies into five classification criteria: research type, empirical type, contribution type, RE activity, and curricula. A total of 79 papers were selected and classified according to these criteria. The results of this systematic mapping study are discussed, and a list of advice obtained from the REE literature for instructors is provided. © 2013, Springer-Verlag London.","<b>Authors:</b><br/>Ouhbi S., Idri A., Fernández-Alemán J.L., Toval A. <br/><b>Key words:</b><br/>Education, Requirements engineering, Software requirements, Systematic mapping study"
135,paper_135,"Garousi Yusifo?lu V., Amannejad Y., Betin Can A.",,"Context: As a result of automated software testing, large amounts of software test code (script) are usually developed by software teams. Automated test scripts provide many benefits, such as repeatable, predictable, and efficient test executions. However, just like any software development activity, development of test scripts is tedious and error prone. We refer, in this study, to all activities that should be conducted during the entire lifecycle of test-code as Software Test-Code Engineering (STCE). Objective: As the STCE research area has matured and the number of related studies has increased, it is important to systematically categorize the current state-of-the-art and to provide an overview of the trends in this field. Such summarized and categorized results provide many benefits to the broader community. For example, they are valuable resources for new researchers (e.g., PhD students) aiming to conduct additional secondary studies. Method: In this work, we systematically classify the body of knowledge related to STCE through a systematic mapping (SM) study. As part of this study, we pose a set of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map. Results: Our study pool includes a set of 60 studies published in the area of STCE between 1999 and 2012. Our mapping data is available through an online publicly-accessible repository. We derive the trends for various aspects of STCE. Among our results are the following: (1) There is an acceptable mix of papers with respect to different contribution facets in the field of STCE and the top two leading facets are tool (68%) and method (65%). The studies that presented new processes, however, had a low rate (3%), which denotes the need for more process-related studies in this area. (2) Results of investigation about research facet of studies and comparing our result to other SM studies shows that, similar to other fields in software engineering, STCE is moving towards more rigorous validation approaches. (3) A good mixture of STCE activities has been presented in the primary studies. Among them, the two leading activities are quality assessment and co-maintenance of test-code with production code. The highest growth rate for co-maintenance activities in recent years shows the importance and challenges involved in this activity. (4) There are two main categories of quality assessment activity: detection of test smells and oracle assertion adequacy. (5) JUnit is the leading test framework which has been used in about 50% of the studies. (6) There is a good mixture of SUT types used in the studies: academic experimental systems (or simple code examples), real open-source and commercial systems. (7) Among 41 tools that are proposed for STCE, less than half of the tools (45%) were available for download. It is good to have this percentile of tools to be available, although not perfect, since the availability of tools can lead to higher impact on research community and industry. Conclusion: We discuss the emerging trends in STCE, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing STCE approaches and spot areas in the field that require more attention from the research community. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Garousi Yusifo?lu V., Amannejad Y., Betin Can A. <br/><b>Key words:</b><br/>Development of test code, Quality assessment of test code, Software test-code engineering, Study repository, Survey, Systematic mapping"
136,paper_136,"Navarro C.X., Molina A.I., Redondo M.A.",,"This paper presents the analysis of recent research on mobile learning and usability areas, applying a systematic mapping study. It also presents the main focus adopted by each publication, the types of mobile device and operative systems. The aim is also to understand the tendencies and needs in the fields of design and evaluation of the m-learning systems. Results demonstrate that research in the area has grown significantly since the 2012, and due to the recent boom of mobile devices in education, we believe that the number of studies will continue to grow in the following years. Also we identify a necessity when we see that not all the m-learning applications have used usability tests, we did not find guidelines or frameworks to evaluate them either. With this results and tendencies, we propose the creation of a model for evaluating the development phases in m-learning applications, considering pedagogical usability, student's experience and user interface usability, to improve the quality of m-learning applications. Within this proposal we have also created a framework that supports different stages of our model. © 2003-2012 IEEE.","<b>Authors:</b><br/>Navarro C.X., Molina A.I., Redondo M.A. <br/><b>Key words:</b><br/>evaluation, m-learning, mapping study, usability"
137,paper_137,"Abelein U., Paech B.",,"User participation and involvement in software development are considered to be essential for a successful software system. Three research areas, human aspects of software engineering, requirements engineering, and information systems, study these topics from various perspectives. We think it is important to analyze user participation and involvement in software engineering comprehensively to encourage further research in this area. We investigate the evidence on effects of user participation and involvement on system success and we explore which methods are available in literature. A systematic mapping study was conducted. The systematic search yielded 3,698 hits, from which we identified 289 unique papers. These papers were reviewed by the first author based on inclusion and exclusion criteria. The second author validated the selection of papers by reviewing the reasons for exclusion and inclusion and the corresponding papers on a sample base. 58 of the 289 papers were selected (22 statistical survey and meta-study papers and 36 methods papers). Based on the empirical evidence of the surveys and meta-studies, we developed a meta-analysis of structural equation models. This overview demonstrates that most papers showed positive correlations between aspects of development processes (including user participation) and human aspects (including user involvement) and system success. The analysis of the proposed solutions from the method papers revealed a wide variety of user participation and involvement practices for most activities within software development. © 2015 Springer Science+Business Media New York","<b>Authors:</b><br/>Abelein U., Paech B. <br/><b>Key words:</b><br/>Literature review, Meta analysis, Software development, Systematic mapping study, User involvement, User participation"
138,paper_138,"Rodríguez P., Haghighatkhah A., Lwakatare L.E., Teppola S., Suomalainen T., Eskeli J., Karvonen T., Kuvaja P., Verner J.M., Oivo M.",,"The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies. © 2016 Elsevier Inc.","<b>Authors:</b><br/>Rodríguez P., Haghighatkhah A., Lwakatare L.E., Teppola S., Suomalainen T., Eskeli J., Karvonen T., Kuvaja P., Verner J.M., Oivo M. <br/><b>Key words:</b><br/>Continuous deployment, Software development, Systematic mapping study"
139,paper_139,"Petersen K., Vakkalanka S., Kuzniarz L.",,"Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.), to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings. © 2015 Elsevier B.V.","<b>Authors:</b><br/>Petersen K., Vakkalanka S., Kuzniarz L. <br/><b>Key words:</b><br/>Guidelines, Software engineering, Systematic mapping studies"
140,paper_140,"Trinkenreich B., Santos G., Barcellos M.P.",,"Background: Maturity models for IT service such as CMMI-SVC and MR-MPS-SV requires identification of critical business process and definition of relevant metrics to support decision-making, but there is no clear direction or strict suggestion about which should be those processes and metrics. Aims: We aim to identify adequate metrics to be used by organizations deploying IT service maturity models and the relationship between those metrics and processes of IT service maturity models or standards. Research questions are: (i) Which metrics are being suggested for IT service quality improvement projects? (ii) How do they relate to IT service maturity models processes? Method: We have defined and executed a systematic mapping review protocol. A specialist on systematic mapping review and IT service maturity models evaluated the protocol and its results. Results: Of 114 relevant studies, 13 addressed the research questions. All of them presented quality metrics, but none presented tools or techniques for metrics identification. Conclusions: We identified 133 metrics, 80 related to specific processes areas of service maturity models. Even being a broad result, not all models aspects were considered in this study. Copyright © 2015 SCITEPRESS - Science and Technology Publications.","<b>Authors:</b><br/>Trinkenreich B., Santos G., Barcellos M.P. <br/><b>Key words:</b><br/>IT service quality, Key performance indicator, Maturity models, Measurement, Systematic mapping study"
141,paper_141,"Koç1 H.",,"Enterprises operate in dynamically changing environments that have an influence on both business and IT areas. Capabilities have been proposed as instruments to align business and IT in such environments. One aim of enterprise modelling is a better communication between the stakeholders of an enterprise at various levels. Thus, the alignment can be facilitated by the exploitation of enterprise models as an abstraction instrument. In this respective the paper analyses the methods for capability design and development from the enterprise modelling perspective by conducting a mapping study, i.e. the level of methodological support for capability modelling is investigated. For this purpose 112 journals and 24 conference proceedings were analysed. The most important findings are that the research in capability design (i) adopts empirical research, mostly in form of case studies and surveys (ii) is mainly motivated by Resource Based View (RBV) and changing environments (iii) proposes development approaches and frameworks as solution artefacts, (iv) lately receives attention in the Information Systems Development & Tools of MIS subject classification lately, (v) provides a scarce methodological support that is mostly represented as procedures and most importantly (vi) only to some extent exploit enterprise models that could enhance stakeholder communication at various abstraction levels. © IFIP International Federation for Information Processing 2015.","<b>Authors:</b><br/>Koç1 H. <br/><b>Key words:</b><br/>Capability development, Capability modelling, Mapping study, Method, Systematic literature review"
142,paper_142,"De Magalhães C.V.C., Da Silva F.Q.B., Santos R.E.S., Suassuna M.",,"Context Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012). Objective In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research. Method We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996-2012 complemented by manual and automatic search procedures that collected articles published in 2013. Results We analyzed 37 papers reporting studies about replication published in the last 17 years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012. Conclusions Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications. © 2015 Elsevier B.V.","<b>Authors:</b><br/>De Magalhães C.V.C., Da Silva F.Q.B., Santos R.E.S., Suassuna M. <br/><b>Key words:</b><br/>Empirical studies, Experiments, Mapping study, Replications, Software engineering, Systematic literature review"
143,paper_143,"Sahinoglu M., Incki K., Aktas M.S.",,"The proliferation of mobile devices and applications has seen an unprecedented rise in recent years. Application domains of mobile systems range from personal assistants to point-of-care health informatics systems. Software development for such diverse application domains requires stringent and well defined development process. Software testing is a type of verification that is required to achieve more reliable system. Even though, Software Engineering literature contains many research studies that address challenging issues in mobile application development, we could not have identified a comprehensive literature review study on this subject. In this paper, we present a systematic mapping of the Software Verification in the field of mobile applications. We provide definitive metrics and publications about mobile application testing, which we believe will allow fellow researchers to identify gaps and research opportunities in this field. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Sahinoglu M., Incki K., Aktas M.S. <br/><b>Key words:</b><br/>Literature review, Mobile application, Software testing, Systematic mapping, Verification"
144,paper_144,"Hosseini M., Shahri A., Phalp K., Taylor J., Ali R.",,"Context: Crowdsourcing, or tapping into the power of the crowd for problem solving, has gained ever-increasing attraction since it was first introduced. Crowdsourcing has been used in different disciplines, and it is becoming well-accepted in the marketplace as a new business model which utilizes Human Intelligence Tasks (HITs). Objective: While both academia and industry have extensively delved into different aspects of crowdsourcing, there seems to be no common understanding of what crowdsourcing really means and what core and optional features it has. Also, we still lack information on the kinds and disciplines of studies conducted on crowdsourcing and how they defined it in the context of their application area. This paper will clarify this ambiguity by analysing the distribution and demographics of research in crowdsourcing and extracting taxonomy of the variability and commonality in the constructs defining the concept in the literature.Method:. We conduct a systematic mapping study and analyse 113 papers, selected via a formal process, and report and discuss the results. The study is combined by a content analysis process to extract a taxonomy of features describing crowdsourcing.Results: We extract and describe the taxonomy of features which characterize crowdsourcing in its four constituents, the crowd, the crowdsourcer, the crowdsourced task and the crowdsourcing platform. In addition, we report on different mappings between these features and the characteristics of the studied papers. We also analyse the distribution of the research using multiple criteria and draw conclusions. For example, our results show a constantly increasing interest in the area, especially in North America and a significant interest from industry. Also, we illustrate that although crowdsourcing is shown to be useful in a variety of disciplines, the research in the field of computer science still seems to be dominant in investigating it. Conclusions: This study allows forming a clear picture of the research in crowdsourcing and understanding the different features of crowdsourcing and their popularity, what type of research was conducted, where and how and by whom. The study enables researchers and practitioners to estimate the current status of the research in this new field. Our taxonomy of extracted features provides a reference model which could be used to configure crowdsourcing and also define it precisely and make design decisions on which of its variation to adopt. © 2015 Elsevier Inc.","<b>Authors:</b><br/>Hosseini M., Shahri A., Phalp K., Taylor J., Ali R. <br/><b>Key words:</b><br/>Crowdsourcing, Crowdsourcing features, Systematic mapping, Taxonomy"
145,paper_145,"Ingibergsson J.T.M., Schultz U.P., Kuhrmann M.",,"Robotics has recently seen an increasing development, and the areas addressed within robotics has extended into domains we consider safety-critical, fostering the development of standards that facilitate the development of safe robots. Safety standards describe concepts to maintain desired reactions or performance in malfunctioning systems, and influence industry regarding software development and project management. However, academia seemingly did not reach the same degree of utilisation of standards. This paper presents the findings from a systematic mapping study in which we study the state-of-the-art in developing software for safety-critical software for autonomous field robots. The purpose of the study is to identify practices used for the development of autonomous field robots and how these practices relate to available safety standards. Our findings from reviewing 49 papers show that standards, if at all, are barely used. The majority of the papers propose various solutions to achieve safety, and about half of the papers refer to non-standardised approaches that mainly address the methodical rather than the development level. The present study thus shows an emerging field still on the quest for suitable approaches to develop safety-critical software, awaiting appropriate standards for this support. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Ingibergsson J.T.M., Schultz U.P., Kuhrmann M. <br/><b>Key words:</b><br/>Autonomous field robots, Development practices, Safety, Standards, Systematic mapping study"
146,paper_146,"Lopez-Herrejon R.E., Linsbauer L., Egyed A.",,"Context Search-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. Objective The main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. Method A systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. Results The most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. Conclusions Our study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Lopez-Herrejon R.E., Linsbauer L., Egyed A. <br/><b>Key words:</b><br/>Evolutionary algorithm, Metaheuristics, Search based software engineering, Software product line, Systematic mapping study"
147,paper_147,"Griffo C., Almeida J.P.A., Guizzardi G.",,"Over the last decades, the field of legal ontologies has seen a sharp increase in the number of published papers. The literature on legal ontologies now covers a wide variety of topics and research approaches. One of these topics is legal core ontologies, which have received significant attention since the 1990s. In order to provide an up-to-date overview of this research area, this article presents a systematic mapping study of published researches on legal core ontologies. The selected papers were analyzed and categorized according to the perspective of their main contribution as well as according to the legal theories used. The study reveals that only a small number of studies use legal theories suitable to address current societal challenges.","<b>Authors:</b><br/>Griffo C., Almeida J.P.A., Guizzardi G. <br/><b>Key words:</b><br/>"
148,paper_148,"Ivan?evi? V., Lukovi? I.",,"The field of educational data mining (EDM) has been slowly expanding to embrace various graph-based approaches to interpretation and analysis of educational data. However, there is a great wealth of software tools for graph creation, visualization, and analysis, both general-purpose and domain-specific, which may discourage EDM practitioners from finding a tool suitable for their graph-related problem. For this reason, we conducted a systematic mapping study on the usage of software tools for graphs in the EDM domain. By analysing papers from the proceedings of previous EDM conferences we tried to understand how and to what end graph tools were used, as well as whether researchers faced any particular challenges in those cases. In this paper, we compile studies that relied on graph tools and provide answers to the posed questions.","<b>Authors:</b><br/>Ivan?evi? V., Lukovi? I. <br/><b>Key words:</b><br/>Educational Data Mining, Graphs, Software Tools, Systematic Mapping Study"
149,paper_149,"Heredia A., Colomo-Palacios R., De Amescua A.",,"Business models (BMs) describe how a company creates and delivers value to customers, the products or services that it offers and the compensation for them. Software companies need to be able to adopt different BMs to be successful in modern economy. Despite the number of publications on the field, there is still not a clear picture of software BMs. The purpose of this study is to structure and characterize the state of the art on software BMs with focus on sales and distribution models to help discover possible research gaps. The authors of this study conducted a systematic mapping study using relevant keywords to identify primary studies in the existing literature related to software BMs from a business management perspective. The search strategy returned 1871 papers and 51 were selected as primary studies. The analysis of results helps clarify the picture of software BMs and highlights the most relevant sources of papers. Results also reveal the broad interest of researchers on this topic. Most of the primary studies were related to service-based BMs, and to a lesser extent on product-based or open-source-based BMs, there is also an increase in the attention of researchers towards models built around mobile apps. While many authors report experience papers, only some authors validate or evaluate new proposals of sales and distribution models. © 2015 The Authors. Published by Elsevier B.V.","<b>Authors:</b><br/>Heredia A., Colomo-Palacios R., De Amescua A. <br/><b>Key words:</b><br/>mobile apps, on-premise software, open-source software, SaaS, software business models, systematic mapping"
150,paper_150,"Kabbedijk J., Bezemer C.-P., Jansen S., Zaidman A.",,"Software as a service is frequently offered in a multi-tenant style, where customers of the application and their end-users share resources such as software and hardware among all users, without necessarily sharing data. It is surprising that, with such a popular paradigm, little agreement exists with regard to the definition, domain, and challenges of multi-tenancy. This absence is detrimental to the research community and the industry, as it hampers progress in the domain of multi-tenancy and enables organizations and academics to wield their own definitions to further their commercial or research agendas. In this article, a systematic mapping study on multi-tenancy is described in which 761 academic papers and 371 industrial blogs are analysed. Both the industrial and academic perspective are assessed, in order to get a complete overview. The definition and topic maps provide a comprehensive overview of the domain, while the research agenda, listing four important research topics, provides a roadmap for future research efforts. © 2014 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Kabbedijk J., Bezemer C.-P., Jansen S., Zaidman A. <br/><b>Key words:</b><br/>Definition, Multi-tenancy, Systematic mapping study"
151,paper_151,"Li Z., Avgeriou P., Liang P.",,"Context: Technical debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefit but may hurt the long-term health of a software system. Objective: This work aims at collecting studies on TD and TD management (TDM), and making a classification and thematic analysis on these studies, to obtain a comprehensive understanding on the TD concept and an overview on the current state of research on TDM. Method: A systematic mapping study was performed to identify and analyze research on TD and its management, covering publications between 1992 and 2013. Results: Ninety-four studies were finally selected. TD was classified into 10 types, 8 TDM activities were identified, and 29 tools for TDM were collected. Conclusions: The term ""debt"" has been used in different ways by different people, which leads to ambiguous interpretation of the term. Code-related TD and its management have gained the most attention. There is a need formore empirical studieswith high-quality evidence on thewhole TDM process and on the application of specific TDM approaches in industrial settings. Moreover, dedicated TDM tools are needed for managing various types of TD in the whole TDM process. © 2014 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Li Z., Avgeriou P., Liang P. <br/><b>Key words:</b><br/>Systematic mapping study, Technical debt, Technical debt management"
152,paper_152,"Abdelmaboud A., Jawawi D.N.A., Ghani I., Elsafi A., Kitchenham B.",,"Context: Cloud computing is a new computing technology that provides services to consumers and businesses. Due to the increasing use of these services, the quality of service (QoS) of cloud computing has become an important and essential issue since there are many open challenges which need to be addressed related to trust in cloud services. Many research issues have been proposed in QoS approaches in the cloud computing area. Objective: The aim of this study is to survey current research on QoS approaches in cloud computing in order to identify where more emphasis should be placed in both current and future research directions. Method: A systematic mapping study was performed to find the related literature, and 67 articles were selected as primary studies that are classified in relation to the focus, research type and contribution type. Result: The majority of the articles are of the validation research type (64%). Infrastructure as a service (48%) was the largest research focus area, followed by software as a service (36%). The majority of contributions concerned methods (48%), followed by models (32%). Conclusion: The results of this study confirm that QoS approaches in cloud computing have become an important topic in the cloud computing area in recent years and there remain open challenges and gaps which require future research exploration. In particular, tools, metrics and evaluation research are needed in order to provide useful and trustworthy cloud computing services that deliver appropriate QoS. Crown Copyright © 2014 Published by Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Abdelmaboud A., Jawawi D.N.A., Ghani I., Elsafi A., Kitchenham B. <br/><b>Key words:</b><br/>Cloud services, Quality of service, Systematic mapping study"
153,paper_153,"Wakil K., Jawawi D.N.A.",,"Background: Model Driven Web Engineering (MDWE) is the application of the model driven paradigm to the domain of web software development, where it is particularly helpful because of the continuous evolution of Web technologies and platforms. Objective: In this paper, we prepare a survey of primary studies on MDWE to explore current work and identify needs for future research. Method: Systematic mapping study uses for finding the most relevant studies and classification. In this study, we found 289 papers and a classification scheme divided them depending on their research focus, contribution type and research type. Results: The papers of solution proposal (20%) research type are majority. The most focused areas of MDWE appear to be: Web Applicability (31%), Molding and Notation (19%), and Services and Oriented (18%). The majority of contributions are methods (33%). Moreover, this shows MDWE as a wide, new, and active area to publications. Conclusions: Whilst additional analysis is warranted within the MDWE scope, in literature, composition mechanisms have been thoroughly discoursed. Furthermore, we have witnessed that the recurrent recommendation for Validation Research, Solution Proposal and Philosophical Papers has been done through earlier analysis.","<b>Authors:</b><br/>Wakil K., Jawawi D.N.A. <br/><b>Key words:</b><br/>MDWE, Model driven web engineering, Systematic mapping study, Web engineering"
154,paper_154,"Idri A., Amazal F.A., Abran A.",,"Context: Analogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques. Objective: The objective of this research is twofold: (1) to classify ASEE studies which primary goal is to propose new or modified ASEE techniques according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools. Method: We performed a systematic mapping of studies for which the primary goal is to develop or to improve ASEE techniques published in the period 1990-2012, and reviewed them based on an automated search of four electronic databases. Results: In total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with Fuzzy Logic (FL) or Genetic Algorithms (GA). Conclusion: Based on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Idri A., Amazal F.A., Abran A. <br/><b>Key words:</b><br/>Analogy, Case-based reasoning, Mapping study, Software development effort estimation, Systematic literature review"
155,paper_155,"Marques M.R., Quispe A., Ochoa S.F.",,"Background: Software engineering is a core subject in computing education. Today, there seems to be a consensus that teaching software engineering requires students to perform practical experiences that simulate the work in the software industry. This represents a challenge for universities and instructors, because these experiences are complex to setup and involve considerable time and effort. Although there are several experiences and proposals reported in the literature, there is no clear solution to address this challenge. Aim: Being knowledgeable about the several approaches reported in the literature for dealing with this challenge is the first step to proposing a new solution. Counting on this knowledge allows instructors to reuse lessons learned from other universities. In order to address this challenge, we conducted a systematic mapping study that intends to answer the following questions: What are the main approaches used to address the practical experiences in software engineering education? Is there an emerging tendency to address this challenge? Which software process models are used to support the practical experiences in software engineering courses? Have the universities changed the way of conducting these experiences over the years? What are the main forums to seek information on practical approaches for teaching software engineering? Method: We used a systematic mapping study to identify and classify available research papers that report the use of practical experiences in software engineering education. Results: There were 173 papers selected, analyzed and classified. The results indicate that universities have realized the value of including practical experiences as part of the software engineering teaching process. However, few proposals indicate how to address that challenge. The practical approaches identified in this study were game learning, case studies, simulation, inverted classrooms, maintenance projects, service learning, and open source development. Only one recent report on the use of traditional approaches (i.e., teaching using expositive lectures) was found. The use of a development process to support these practical experiences seems not to be a concern for software engineering instructors. Only 40% of these studies report the use of a development process to guide the process experience. The reported processes are mainly agile methods. Conferences are the most used forum to publish studies in this area (72%). One third of these studies have been published over the last five years. Conclusion: There is a clear concern for teaching software engineering involving practical experiences, and there are several initiatives exploring how to do it. The map gives us an overview of the different proposals to address this challenge, and also allows us to make some preliminary conclusions about the preferred approaches. © 2014 IEEE.","<b>Authors:</b><br/>Marques M.R., Quispe A., Ochoa S.F. <br/><b>Key words:</b><br/>software engineering education, systematic mapping"
156,paper_156,"Pereira V., Delamaro M.E.",,"Despite offering a wide variety of elements for graphical representation of models, the UML does not have a well-defined semantics. Therefore, over the years researches seek to assign some kind of formal semantics for UML. Objective: In this context, this paper seeks to bring evidence about the techniques for formalizing the UML semantics available in the literature, particularly those using temporal logic. Method: For this purpose, we conducted a systematic mapping study based on searching of major electronic databases. Results: We explored 278 studies, of which we claim 13 studies for analysis. In other words, the result shows that the overall picture defined by them is interest, because it shows that the majority of studies deal only with the formalization of one type of UML diagram. Conclusion: Summing up, we found out that State Diagram is the more formalized diagram in the studies. It is difficult to find the formalization of three or more UML diagrams, perhaps because of the difficulty in ensuring the overlap between UML elements. Furthermore, the results can provide perception of new research in the UML semantics for investigating and defining new tools/process to assist the software engineers. Copyright © 2015 SCITEPRESS - Science and Technology Publications.","<b>Authors:</b><br/>Pereira V., Delamaro M.E. <br/><b>Key words:</b><br/>Formal verification, Systematic mapping, Temporal logic, UML semantics, Unified modeling language"
157,paper_157,"Silva W., Costa Valentim N.M., Conte T.",,"With the increasing use of interactive applications, there is a need for a development with better quality and a good interaction that facilitates the use for end users, because such applications are increasingly present in daily life. Therefore, it is necessary to include usability, which is one of the important quality attributes, in the development process for obtaining good acceptance rates and, consequently, improving the quality of these applications. In this paper we present a Systematic Mapping Study (SM) that assists categorizing and summarizing technologies that have been used in order to improve usability. The results from our SM show some technologies that can help improving usability in various applications. Also, it identifies gaps that still need to be researched. We found that most technologies have been proposed for the Testing phase (67.28%) and that Web applications are the most evaluated type of application (52.65%). We also identified that few technologies assist designers improving usability in the early stages of the development process (13.50% Analysis phase and 15.95% Design phase). The results from this SM allow observing the state of the art regarding technologies that can be integrated into the development process, aimed at improving the usability of interactive applications. Copyright © 2015 SCITEPRESS Science and Technology Publications All rights reserved.","<b>Authors:</b><br/>Silva W., Costa Valentim N.M., Conte T. <br/><b>Key words:</b><br/>HCI, Human computer interaction, SE, Software engineering, Systematic mapping, Usability"
158,paper_158,"Gonçales L.J., Farias K., Scholl M., Veronez M., Oliveira T.",,"Context: Model comparison plays a central role in many software engineering activities. However, a comprehensive understanding about the state-of-art is still required. Goal: This paper, therefore, aims at classifying, identifying publication fora, and performing thematic analysis of the current literature in model comparison for creating an extensive and detailed understanding about this area, thereby determining gaps by graphing and pinpointing in which research areas and for which study types a shortage of publications still exits. Method: We have conducted a systematic mapping study to scrutinize those contributions produced over time, which research topics have most investigated, and which research methods that have been applied. For this, we have followed well-established empirical guidelines to define and apply a systematic mapping study. Results: The results are: (1) majority of studies (14 out of 40) provide generic model comparison techniques, rather than comparison techniques for UML diagrams, (2) a categorization and quantification of the current studies in a variety of dimensions, and (3) an overview of current research topics and trends.","<b>Authors:</b><br/>Gonçales L.J., Farias K., Scholl M., Veronez M., Oliveira T. <br/><b>Key words:</b><br/>Component, Mapping study, Model comparison, Model matching, Model similarity"
159,paper_159,"Riaz M., Breaux T., Williams L.",,"Context Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. Objective To characterize the research design of empirical studies exploring software pattern application involving human participants. Method We conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. Results Use of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for 'efficiency' and 'usability' are commonly used to evaluate the problem solving process. While measures for 'completeness', 'correctness' and 'quality' are commonly used to evaluate the final artifact. Overall, 'time to complete a task' is the most frequently used measure, employed in 15 studies to measure 'efficiency'. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity. Conclusion Subtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants' experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Riaz M., Breaux T., Williams L. <br/><b>Key words:</b><br/>Empirical design, Empirical evaluation, Mapping study, Software pattern, Systematic review"
160,paper_160,"Da Silva T.S., Silveira F.F., Silveira M.S., Hellmann T., Maurer F.",,"Agile User-Centered Design is an emergent and extremely important theme, but what does it exactly mean? Agile User-Centered Design is the use of user-centered design (UCD) in Agile environments. We build on previous work to provide a systematic mapping of Agile UCD publications at the two major agile and human-computer interaction (HCI) conferences. The analysis presented in this paper allows us to answer primary research questions such as: what is agile UCD, what types of HCI techniques have been used to integrate agile and UCD, what types of studies on agile UCD have been published, what types of research methods have been used in Agile UCD studies, and what benefits do these publications offer? Furthermore, we explore topics such as: who are the major authors in this field, and is the field driven by academics, practitioners, or collaborations? This paper presents our analysis of these topics in order to better structure future work in the field of Agile UCD and to provide a better understanding of what this field actually entails. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Da Silva T.S., Silveira F.F., Silveira M.S., Hellmann T., Maurer F. <br/><b>Key words:</b><br/>Agile software development, Empirical, Systematic mapping, Usability, User-centered design"
161,paper_161,"Lima P.A.L., Franco Fraga G.C.C., Dos Anjos E.G., Da Silva D.R.D.",,"Modularity is one of the most important quality attributes during system development. Its concepts are commonly used in disciplines of information technology courses, mainly in subjects as software project, software architecture, and others. However, it is notable among certain groups of students that this issue is not fully absorbed in a practical way. Although some researchers and practitioners have approach themes like this, there is still a lack of research about how modularity can be approached in IT courses. This paper presents a systematic mapping study about how the modularity is addressed in education. The main objective is to understand what are the main areas in this field and find more interesting points of research to improve the practice of modularity during IT disciplines. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Lima P.A.L., Franco Fraga G.C.C., Dos Anjos E.G., Da Silva D.R.D. <br/><b>Key words:</b><br/>IT learning, Modularity, Software engineering"
162,paper_162,"Papatheocharous E., Andersson J., Axelsson J.",,"This paper surveys work on ecosystems and open innovation of systems in the context of software engineering for embedded systems. The primary research goal is to develop a research agenda based on the topics identified within the research publications on the topic. The agenda is based on a systematic mapping study of 260 publications obtained from digital libraries and is influenced by a set of areas of interest, i.e., product lines, open source, third party, business models, open innovation, and strategy. The results from the study include analysis of the type of research conducted in the field, its origin and research contribution. The study identifies the need for more solutions to specific open innovation problems such as mapping business models to technical platforms, defining open ecosystem processes that foster open innovation, and improving how ecosystem players can leverage on tool support for open innovation. A direction for future research is also provided. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Papatheocharous E., Andersson J., Axelsson J. <br/><b>Key words:</b><br/>Embedded systems, Open innovation, Software ecosystems"
163,paper_163,"Murillo-Morera J., Quesada-López C., Jenkins M.",,"Context: Software fault prediction has been an important research topic in the software engineering field for more than 30 years. Software defect prediction models are commonly used to detect faulty software modules based on software metrics collected during the software development process. Objective: Data mining techniques and machine learning studies in the fault prediction software context are mapped and characterized. We investigated the metrics and techniques and their performance according to performance metrics studied. An analysis and synthesis of these studies is conducted. Method: A systematic mapping study has been conducted for identifying and aggregating evidence about software fault prediction. Results: About 70 studies published from January 2002 to December 2014 were identified. Top 40 studies were selected for analysis, based on the quality criteria results. The main metrics used were: Halstead, McCabe and LOC (67.14%), Halstead, McCabe and LOC + Object-Oriented (15.71%), others (17.14%). The main models were: Machine Learning(ML) (47.14%), ML + Statistical Analysis (31.42%), others (21.41%). The data sets used were: private access (35%) and public access (65%). The most frequent combination of metrics, models and techniques were: Halstead, McCabe and LOC + Random Forest, Naive Bayes, Logistic Regression and Decision Tree representing the (60%) of the analyzed studies. Conclusions: This article has identified and classified the performance of the metrics, techniques and their combinations. This will help researchers to select datasets, metrics and models based on experimental results, with the objective to generate learning schemes that allow a better prediction software failures. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Murillo-Morera J., Quesada-López C., Jenkins M. <br/><b>Key words:</b><br/>Fault prediction models, Software metrics, Software quality"
164,paper_164,"Gottardi T., Braga R.T.V.",,"In the context of a Model Driven Software Engineering research effort, it was identified the need of surveying the success cases categorized by specific domains and by general purpose development processes. However, no systematic mapping related to this specific context was found. Therefore, we have conducted a systematic mapping with two objectives. The first objective was to identify specific domains in which MDSE is successful, while the second objective was to identify what are the challenges to apply this methodology to general purpose development processes. As results, we have identified that MDSE success cases are clustered into four domains: business information systems, network system design, web software applications and embedded systems. We could only identify five studies related to general purpose approaches and their challenges. The analysis of the results indicate that MDSE application is consolidated in specific domains. A common feature identified among studies related to general purpose processes is that their authors have reported a lack of methodologies that support MDSE in software projects since the inception phase. This secondary study was also the first to be conducted using a collaborative systematic mapping tool. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Gottardi T., Braga R.T.V. <br/><b>Key words:</b><br/>"
165,paper_165,"Feloni D.F.G., Braga R.T.V.",,"A widely recognized fact is that the quality of software products is largely determined by the quality of the process used to develop them. Regardless of the number of tools/frameworks developed to aid companies to perform software process assessment (SPA) and improvement (SPI) activities, the industry often suffers with quality issues in their products. In order to evaluate the SPA and SPI research areas, a systematic mapping was performed to identify the available tools/frameworks and methodologies used in research, as well as the existing gaps in the area. As a result it was possible to identify the current trends in the SPA and SPI research areas that can be used as a guideline to future work and can contribute to the software engineering and quality community. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Feloni D.F.G., Braga R.T.V. <br/><b>Key words:</b><br/>"
166,paper_166,"Heredia A., Colomo-Palacios R., Amescua-Seco A.",,"Software professionals often face trouble when developing software products as it is a highly dynamic, knowledge-intensive complex process. The success of the software process heavily depends on the people involved, among other factors, making their education and training an interesting topic for research. The purpose of this study is to structure and characterize the state of the practice on software process education to help identify best practices and find new challenges. To do so, authors conducted a systematic mapping study to identify primary studies in the existing literature related to software process education. The analysis of results helps clarify the general characteristics of the software process education and training initiatives, the lessons learned in previous research, and the future works proposed by the authors in previous research on software process education. Copyright © by the paper's authors.","<b>Authors:</b><br/>Heredia A., Colomo-Palacios R., Amescua-Seco A. <br/><b>Key words:</b><br/>"
167,paper_167,"Pedreira O., García F., Brisaboa N., Piattini M.",,"Context: Gamification seeks for improvement of the user's engagement, motivation, and performance when carrying out a certain task, by means of incorporating game mechanics and elements, thus making that task more attractive. Much research work has studied the application of gamification in software engineering for increasing the engagement and results of developers. Objective: The objective of this paper is to carry out a systematic mapping of the field of gamification in software engineering in an attempt to characterize the state of the art of this field identifying gaps and opportunities for further research. Method: We carried out a systematic mapping with a view to finding the primary studies in the existing literature, which were later classified and analyzed according to four criteria: the software process area addressed, the gamification elements used, the type of research method followed, and the type of forum in which they were published. A subjective evaluation of the studies was also carried out to evaluate them in terms of methodology, empirical evidence, integration with the organization, and replicability. Results: As a result of the systematic mapping we found 29 primary studies, published between January 2011 and June 2014. Most of them focus on software development, and to a lesser extent, requirements, project management, and other support areas. In the main, they consider very simple gamification mechanics such as points and badges, and few provide empirical evidence of the impact of gamification. Conclusions: Existing research in the field is quite preliminary, and more research effort analyzing the impact of gamification in SE would be needed. Future research work should look at other game mechanics in addition to the basic ones and should tackle software process areas that have not been fully studied, such as requirements, project management, maintenance, or testing. Most studies share a lack of methodological support that would make their proposals replicable in other settings. The integration of gamification with an organization's existing tools is also an important challenge that needs to be taken up in this field. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Pedreira O., García F., Brisaboa N., Piattini M. <br/><b>Key words:</b><br/>Gamification, Software engineering, Systematic mapping"
168,paper_168,"Medeiros J.D.R.V., Alves D.C.P., Vasconcelos A., Silva C., Wanderley E.",,"Interest in the adoption of Agile methodologies has grown in recent years as a strategy to minimize problems in software development. However recent studies indicate high rates of failure also in projects that use agile processes. In this context, this research conducted an exploratory study to investigate how Requirements Engineering is used in projects that adopt agile methodologies. For this, a Systematic Mapping was performed and it identified the engineering requirements techniques that are running in the industry, the problems and limitations in projects that adopt agile methodologies. The low involvement of users and the constant changes of requirements were identified as the main challenges to be overcome. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Medeiros J.D.R.V., Alves D.C.P., Vasconcelos A., Silva C., Wanderley E. <br/><b>Key words:</b><br/>Agile methodologies, Requirements, Systematic mapping"
169,paper_169,"Moissa B., Gasparini I., Kemczinski A.",,"Learning Analytics (LA) is a field that aims to optimize learning through the study of dynamical processes occurring in the students' context. It covers the measurement, collection, analysis and reporting of data about students and their contexts. This study aims at surveying existing research on LA to identify approaches, topics, and needs for future research. A systematic mapping study is launched to find as much literature as possible. The 127 papers found (resulting in 116 works) are classified with respect to goals, data types, techniques, stakeholders and interventions. Despite the increasing interest in field, there are no studies relating it to the Massive Open Online Courses (MOOCs) context. The goal of this paper is twofold, first we present the systematic mapping on LA and after we analyze its findings in the MOOCs context. As results we provide an overview of LA and identify perspectives and challenges in the MOOCs context. Copyright © 2015, IGI Global.","<b>Authors:</b><br/>Moissa B., Gasparini I., Kemczinski A. <br/><b>Key words:</b><br/>Learning Analytics, Massive Open Online Courses, MOOCs, Students' Data Analysis, Survey, Systematic Mapping, Technology-Enhanced Learning, TEL"
170,paper_170,"Fernandes A.F., Cardoso J., Marcelino M.J.",,"MOOC platforms are Web-based learning environments which allow a global participation on a large scale and with free acceß. The paradigm presents itself as a new teaching trend, changing the way education can be offered and funded worldwide. Many institutions are now investing in this teaching mode. However, since it is a web-based tool, the connection performance can impact both the way learning is experienced by the student as well as the operation of the platform. The ""Quality of Experience"" (QoE) concept has been widely used to refer to how users describe a service they have used while the ""Quality of Service"" (QoS) concept deals with the technical performance parameters that are aßociated with the connection quality. This paper starts the proceß of developing a systematic mapping around MOOC platforms, and QoS and QoE concepts, aiming to provide an overview of the current state of research on these ißues.","<b>Authors:</b><br/>Fernandes A.F., Cardoso J., Marcelino M.J. <br/><b>Key words:</b><br/>MOOC, QoE, QoS, Systematic Mapping"
171,paper_171,"Salehi S., Selamat A., Fujita H.",,"Granular computing has attracted many researchers as a new and rapidly growing paradigm of information processing. In this paper, we apply systematic mapping study to classify the granular computing researches to discover relative derivations to specify its research strength and quality. Our search scope is limited to the Science Direct and IEEE Transactions papers published between January 2012 and August 2014. We defined four perspectives of classification schemes to map the selected studies that are focus area, contribution type, research type and framework. Results of mapping the selected studies show that almost half of the research focused area belongs to category of data analysis. In addition, most of the selected papers belong to proposing the solutions in research type scheme. Distribution of papers between tool, method and enhancement categories of contribution type are almost equal. Moreover, 39% of the relevant papers belong to the rough set framework. The results show that there is little attention paid to cluster analysis in existing frameworks to discover granules for classification. We applied five clustering algorithms on three datasets from UCI repository to compare the form of information granules, and then classify the patterns and define them to a specific class based on their geometry and belongings. The clustering algorithms are DBSCAN, c-means, k-means, GAk-means and Fuzzy-GrC and the comparison of information granules are based on the coverage, misclassification and accuracy. The survey of experimental results mostly shows Fuzzy-GrC and GAk-means algorithm superior to other clustering algorithms, while, c-means clustering algorithm shows inferior to other clustering algorithms. © 2015 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Salehi S., Selamat A., Fujita H. <br/><b>Key words:</b><br/>c-means, Clustering algorithm, DBSCAN, Granular classifier, Granular computing, k-means, Systematic mapping"
172,paper_172,"Ouhbi S., Idri A., Fer?andez-Alemán J.L., Toval A.",,"Predicting software product quality (SPQ) is becoming a permanent concern during software life cycle phases. In this paper, a systematic mapping study was performed to summarize the existing SPQ prediction (SPQP) approaches in literature and to organize the selected studies according to seven classification criteria: SPQP approaches, research types, empirical types, data sets used in the empirical evaluation of these studies, artifacts, SQ models, and SQ characteristics. Publication channels and trends were also identified. After identifying 182 documents in ACM Digital Library, IEEE Xplore, ScienceDirect, SpringerLink, and Google scholar, 69 papers were selected. The results show that the main publication source of the papers identified was conference. Data mining techniques are the most frequently SPQP approaches reported in literature. Solution proposal was the main research type identified. The majority of the papers selected were history-based evaluations using existing data which were mainly obtained from open source software projects and domain specific projects. Source code was the main artifact concerned with SPQP approaches. Well-known SQ models were hardly mentioned and reliability is the SQ characteristic through which SPQP was mainly achieved. SPQP-related subject seems to need more investigation from researchers and practitioners. Moreover, SQ models and standards need to be considered more in future SPQP research.","<b>Authors:</b><br/>Ouhbi S., Idri A., Fer?andez-Alemán J.L., Toval A. <br/><b>Key words:</b><br/>Prediction, Software product quality, Systematic mapping study"
173,paper_173,"Pitangueira A.M., Maciel R.S.P., Barros M.",,"The selection and prioritization of software requirements represents an area of interest in Search-Based Software Engineering (SBSE) and its main focus is finding and selecting a set of requirements that may be part of a software release. This paper presents a systematic review and mapping that investigated, analyzed, categorized and classified the SBSE approaches that have been proposed to address software requirement selection and prioritization problems, reporting quantitative and qualitative assessment. Initially 39 papers returned from our search strategy in this area and they were analyzed by 18 previously established quality criteria. The results of this systematic review show which aspects of the requirements selection and prioritization problems were addressed by researchers, which approaches and search techniques are currently adopted to address these problems, as well as the strengths and weaknesses in this research area highlighted from the quality criteria. © 2014 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Pitangueira A.M., Maciel R.S.P., Barros M. <br/><b>Key words:</b><br/>Requirements prioritization, Requirements selection, Systematic review"
174,paper_174,"Mirza U.M., Arslan M.A., Cedersjo G., Sulaman S.M., Janneck J.W.",,"Dataflow is a natural way of modelling streaming applications, such as multimedia, networking and other signal processing applications. In order to cope with the computational and parallelism demands of such streaming applications, multiprocessor systems are replacing uniprocessor systems. Mapping and scheduling these applications on multiprocessor systems are crucial elements for efficient implementation in terms of latency, throughput, power and energy consumption etc. Performance of streaming applications running on multiprocessor systems may widely vary with mapping and scheduling strategy. This paper performs a systematic literature review of available research carried out in the area of mapping and scheduling of dataflow graphs. © 2014 IEEE.","<b>Authors:</b><br/>Mirza U.M., Arslan M.A., Cedersjo G., Sulaman S.M., Janneck J.W. <br/><b>Key words:</b><br/>"
175,paper_175,"Zhi J., Garousi-Yusifo?lu V., Sun B., Garousi G., Shahnewaz S., Ruhe G.",,"Context: Software documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. Objectives: In this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. Method: We use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. Results: We present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1-11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one ""System under Study"" (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. Conclusion: Our study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies, (2) studies involving large-scale development projects, or from large number of study participants of various organizations, (3) more industry-academia collaborations, (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost. © 2014 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Zhi J., Garousi-Yusifo?lu V., Sun B., Garousi G., Shahnewaz S., Ruhe G. <br/><b>Key words:</b><br/>Documentation benefit, Software documentation, Systematic mapping"
176,paper_176,"Pitangueira A.M., Maciel R.S.P., Barros M.",,"The selection and prioritization of software requirements represents an area of interest in Search-Based Software Engineering (SBSE) and its main focus is finding and selecting a set of requirements that may be part of a software release. This paper presents a systematic review and mapping that investigated, analyzed, categorized and classified the SBSE approaches that have been proposed to address software requirement selection and prioritization problems, reporting quantitative and qualitative assessment. Initially 39 papers returned from our search strategy in this area and they were analyzed by 18 previously established quality criteria. The results of this systematic review show which aspects of the requirements selection and prioritization problems were addressed by researchers, which approaches and search techniques are currently adopted to address these problems, as well as the strengths and weaknesses in this research area highlighted from the quality criteria. © 2014 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Pitangueira A.M., Maciel R.S.P., Barros M. <br/><b>Key words:</b><br/>Requirements prioritization, Requirements selection, Systematic review"
177,paper_177,"Eisa T.A.E., Salim N., Alzahrani S.",,"Purpose - The purpose of this paper is to analyse the state-of-the-art techniques used to detect plagiarism in terms of their limitations, features, taxonomies and processes. Design/methodology/approach - The method used to execute this study consisted of a comprehensive search for relevant literature via six online database repositories namely, IEEE xplore, ACM Digital Library, ScienceDirect, EI Compendex, Web of Science and Springer using search strings obtained from the subject of discussion. Findings - The findings revealed that existing plagiarism detection techniques require further enhancements as existing techniques are incapable of efficiently detecting plagiarised ideas, figures, tables, formulas and scanned documents. Originality/value - The contribution of this study lies in its ability to have exposed the current trends in plagiarism detection researches and identify areas where further improvements are required so as to complement the performances of existing techniques. © Emerald Group Publishing Limited.","<b>Authors:</b><br/>Eisa T.A.E., Salim N., Alzahrani S. <br/><b>Key words:</b><br/>Detection, Plagiarism, Processes, Taxonomy, Techniques"
178,paper_178,"Müller R., Zeckzer D.",,"The ongoing 2D vs. 3D research debate from information visualization also affects software visualization. There are many 2D, 3D, and combinations of 2D and 3D visualizations for software representing its structure, behavior, or evolution. This study contributes findings to this debate and presents the results of analyzing the applications of 3D in software visualization with the objectives to outline the state-of-the-art, to reveal trends, and to identify research gaps. The analysis combined a systematic mapping study to get an overview and a systematic literature review to gain deeper insights. The relevant papers were identified by three different search strategies (manual browsing, keyword, and backward search). Starting with a set of 4386 publications from the fields of information and software visualization 155 relevant papers dealing with 2D & 3D or 3D software visualizations were identified. These papers were analyzed according to dimensionality, aspect, year, evaluation method, and application of the third dimension. In a nutshell, the majority of 3D software visualizations represents the structural aspect, is either evaluated using case studies showing working examples or not evaluated at all, and applies a 2D layout using the third dimension for displaying software metrics.","<b>Authors:</b><br/>Müller R., Zeckzer D. <br/><b>Key words:</b><br/>3D, Software visualization, Systematic literature review, Systematic mapping study"
179,paper_179,"Cruz S., Da Silva F.Q.B., Capretz L.F.",,"In this article, we present a systematic mapping study of research on personality in software engineering. The goal is to plot the landscape of current published empirical and theoretical studies that deal with the role of personality in software engineering. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported studies about personality. Our search retrieved more than 19,000 articles, from which we selected 90 articles published between 1970 and 2010. Nearly 72% of the studies were published after 2002 and 83% of the studies reported empirical research findings. Data extracted from the 90 studies showed that education and pair programming were the most recurring research topics, and that MBTI was the most used test. Research related to pair programming, education, team effectiveness, software process allocation, software engineer personality characteristics, and individual performance concentrated over 88% of the studies, while team process, behavior and preferences, and leadership performance were the topics with the smallest number of studies. We conclude that the number of articles has grown in the last few years, but contradictory evidence was found that might have been caused by differences in context, research method, and versions of the tests used in the studies. While this raises a warning for practitioners that wish to use personality tests in practice, it shows several opportunities for the research community to improve and extend findings in this field. © 2014 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Cruz S., Da Silva F.Q.B., Capretz L.F. <br/><b>Key words:</b><br/>Empirical software engineering, Human factors in software engineering, Mapping study, Software psychology, Systematic literature review"
180,paper_180,"Aytekin A.I., Tüzün E., Macit Y., Tekinerdogan B.",,"Application Lifecycle Management (ALM) can be defined as an approach that is starting from the idea of a software application and continuing with development, deployment and maintenance process. The overall objective of this study is evaluating the publications about Application Lifecycle Management by using an Evidence-Based method as Systematic Mapping Study. This systematic mapping aims to investigate the publications according to type and publication year, figure out the answers of the specified research questions and make suggestions for future work. Using a well-planned review protocol, related publications seen in the digital libraries (IEEE Explorer, ACM Digital Library etc.), white papers and industrial reports from search engines were classified by publication type, study type and related ALM area. Our study shows that industrial researches about conceptual ALM and benefits of ALM tools are heavily discussed whereas academic publications and experimental researches are less published.","<b>Authors:</b><br/>Aytekin A.I., Tüzün E., Macit Y., Tekinerdogan B. <br/><b>Key words:</b><br/>Application lifecycle management, Evidence-based software engineering, Systematic mapping study"
181,paper_181,"Granda M.F., Condori-Fernandez N., Vos T.E.J., Pastor O.",,"In Model-Driven Development (MDD), defects are managed at the level of conceptual models because the other artefacts are generated from them, such as more refined models, test cases and code. Although some studies have reported on defect types at model level, there still does not exist a clear and complete overview of the defect types that occur at the abstraction level. This paper presents a systematic mapping study to identify the model defect types reported in the literature and determine how they have been detected. Among the 282 articles published in software engineering area, 28 articles were selected for analysis. A total of 226 defects were identified, classified and their results analysed. For this, an appropriate defect classification scheme was built based on appropriate dimensions for models in an MDD context. © 2015 IEEE.","<b>Authors:</b><br/>Granda M.F., Condori-Fernandez N., Vos T.E.J., Pastor O. <br/><b>Key words:</b><br/>Conceptual Schema Defects, Defect Classification Scheme, Model-Driven Development, Systematic Mapping Study"
182,paper_182,"Cartaxo B., Almeida A., Barreiros E., Saraiva J., Ferreira W., Soares S.",,"Background: It has become evident that empirical studies in software engineering (SE) have problems related to context characterization. This situation jeopardizes studies replication, result interpretation, knowledge transfer between academia and industry, and evidence integration of secondary studies. Goals: Our goals in this research are to identify and classify the mechanisms that support context characterization of empirical studies in SE. Method: A systematic mapping study with exhaustive coverage was conducted in accordance with the guidelines of evidence-based software engineering. Results: Out of 13,355 studies, 13 studies published between 1999 and 2012 were selected. Only one mechanism adopts the omnibus context approach, against 12 that follow the discrete approach. Ten studies present mechanisms to support context characterization of experiments. Only four out of the ten software engineering topics are covered by the found mechanisms. Conclusions: We found few mechanisms that support context characterization in SE. Besides, these mechanisms do not cover the specificities of many software engineering topics and empirical methods. Thus, we believe that more research to define mechanisms focused on these specificities is needed. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Cartaxo B., Almeida A., Barreiros E., Saraiva J., Ferreira W., Soares S. <br/><b>Key words:</b><br/>Context, Empirical studies, Mechanisms, Software engineering, Systematic mapping study"
183,paper_183,"El Bajta M., Idri A., Fernández-Alemán J.L., Ros J.N., Toval A.",,"Software cost estimation plays a central role in the success of software project management in the context of global software development (GSD). The importance of mastering software cost estimation may appear to be obvious. However, as regards the issue of customer satisfaction, end-users are often unsatisfied with software project management results. In this paper, a systematic mapping study (SMS) is carried out with the aim of summarising software cost estimation in the context of GSD research by answering nine mapping questions. A total, of 16 articles were selected and classified according to nine criteria: publication source, publication year, research type, research approach, contribution type, software cost estimation techniques, software cost estimation activity, cost drivers and cost estimation performances for GSD projects. The results show that the interest in estimating software cost for GSD projects has increased in recent years and reveal that conferences are the most frequently targeted publications. Most software cost estimation for GSD research has focused on theory. The dominant contribution type of software cost estimation for GSD research is that of models, while the predominant activity was identified as being software development cost. Identifying empirical solutions to address software cost estimation for GSD is a promising direction for researchers.","<b>Authors:</b><br/>El Bajta M., Idri A., Fernández-Alemán J.L., Ros J.N., Toval A. <br/><b>Key words:</b><br/>Global software development, Software cost estimation, Systematic mapping study"
184,paper_184,"García-Mireles G.A., Moraga M.Á., García F., Piattini M.",,"Enhancing product quality might be a main goal of a software process improvement initiative (SPI). Quality is, however, a complex concept, and experts recommend identifying relevant product quality characteristics to satisfy users/customers' needs. There is thus a need to understand how SPI initiatives contribute to the improvement of software product quality characteristics. This paper aims to provide an overview of an up-to-date state-of-the-art regarding initiatives that focus on promoting product quality improvement by applying SPI approaches. This goal was achieved by conducting a systematic mapping study, as a result of which we identified 74 primary papers including both theoretical (75.7%) and empirical (24.3%) papers. The main product quality characteristics addressed are security, usability and reliability. Security-related process models, on the other hand, are those most cited (53%). The empirical papers suggest that traditional process reference models, such as CMM, CMMI or ISO 9001, moderately increase product quality characteristics, these principally being maintainability and reliability. However, there is a need for more empirical research to evaluate the impact of SPI initiatives on software product quality by considering contextual factors. SPI initiatives should be more driven by performance goals related to product quality characteristics. © 2015 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>García-Mireles G.A., Moraga M.Á., García F., Piattini M. <br/><b>Key words:</b><br/>Software process improvement, Software product quality, Systematic mapping study"
185,paper_185,"Zhang G., Zhang X., Zhang W.",,"In this note, a general scheme is proposed for the high order unstable delay process with one or more positive poles, using the mirror mapping technique. The Nyquist criteria is employed to establish a systematic methodology to tune the parameter. The stabilizing parameter region could guarantee the prespecified robustness specification. In the scheme, a control law is designed based on the all-pole Padé approximated model. The unstable process was first mapped into a minimum-phase system, and the actual control is obtained by the closed-loop gain shaping algorithm (CGSA). The advantages are that one has a concise design procedure and can achieve good performance such as disturbance rejection and robustness. Finally, three highly cited examples are used to illustrate the effectiveness of the proposed method. © 2015 ISA.","<b>Authors:</b><br/>Zhang G., Zhang X., Zhang W. <br/><b>Key words:</b><br/>Closed-loop gain shaping, High order, Mirror-mapping technique, Robust synthesis, Unstable delay process"
186,paper_186,"Tauscher H., Scherer R.J.",,"A generic visualization framework should allow for the specification of arbitrary visualizations to be generated from Building Information Models. It has been shown before, how simple visualizations can be produced with mapping rule sets and how two simple visualizations can be combined into a more complex visualization using three different combination methods. Now these approaches are extended to arbitrary complex visualizations by nesting the combination methods hierarchically. Systematic analysis yields nine different nesting cases, which are evaluated and illustrated by use case examples. Copyright:© 2015 The authors.","<b>Authors:</b><br/>Tauscher H., Scherer R.J. <br/><b>Key words:</b><br/>BIM, Hierarchical nesting, Model mapping, Visualization"
187,paper_187,"Nye B.D.",,"As information and communication technology access expands in the developing world, learning technologies have the opportunity to play a growing role to enhance and supplement strained educational systems. Intelligent tutoring systems (ITS) offer strong learning gains, but are a class of technology traditionally designed for most-developed countries. Recently, closer consideration has been made to ITS targeting the developing world and to culturally-adapted ITS. This paper presents findings from a systematic literature review that focused on barriers to ITS adoption in the developing world. While ITS were the primary focus of the review, the implications likely apply to a broader range of educational technology as well. The geographical and economic landscape of tutoring publications is mapped out, to determine where tutoring systems research occurs. Next, the paper discusses challenges and promising solutions for barriers to ITS within both formal and informal settings. These barriers include student basic computing skills, hardware sharing, mobile-dominant computing, data costs, electrical reliability, internet infrastructure, language, and culture. Differences and similarities between externally-developed and locally-developed tutoring system research for the developing world are then considered. Finally, this paper concludes with some potential future directions and opportunities for research on tutoring systems and other educational technologies on the global stage. © 2014 International Artificial Intelligence in Education Society.","<b>Authors:</b><br/>Nye B.D. <br/><b>Key words:</b><br/>Barriers to adoption, Digital divide, Intelligent tutoring systems, Mobile learning, Systematic mapping study"
188,paper_188,"Levina A., Taranov S.",,"In computer science, robustness is the ability of a computer system to cope with errors during execution. Robust codes are new nonlinear systematic error detecting codes that provide uniform protection against all errors, whereas classical linear errordetection code detects only a certain class of errors. Therefore, defence by the linear codes can be ineffective in many channels and environments, when error distribution is unknown. The probability of error masking can increase depending on codeword distribution. However, mapping the most probable codewords to a predefined set can reduce the maximum of the error masking distribution. The algorithm proposed in this paper is based on the second-order wavelet decomposition of B-splines under non-uniform nets. In this paper, we propose a general approach to the algorithm construction of spline-wavelet decompositions of linear space over an arbitrary field. This approach is based on the generalization of calibration relations and functional systems, which are biorthogonal to basic systems of relevant space. The obtained results permit the construction of second-order spline-wavelet robust code. © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.","<b>Authors:</b><br/>Levina A., Taranov S. <br/><b>Key words:</b><br/>Gray mapping, Non-uniform distribution, Robust code, Spline-wavelet decomposition"
189,paper_189,"Levina A., Sergey T.",,"Robust codes are new nonlinear systematic error detecting codes that provide uniform protection against all errors, whereas classical linear error-detection code detects only a certain class of errors. Therefore, defence by the linear codes can be ineffective in many channels and environments, when error distribution is unknown. This drawback makes the linear codes vulnerable to side channel attacks. In turn, resistance of the robust code to side channel attacks can deteriorate if codeword distribution is non-uniform. The probability of error masking can increase depending on codeword distribution. However, mapping the most probable codewords to a predefined set can reduce the maximum of the error masking distribution, thus preventing attackers from using this vulnerability. In this paper, we propose a general approach to the algorithm construction of spline-wavelet decompositions of linear space over an arbitrary field. This approach is based on the generalization of calibration relations and functional systems, which are biorthogonal to basic systems of relevant space. The obtained results permit the construction of spline-wavelet robust code. The algorithm proposed in this paper is based on the second-order wavelet decomposition of B-splines under non-uniform nets. The encoding function of the obtained code construction was investigated. In this article, we prove that this encoding function is a bent-function. We also provide a proof that the proposed spline-wavelet robust code is optimal. This paper explores the characteristics of the code in the case of non-uniform codeword distribution. © 2015 IEEE.","<b>Authors:</b><br/>Levina A., Sergey T. <br/><b>Key words:</b><br/>Gray mapping, non-uniform distribution, robust code, spline-wavelet decomposition"
190,paper_190,"Fonseca V.S., Barcellos M.P., De Almeida Falbo R.",,"During software projects, it is necessary to collect, store and analyze data to support decision making at project and organizational levels. Software measurement is a key practice to process quality improvement and project management. Given the nature of measurement activities, supporting tools are essential. Different tools can be combined to support the measurement process and provide the necessary information for decision making. However, usually these tools are developed by different developers, at different points in time and without concern for integration. As a result, organizations have to deal with integration issues to allow tools communication and properly support the measurement process. This paper presents a study investigating in the literature initiatives involving tools integration to support software measurement. As a result, twelve proposals were analyzed and their characteristics are presented.","<b>Authors:</b><br/>Fonseca V.S., Barcellos M.P., De Almeida Falbo R. <br/><b>Key words:</b><br/>Integration, Interoperability, Software measurement, Software measurement process, Software measurement tools, Systematic mapping"
191,paper_191,"Carvalho D.A., Souza Neto P.A., Vargas-Solar G., Bennani N., Ghedira C.",,"This paper identifies trends and open issues regarding the use of SLA in data integration solutions on multi-cloud environments. Therefore it presents results of a Systematic Mapping [3] that analyzes the way SLA, data integration and multi-cloud environments are correlated in existing works. The main result is a classification scheme consisting of facets and dimensions namely (i) data integration environment (cloud, data warehouse, federated database, multi-cloud), (ii) data integration description (knowledge, metadata, schema), and (iii) data quality (confidentiality, privacy, security, SLA, data protection, data provenance). The proposed classification scheme is used to organize a collection of representative papers and discuss the numerical analysis about research trends in the domain. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Carvalho D.A., Souza Neto P.A., Vargas-Solar G., Bennani N., Ghedira C. <br/><b>Key words:</b><br/>Data integration, Multi-cloud environment, Service level agreement, Systematic mapping"
192,paper_192,"Allotta B., Costanzi R., Magrini M., Monni N., Moroni D., Pascali M.A., Reggiannini M., Ridolfi A., Salvetti O., Tampucci M.",,"In the framework of the ARROWS project (September 2012-August 2015), a venture funded by the European Commission, several modular Autonomous Underwater Vehicles (AUV) have been developed to the main purposes of mapping, diagnosing, cleaning, and securing underwater and coastal archaeological sites. These AUVs consist of modular mobile robots, designed and manufactured according to specific suggestions formulated by a pool of archaeologists featuring long-standing experience in the field of Underwater Cultural Heritage preservation. The vehicles are typically equipped with acoustic modems to communicate during the dive and with different payload devices to sense the environment. The selected sensors represent appealing choices to the oceanographic engineer since they provide complementary information about the surrounding environment. The main topics discussed in this paper concern (i) performing a systematic mapping of the marine seafloors, (ii) processing the output maps to detect and classify potential archaeological targets and finally (iii) developing dissemination systems with the purpose of creating virtual scenes as a photorealistic and informative representation of the surveyed underwater sites. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Allotta B., Costanzi R., Magrini M., Monni N., Moroni D., Pascali M.A., Reggiannini M., Ridolfi A., Salvetti O., Tampucci M. <br/><b>Key words:</b><br/>"
193,paper_193,"Trinkenreich B., Santos G.",,"Background: Maturity models for IT service require proper identification of critical business process and definition of relevant metrics to support decision-making, but there is no clear direction about what should be those critical business processes and metrics. Aims: This is part of a research in progress concerning the identification of adequate metrics to be used by organizations deploying IT service maturity models. We have conducted a systematic mapping study to answer: (i) What metrics are being suggested for IT service quality improvement projects? and (ii) How do they relate to IT service maturity models processes? In this paper, we aim to answer new research questions: (iii) What kind of relationship exist between processes that appear in derived metrics that include more than one process? (iv) Which of literature suggested metrics are being used by organizations? Method: We have conducted a case study in industry. Results: From relationship found between mapping study metrics, we had analysed those ones used by organization that had available data, but we could not evidence a correlation between them, even being related. However, as a result of this analysis, we had confirmed the need to evaluate IT services through multiple metrics or define metrics in a way that the same metric be able to present different aspects about IT services management, in order to provide a comprehensive approach about the organization scenario. Copyright © 2015 SCITEPRESS - Science and Technology Publications.","<b>Authors:</b><br/>Trinkenreich B., Santos G. <br/><b>Key words:</b><br/>Case study, IT service quality, Key performance indicator, Maturity models, Measurement"
194,paper_194,"Khurshid R.P., Kuchenbecker K.J.",,"A teleoperation system with high transparency enables the operator to focus on completing the task at hand instead of on controlling the robot. We previously proposed that modifying the mapping from human movement to desired robot movement might improve the transparency of teleoperators in ways similar to adding sensory feedback. Specifically, we created non-Cartesian motion mappings that correct for systematic reaching errors made by humans, so that the robot motion resembles the operators intent rather than his or her produced movement. This article presents a study that compares subjects performance on a virtual teleoperated targeting task under three different motion mappings: a Cartesian-scaling motion mapping that is typically implemented in teleoperators, a corrective variable-similarity motion mapping that is fit to aggregate data from subjects in a previous study, and a corrective variable-similarity motion mapping that is fit to calibration data collected from each subject. Twelve participants reached toward 120 targets under each of the three motion mappings with balanced random presentation order and a washout task between conditions. Subjects were able to complete the targeting task with higher accuracy in the initial direction of robot motion, at higher speeds, and with more natural and efficient reaching movements under the variable-similarity motion mappings. Subjects also overwhelmingly preferred the variable-similarity motion mappings. These results indicate that subjects experienced a higher level of transparency when using the virtual teleoperator with the variable-similarity motion mappings than with the standard Cartesian mapping. Therefore, mappings that correct for systematic errors in human motion, such as the variable-similarity motion mappings tested here, should be considered in teleoperator design. © 2015 by the Massachusetts Institute of Technology.","<b>Authors:</b><br/>Khurshid R.P., Kuchenbecker K.J. <br/><b>Key words:</b><br/>"
195,paper_195,"Zhou H., Huang Y., Huang S., Chen K., Zhang Y., Feng Z.",,"To understand the usage of TV spectrum, a comprehensive measurement is conducted in Beijing, China. The measurement consists of two parts, i.e., fixed measurement and radio environment mapping (REM). In the fixed measurement, spectrum utilization is calculated considering not only time domain utilization but also specific Chinese TV standards. Thus the spectrum utilization is 7% higher than reported previously. To study the geographical distribution of signal strength, REM is constructed for a small area in the downtown. Since traditional systematic sampling may place sample positions in inaccessible areas, a novel sampling algorithm named simulated annealing assisted electron repulsion (SAER) is proposed. Results show SAER results in smaller error in REM than systematic sampling and signal strength variation can reach 30 dB in the considered area, which means spatial spectrum access opportunities may exist for cognitive radio. © 2015 IEEE.","<b>Authors:</b><br/>Zhou H., Huang Y., Huang S., Chen K., Zhang Y., Feng Z. <br/><b>Key words:</b><br/>Cognitive Radio Networks, Radio Environment Mapping, Spectrum Measurement, TV Whitespace"
196,paper_196,"Ng W.W.Y., Lv Y., Yeung D.S., Chan P.P.K.",,"Hashing methods have become important retrieval methods for large image databases due to its fast searching time. However, current hashing methods mapping high dimensional real-valued vectors of images to low dimensional hash codes directly may not yield good performance. These methods perform dimensionality reduction together with the Hamming quantization which lead to a difficult nonlinear problem. In this work, we propose a Two-phase Mapping Hashing (TMH) method which first maps images from the high dimensional real-valued space to a high dimensional Hamming space (hash code) using the SKLSH to preserve pairwise similarity. Then, the mapping from the high dimensional Hamming space to a low dimensional Hamming space is found via a minimization of reconstruction error between the two Hamming spaces. The shorter hash code created by the TMH preserves pairwise similarity of images in the input space and yields better precision and recall rates in comparison to SKLSH. Experimental results show that the TMH outperforms the LSH, the Compressed Hashing, the ITQ and the SKLSH. © 2014 Elsevier B.V.","<b>Authors:</b><br/>Ng W.W.Y., Lv Y., Yeung D.S., Chan P.P.K. <br/><b>Key words:</b><br/>Hashing, Image retrieval"
197,paper_197,"Wittek P., Darányi S., Liu Y.-H.",,"We report work in progress on measuring forces underlying the semantic drift by comparing it with plate tectonics in geology. Based on a brief survey of energy as a key concept in machine learning, and the Aristotelian concept of potentiality vs. actuality allowing for the study of energy and dynamics in language, we propose a field approach to lexical analysis. Until evidence to the contrary, it was assumed that a classical field in physics is appropriate to model word semantics. The approach used the distributional hypothesis to statistically model word meaning.We do not address the modelling of sentence meaning here. The computability of a vector field for the indexing vocabulary of the Reuters- 21578 test collection by an emergent self-organizing map suggests that energy minima as learnables in machine learning presuppose concepts as energy minima in cognition. Our finding needs to be confirmed by a systematic evaluation. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Wittek P., Darányi S., Liu Y.-H. <br/><b>Key words:</b><br/>"
198,paper_198,"Dutra E., Santos G.",,"Background: Several problems affect software process improvement (SPI) initiatives planning and institutionalization. Past SPI initiatives provide valuable knowledge that can be used to identify risks that might affect new ones. Aims: In this article we aimed to identify risks that might affect SPI initiatives involving the adoption of software development maturity models such as CMMI-DEV and MR-MPS-SW. Method: We (i) conducted a systematic mapping study to identify potential risks sources in Brazilian SPI-related literature and (ii) performed a qualitative analysis of selected articles using coding procedures and thematic analysis. Results: From 86 articles discussing SPI-related problems we were able to identify 17 risk categories, or top-level risks, that might affect software development maturity models deployment. These 17 risk categories are decomposed on 135 first level risks. Each risk is associated to possible causes, consequences and events. Among the most critical risk categories identified are: inadequate process definition, lack of support or commitment, lack of human resources, resistance to SPI initiative and lack of knowledge. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Dutra E., Santos G. <br/><b>Key words:</b><br/>Qualitative analysis, Risk management, Software process improvement"
199,paper_199,"Rockah-Shmuel L., Tóth-Petróczy Á., Tawfik D.S.",,"Systematic mappings of the effects of protein mutations are becoming increasingly popular. Unexpectedly, these experiments often find that proteins are tolerant to most amino acid substitutions, including substitutions in positions that are highly conserved in nature. To obtain a more realistic distribution of the effects of protein mutations, we applied a laboratory drift comprising 17 rounds of random mutagenesis and selection of M.HaeIII, a DNA methyltransferase. During this drift, multiple mutations gradually accumulated. Deep sequencing of the drifted gene ensembles allowed determination of the relative effects of all possible single nucleotide mutations. Despite being averaged across many different genetic backgrounds, about 67% of all nonsynonymous, missense mutations were evidently deleterious, and an additional 16% were likely to be deleterious. In the early generations, the frequency of most deleterious mutations remained high. However, by the 17th generation, their frequency was consistently reduced, and those remaining were accepted alongside compensatory mutations. The tolerance to mutations measured in this laboratory drift correlated with sequence exchanges seen in M.HaeIIIs natural orthologs. The biophysical constraints dictating purging in nature and in this laboratory drift also seemed to overlap. Our experiment therefore provides an improved method for measuring the effects of protein mutations that more closely replicates the natural evolutionary forces, and thereby a more realistic view of the mutational space of proteins. © 2015 Rockah-Shmuel et al.","<b>Authors:</b><br/>Rockah-Shmuel L., Tóth-Petróczy Á., Tawfik D.S. <br/><b>Key words:</b><br/>"
200,paper_200,"Griffith M., Griffith O.L., Smith S.M., Ramu A., Callaway M.B., Brummett A.M., Kiwala M.J., Coffman A.C., Regier A.A., Oberkfell B.J., Sanderson G.E., Mooney T.P., Nutter N.G., Belter E.A., Du F., Lon",,"In this work, we present the Genome Modeling System (GMS), an analysis information management system capable of executing automated genome analysis pipelines at a massive scale. The GMS framework provides detailed tracking of samples and data coupled with reliable and repeatable analysis pipelines. The GMS also serves as a platform for bioinformatics development, allowing a large team to collaborate on data analysis, or an individual researcher to leverage the work of others effectively within its data management system. Rather than separating ad-hoc analysis from rigorous, reproducible pipelines, the GMS promotes systematic integration between the two. As a demonstration of the GMS, we performed an integrated analysis of whole genome, exome and transcriptome sequencing data from a breast cancer cell line (HCC1395) and matched lymphoblastoid line (HCC1395BL). These data are available for users to test the software, complete tutorials and develop novel GMS pipeline configurations. The GMS is available at. © 2015 Griffith et al.","<b>Authors:</b><br/>Griffith M., Griffith O.L., Smith S.M., Ramu A., Callaway M.B., Brummett A.M., Kiwala M.J., Coffman A.C., Regier A.A., Oberkfell B.J., Sanderson G.E., Mooney T.P., Nutter N.G., Belter E.A., Du F., Long R.L., Abbott T.E., Ferguson I.T., Morton D.L., Burnett M.M., Weible J.V., Peck J.B., Dukes A., McMichael J.F., Lolofie J.T., Derickson B.R., Hundal J., Skidmore Z.L., Ainscough B.J., Dees N.D., Schierding W.S., Kandoth C., Kim K.H., Lu C., Harris C.C., Maher N., Maher C.A., Magrini V.J., Abbott B.S., Chen K., Clark E., Das I., Fan X., Hawkins A.E., Hepler T.G., Wylie T.N., Leonard S.M., Schroeder W.E., Shi X., Carmichael L.K., Weil M.R., Wohlstadter R.W., Stiehr G., McLellan M.D., Pohl C.S., Miller C.A., Koboldt D.C., Walker J.R., Eldred J.M., Larson D.E., Dooling D.J., Ding L., Mardis E.R., Wilson R.K. <br/><b>Key words:</b><br/>"
201,paper_201,"Huijs M., Jansen S., Brinkkemper S.",,"Independent software vendors need to grow beyond their domestic markets. Software producing organizations are faced with a great number of options and opportunities on how they choose to conduct internationalization. Interestingly, efforts conducted have a high failure rate and software companies rarely succeed at first. In this paper we present a systematic mapping study and the results of 20 interviews with CEOs in the Dutch software sector. This study highlights the most important decisions made during the process of internationalization: the drivers, the process planning, market selection, and the followed market entry strategy. The choices available to the key decision makers in the right market selection and entry strategy are most strongly influenced and limited by the product architecture, characteristics of the product and company, and the level of internationalization experience located within the independent software company. The findings from this research support decision making in internationalization projects by software firms and policy makers in finding support strategies for export missions. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Huijs M., Jansen S., Brinkkemper S. <br/><b>Key words:</b><br/>"
202,paper_202,"Júnior P.A.P., Penteado R.D.",,"Background: Aspect-Oriented Requirements Engineering (AORE) is a research field that aims to provide appropriate strategies for identification, modularization and composition of crosscutting concerns (also called early-aspects). Several AORE approaches have been developed recently, with different features, strengths and limitations. Goals: the aim of this paper is threefold: (i) cataloguing existing AORE approaches based on the activities encompassed by them, (ii) describing what types of techniques have been used by these approaches for Concern Identification and Classification  a bottleneck activity, and (iii) identifying which are the most used means of publication of AORE-based studies and how it has been the progress of these studies over the years. Results: we have selected and analyzed 60 (sixty) papers and among them, we identified 38 (thirty-eight) AORE distinct approaches. Some interesting results are: (i) few approaches lead to Conflict Identification and Resolution, an activity responsible for discovering and treating the mutual influence between different concerns existing in a software, (ii) there is a lack of evaluation studies about already existing AORE approaches, (iii) the most productive research institutions on AORE in the world are located in Lancaster (UK) and Nova Lisboa (Portugal), among other. © Springer International Publishing Switzerland 2015.","<b>Authors:</b><br/>Júnior P.A.P., Penteado R.D. <br/><b>Key words:</b><br/>Aspect-oriented requirements engineering, Concern identification and classification, Crosscutting concerns, Early-Aspects, Systematic mapping"
203,paper_203,"Gianni F., Divitini M.",,"Smart cities are a popular and recognized research topic. In urban spaces, the learning factor is an important component for citizens and local communities. This paper presents a systematic mapping of the literature on smart city learning, with focus on how technology is used to enhance smart city learning. The goal is to map the state of the art and to identify gaps in current research that can prompt new research in this area. Articles were collected from various online databases and relevant journal publications, selected according to defined inclusion/exclusion criteria. Abstracts were coded based on a number of criteria, including e.g. learning goal, used technology, and theoretical approach. Following the coding process results were analyzed to identify themes. In the paper we shed light on the current understanding of smart city learning by (i) Identifying common scenarios and learning settings, (ii) publication patterns, (iii) technical features in the supporting technology, (iv) learning theories and approaches that are mostly used, and (v) adopted type of research and research methods. The mapping shows that the concept of smart city learning is growing in popularity, with increasing number of publications in this area in the last years. However, the field is rather fragmented, with very different understanding of the concept. Smart city learning is also emerging as a very complex form of learning, with different stakeholders, learning activities, and technological solutions combined in rich eco-systems. The mapping also points out two largely unexplored areas of technological support, namely the Internet of Things (IoT) and the use of city-related data.","<b>Authors:</b><br/>Gianni F., Divitini M. <br/><b>Key words:</b><br/>Behavior change, Smart-city learning, Technology"
204,paper_204,"Lopez-Herrejon R.E., Fischer S., Ramler R., Egyed A.",,"Software Product Lines (SPLs) are families of related software systems distinguished by the set of features each one provides. Over the past decades SPLs have been the subject of extensive research and application both in academia and industry. SPLs practices have proven benefits such as better product customization and reduced time to market. Testing SPLs pose additional challenges stemming from the typically large number of product variants which make it infeasible to test every single one of them. In recent years, there has been an extensive research on applying Combinatorial Interaction Testing (CIT) for SPL testing. In this paper we present the first systematic mapping study on this subject. Our research questions aim to gather information regarding the techniques that have been applied, the nature of the case studies used for their evaluation, and what phases of CIT have been addressed. Our goal is to identify common trends, gaps, and opportunities for further research and application. © 2015 IEEE.","<b>Authors:</b><br/>Lopez-Herrejon R.E., Fischer S., Ramler R., Egyed A. <br/><b>Key words:</b><br/>"
205,paper_205,"Xu Y., Liu Y., Zheng J.",,"Software visualization has been adopted to help engineers understand the design and functionality better and faster. A number of visualization techniques have been developed in the field of structure, behavior and evolution recently. However, there is little attempt to comprehensively review current state of the art for software professionals. As a consequence, this paper employs a systematic review of research literature on the visualization of code, to identify current application tasks, discuss variety effectiveness of visual representations, and sort out their relationships to improve usability. Finally, unsolved issues and future research opportunities have been discussed. Copyright © 2015 by KSI Research Inc. and Knowledge Systems Institute Graduate School.","<b>Authors:</b><br/>Xu Y., Liu Y., Zheng J. <br/><b>Key words:</b><br/>Code visualization, Mapping, Metaphor, Systematic literature review"
206,paper_206,"Santos J.A.M., Santos A.R., De Mendonça M.G.",,"Context. Researchers are increasingly resorting of secondary studies (e.g. systematic literature reviews and mapping studies) in Software Engineering. This method is strongly dependent on the source of primary studies adopted, which is a bias. We did not find guidelines or benchmarks to evaluate the sources in a systematic way. Objective. In this paper we aim to tackle the selection of electronic data sources while conducting such kind of studies evaluating the equilibrium between the volume and number of relevant papers. Method. In this sense, we proceed towards a secondary study to analyze the overlapping of three different electronic data sources. We also compared our results with other similar studies. Results. Our results show minimum overlapping and no effortless combination of electronic data sources at all. Conclusion. We conclude that researchers shall resort of completeness to work with a feasible set of papers to review. Specially in secondary studies adopting general and no standardized terms. Copyright © 2015 by the authors.","<b>Authors:</b><br/>Santos J.A.M., Santos A.R., De Mendonça M.G. <br/><b>Key words:</b><br/>"
207,paper_207,"Valsamidis S., Petasakis I., Tenidou E., Tsourgiannis L.",,"Genetically modified trees are now one more expression of human intervention in nature. As for other people is a suicidal mood at the cost of profit maximization, for others is an inevitable trend for the survival of continuing growing world population that gathers several advantages, a survey of those directly involved in the primary sector, farmers, and deemed necessary. The implementation of this research includes the completion of 100 questionnaires from farmers in the region of Evros. Then, using the widely used Excel and SPSS software packages are processed research results with the discovery of useful correlations. The results show that in the region of Evros opinions of farmers who have negative attitude to the cultivation of genetically modified forest trees is much more (70) versus those with positive (30). Both trends express their concern on different characteristics, the systematic mapping of which is attempted in this work. The intension to cultivate genetically modified trees in relation to the demographic characteristics of the farmers, the farm size, the farm size and the type of cultivation are also investigated. Copyright © 2015 for this paper by authors.","<b>Authors:</b><br/>Valsamidis S., Petasakis I., Tenidou E., Tsourgiannis L. <br/><b>Key words:</b><br/>Analysis of attitudes, Evros, Farmers, Genetically Modified Forest Trees"
208,paper_208,"Devroey X., Cordy M., Schobbens P.-Y., Legay A., Heymans P.",,"State machine formalisms equipped with hierarchy and parallelism allow to compactly model complex system behaviours. Such models can then be transformed into executable code or inputs for model-based testing and verification techniques. Generated artifacts are mostly flat descriptions of system behaviour. Flattening is thus an essential step of these transformations. To assess the importance of flattening, we have defined and applied a systematic mapping process and 30 publications were finally selected. However, it appeared that flattening is rarely the sole focus of the publications and that care devoted to the description and validation of flattening techniques varies greatly. Preliminary assessment of associated tool support indicated limited tool availability and scalability on challenging models. We see this initial investigation as a first step towards generic flattening techniques and scalable tool support, cornerstones of reliable model-based behavioural development. © 2015 IEEE.","<b>Authors:</b><br/>Devroey X., Cordy M., Schobbens P.-Y., Legay A., Heymans P. <br/><b>Key words:</b><br/>Flattening, State machine, Systematic Mapping Study"
209,paper_209,"Li Y.-T., Jhan J.-P., Rau J.-Y.",,"In recent years, Unmanned Aerial System (UAS) has been applied to collect remote sensing data for category mapping, disaster investigation, vegetation monitoring and etc. However, due to the low payload and short operation time that reduces the mobility and image collection efficiency, only small frame camera is mounted for small area mapping purpose. In this study, one nadir and four oblique view cameras composed multi-camera imaging system is mounted on a high-payload UAS, which has capability to increase the field of view (FOV) for large ground coverage area topographic mapping. To reduce the number of unknown parameters and images, the original multi images taken in the same time are mosaicked into one large frame image through a modified projective transformation (MPT) model with two systematic error correction schemes. The coefficients of MPT are estimated from an indoor camera calibration filed, which calibrates the interior orientation parameters (IOPs) of each camera and relative orientation parameters (ROPs) among four oblique cameras and nadir camera. The systematic errors are considered since the environment of camera calibration field and flight conditions are different, the scale and displacement errors during image mosaicking are thus analyzed and corrected. The experimental result shows that the internal accuracy (average length of residuals) of the mosaicked images can achieve sub-pixel accuracy. The comparison of topographic mapping accuracy among mosaicked image and original images were conducted through rigorous aerial triangulation. The results show the mosaicked images could achieve the mapping of 1/1000 topographic standard.","<b>Authors:</b><br/>Li Y.-T., Jhan J.-P., Rau J.-Y. <br/><b>Key words:</b><br/>Image mosaicking, Multi-camera imaging system, UAS"
210,paper_210,"Brown R.A., Swanson L.W.",,"Golgi (http://www.usegolgi.com) is a prototype interactive brain map of the rat brain that helps researchers intuitively interact with neuroanatomy, connectomics, and cellular and chemical architecture. The flood of -omic data urges new ways to help researchers connect discrete findings to the larger context of the nervous system. Here we explore Golgis underlying reasoning and techniques and how our design decisions balance the constraints of building both a scientifically useful and usable tool. We demonstrate how Golgi can enhance connectomic literature searches with a case study investigating a thalamocortical circuit involving the Nucleus Accumbens and we explore Golgis potential and future directions for growth in systems neuroscience and connectomics. © 2015 Brown and Swanson.","<b>Authors:</b><br/>Brown R.A., Swanson L.W. <br/><b>Key words:</b><br/>API, Brain mapping, Connectome, Interactive, Neuroanatomy"
211,paper_211,"Abelein U., Paech B., Kern M., Woydich M.",,"In previous work we showed in a systematic mapping study that there is no method to enhance user-developer communication (UDC) in the design and implementation phase of large-scale IT projects (LSI). We then defined the UDC-LSI method. It is substantial especially for newly designed methods to evaluate them within a real-world context. As it is difficult to find a company willing to apply an untested method, as a first step to full evaluation we present in this work a case study where we study the utility and acceptance of a simulated application of the UDC-LSI method. To make the simulation as real as possible we first thoroughly analyzed the as-is status of the iPeople project. Then we simulated an instantiation of the UDC-LSI method for the iPeople project and we evaluated this instantiation with project participants. The case study showed that it is possible to instantiate the method for the project under study. The evaluation confirmed a positive effect of the UDC-LSI method on system success (effectiveness), the feasibility and high acceptance of the method and a positive effort-benefit ratio (efficiency). © 2015 IEEE.","<b>Authors:</b><br/>Abelein U., Paech B., Kern M., Woydich M. <br/><b>Key words:</b><br/>Case Study, User Involvement, User Participation, User-Developer Communication"
212,paper_212,"Nascimento D.M.C., Almeida Bittencourt R., Chavez C.",,"Context: It is common practice in academia to have students work with toy projects in software engineering (SE) courses. One way to make such courses more realistic and reduce the gap between academic courses and industry needs is getting students involved in open source projects (OSP) with faculty supervision. Objective: This study aims to summarize the literature on how OSP have been used to facilitate students learning of SE. Method: A systematic mapping study was undertaken by identifying, filtering and classifying primary studies using a predefined strategy. Results: 72 papers were selected and classified. The main results were: (a) most studies focused on comprehensive SE courses, although some dealt with specific areas, (b) the most prevalent approach was the traditional project method, (c) studies general goals were: learning SE concepts and principles by using OSP, learning open source software or both, (d) most studies tried out ideas in regular courses within the curriculum, (e) in general, students had to work with predefined projects, (f) there was a balance between approaches where instructors had either inside control or no control on the activities performed by students, (g) when learning was assessed, software artefacts, reports and presentations were the main instruments used by teachers, while surveys were widely used for students self-assessment, (h) most studies were published in the last seven years. Conclusions: The resulting map gives an overview of the existing initiatives in this context and shows gaps where further research can be pursued. © 2015 Taylor & Francis.","<b>Authors:</b><br/>Nascimento D.M.C., Almeida Bittencourt R., Chavez C. <br/><b>Key words:</b><br/>open source software, software engineering education, systematic mapping study"
213,paper_213,"Louis V.R., Phalkey R., Horstick O., Ratanawong P., Wilder-Smith A., Tozan Y., Dambach P.",,"Introduction: The global spread and the increased frequency and magnitude of epidemic dengue in the last 50 years underscore the urgent need for effective tools for surveillance, prevention, and control. This review aims at providing a systematic overview of what predictors are critical and which spatial and spatio-temporal modeling approaches are useful in generating risk maps for dengue. Methods: A systematic search was undertaken, using the PubMed, Web of Science, WHOLIS, Centers for Disease Control and Prevention (CDC) and OvidSP databases for published citations, without language or time restrictions. A manual search of the titles and abstracts was carried out using predefined criteria, notably the inclusion of dengue cases. Data were extracted for pre-identified variables, including the type of predictors and the type of modeling approach used for risk mapping. Results: A wide variety of both predictors and modeling approaches was used to create dengue risk maps. No specific patterns could be identified in the combination of predictors or models across studies. The most important and commonly used predictors for the category of demographic and socio-economic variables were age, gender, education, housing conditions and level of income. Among environmental variables, precipitation and air temperature were often significant predictors. Remote sensing provided a source of varied land cover data that could act as a proxy for other predictor categories. Descriptive maps showing dengue case hotspots were useful for identifying high-risk areas. Predictive maps based on more complex methodology facilitated advanced data analysis and visualization, but their applicability in public health contexts remains to be established. Conclusions: The majority of available dengue risk maps was descriptive and based on retrospective data. Availability of resources, feasibility of acquisition, quality of data, alongside available technical expertise, determines the accuracy of dengue risk maps and their applicability to the field of public health. A large number of unknowns, including effective entomological predictors, genetic diversity of circulating viruses, population serological profile, and human mobility, continue to pose challenges and to limit the ability to produce accurate and effective risk maps, and fail to support the development of early warning systems. © Louis et al.","<b>Authors:</b><br/>Louis V.R., Phalkey R., Horstick O., Ratanawong P., Wilder-Smith A., Tozan Y., Dambach P. <br/><b>Key words:</b><br/>Dengue, Dengue control, GIS, Land cover, Prediction, Remote sensing, Risk mapping, Spatial, Surveillance, Systematic review"
214,paper_214,"Liu J., Liu D., Cheng J., Tang Y.",,"Visual simultaneous localization and mapping (VSLAM) is becoming increasingly popular in research and industry as a solution for mapping an unknown environment with moving cameras. However, classic methods such as the Extended Kalman Filter (EKF)-based VSLAM have two significant limitations: First, their robustness and accuracy drop dramatically when low frame rate cameras are used or sudden changes in camera velocity occur. Second, their dynamic models are expensive to build, or are too simple to simulate complex movements. In this paper, a novel VSLAM approach called conditional simultaneous localization and mapping (C-SLAM) is proposed in which camera state transition is derived from image data using optical flow constraints and epipolar geometry in the prediction stage. This improvement not only increases prediction accuracy but also replaces commonly used predefined dynamic models which require additional computation. Compared to classic VSLAM approaches, C-SLAM performs more accurately in prediction and has high computational efficiency, especially under conditions such as abrupt changes in camera velocity or low camera frame rate. Such advantages are supported by the experimental results and analysis presented in this paper. © 2014 Elsevier B.V.","<b>Authors:</b><br/>Liu J., Liu D., Cheng J., Tang Y. <br/><b>Key words:</b><br/>Conditional filtering, Extended Kalman filter, Low frame rate, Visual simultaneous localization and mapping"
215,paper_215,"Giardino C., Unterkalmsteiner M., Paternoster N., Gorschek T., Abrahamsson P.",,"An impressive number of new startups are launched every day as a result of growing new markets, accessible technologies, and venture capital. New ventures such as Facebook, Supercell, Linkedin, Spotify, WhatsApp, and Dropbox, to name a few, are good examples of startups that evolved into successful businesses. However, despite many successful stories, the great majority of them fail prematurely. Operating in a chaotic and rapidly evolving domain conveys new uncharted challenges for startuppers. In this study, the authors characterize their context and identify common software development startup practices. © 2014 IEEE.","<b>Authors:</b><br/>Giardino C., Unterkalmsteiner M., Paternoster N., Gorschek T., Abrahamsson P. <br/><b>Key words:</b><br/>software development, software engineering, startups, systematic mapping study"
216,paper_216,"Hyberg P., Jansson M., Ottersten B.",,"Interpolation (mapping) of data from a given antenna array onto the output of a virtual array of more suitable configuration is well known in array signal processing. This operation allows arrays of any geometry to be used with fast direction-of-arrival (DOA) estimators designed for linear arrays. Conditions for preserving DOA error variance under such mappings have been derived by several authors. However, in many cases, such as omnidirectional signal surveillance over multiple octaves, systematic mapping errors will dominate over noise effects and cause significant bias in the DOA estimates. To prevent mapping errors from unduly affecting the DOA estimates, this paper uses a geometrical interpretation of a Taylor series expansion of the DOA estimator criterion function to derive an alternative design of the mapping matrix. Verifying simulations show significant bias reduction in the DOA estimates compared with previous designs. The key feature of the proposed design is that it takes into account the orthogonality between the manifold mapping errors and certain gradients of the estimator criterion function.With the new design, mapping of narrowband signals between dissimilar array geometries over wide sectors and large frequency ranges becomes feasible. © 2004 IEEE.","<b>Authors:</b><br/>Hyberg P., Jansson M., Ottersten B. <br/><b>Key words:</b><br/>Array interpolation, Array mapping, Array preprocessing, Bias reduction"
217,paper_217,"Serrano J.F., Acuña S.T., Macías J.A.",,"Experimentation plays a major role in the development and validation of new theories in the field of human-computer interaction (HCI). In this paper, we address quantitative empirical studies, an approach that has become the standard methodology for conducting research in HCI. This research provides an in-depth review, gathering a representative and relatively extensive collection of primary studies. We aim to identify relevant research lines in HCI in which quantitative approaches have been employed in order to construct a consistent and comprehensive taxonomy of quantitative approaches and form a broad picture of the procedures, tools and techniques employed. A systematic mapping study methodology has been adopted to produce reliable results in a reproducible and agile manner. Copyright 2014 ACM.","<b>Authors:</b><br/>Serrano J.F., Acuña S.T., Macías J.A. <br/><b>Key words:</b><br/>Empirical process, Experimental study, Human-computer interaction, Review, Systematic mapping study"
218,paper_218,"Cota C.X.N., Díaz A.I.M., Duque M.A.R.",,"This paper presents the analysis of recent research on mobile learning and usability areas, applying a systematic mapping study. It also presents the main focus adopted by each publication, the types of mobile device and operative systems. The aim is also to understand the tendencies and needs in the fields of design and evaluation of the m-learning systems. Results demonstrate that research in the area has had a large growth since the 2012, and due to the recent boom of mobile devices in education, we believe that the number of studies will continue to grow in the following years. Also we identify a necessity when we see that not all the mlearning applications have used usability tests, we did not find guidelines or frameworks to evaluate them either. With this results and tendencies, we propose the creation of a model that evaluates the m-learning applications during the development stages, considering educational factors, usability and student experience, to improve the quality in use and to improve the student experience when using mobile learning. Copyright 2014 ACM.","<b>Authors:</b><br/>Cota C.X.N., Díaz A.I.M., Duque M.A.R. <br/><b>Key words:</b><br/>Aprendizaje móvil, M-learning, Mapeo sistemático, Usabilidad"
219,paper_219,"Idri A., Amazal F.a., Abran A.",,"Context: Analogy-based software development effort estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, to our knowledge, no systematic mapping has been created of ASEE studies and no review has been carried out to analyze the empirical evidence on the performance of ASEE techniques. Objective: The objective of this research is twofold: (1) to classify ASEE papers according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends, and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools. Method: We performed a systematic mapping of ASEE studies published in the period 1990-2012, and reviewed them based on an automated search of four electronic databases. Results: In total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with fuzzy logic (FL) or genetic algorithms (GA). Conclusion: Based on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Idri A., Amazal F.a., Abran A. <br/><b>Key words:</b><br/>Analogy, Case-based reasoning, Mapping study, Software development effort estimation, Systematic literature review"
220,paper_220,"Misirli A.T., Bener A.B.",,"Bayesian Networks (BN) have been used for decision making in software engineering for many years. We investigate the current status of BNs in predicting software quality in three aspects: 1) techniques used for parameter learning, 2) techniques used for structure learning, and 3) type of variables that represent BN nodes. We performed a systematic mapping study on 38 primary studies that employed BNs to predict software quality. The most popular technique for building the final structure of BNs is the use of expert knowledge with different inference algorithms. Variables in BNs are treated as categorical in more than 70% of studies. Compared to other domains, the usage of BNs is still very limited due to high dependency on expert knowledge and tools. Copyright 2014 ACM.","<b>Authors:</b><br/>Misirli A.T., Bener A.B. <br/><b>Key words:</b><br/>Applications of Bayesian networks, Software defect prediction, Systematic mapping"
221,paper_221,"Wyrzykowski R., Wozniak M., Kuczynski L.",,"The use of erasure codes could radically improve the availability of distributed storage in comparison to replication systems with similar storage and bandwidth requirements. The investigation reported in this chapter confirms the advantage of using modern heterogeneous multi/many-core architectures, especially GPUs, for the efficient implementation of the Reed-Solomon erasure codes. The chapter investigates how to map systematically the RS erasure codes onto heterogeneous multicore architectures. It starts with the Cell/B.E. architecture-an innovative solution which is significantly different from the conventional multicore architectures. The chapter focuses on extending the methods developed for the cell multicore on heterogeneous computing platforms combining general purpose multicore CPUs and modern GPU accelerators. The resulting methods allow adaptation of the underlying numerical algorithms to internal characteristics of these platforms. The investigation carried out in the chapter confirmed the advantage of using modern multicore architectures for the efficient implementation of the classic RS erasure codes. © 2014 John Wiley & Sons, Inc.","<b>Authors:</b><br/>Wyrzykowski R., Wozniak M., Kuczynski L. <br/><b>Key words:</b><br/>Cell/B.E. architecture, Heterogeneous multicore architectures, Linear algebra algorithms, Multicore GPU architectures, Reed-Solomon erasure codes, Vectorization algorithm"
222,paper_222,"Misirli A.T., Bener A.B.",,"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making. © 2014 IEEE.","<b>Authors:</b><br/>Misirli A.T., Bener A.B. <br/><b>Key words:</b><br/>Bayesian networks, Bayesian statistics, Evidence-based decision-making, post-release defects, software metrics, software reliability"
223,paper_223,"Do?an S., Betin-Can A., Garousi V.",,"Context The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field. Methods We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area. Results Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community. © 2014 Elsevier Inc.","<b>Authors:</b><br/>Do?an S., Betin-Can A., Garousi V. <br/><b>Key words:</b><br/>Systematic literature review, Testing, Web application"
224,paper_224,"Yang J., Yang J.",,"It is often the odometry accumulative error without bound after long-range movement that decreases the precision of global localization for wheeled mobile robots. Therefore, an efficient approach to odometry error modeling is proposed regarding gentle drive type mobile robots. The approximate functional expressions, between process input of odometry and non-systematic error as well as systematic error, are derived based on odometry error propagation law. Further, the odometry error model is applied to the global localization to compensate the accumulative error during long-time navigation. In addition, Because a lot of candidate poses of robots are generated in the process of monocular visual localization, particle swarm optimization is applied to acquire the optimal pose for mobile robots during global localization. The experiments denote that in spite of sacrificing a little computation time, the proposed method decreases odometry accumulative errors, and improves the global localization precision during autonomous navigation efficiently. © 2014 NSP Natural Sciences Publishing Cor.","<b>Authors:</b><br/>Yang J., Yang J. <br/><b>Key words:</b><br/>3D reconstruction, Global localization, Non-systematic error covariance, Odometry error modeling, Particle swarm optimization, Scale invariant feature transform, Simultaneous localization and mapping"
225,paper_225,"Durelli R.S., Santibanez D.S.M., Marinho B., Honda R., Delamaro M.E., Anquetil N., De Camargo V.V.",,"Background: Perhaps the most common of all software engineering activities is the modernization of software. Unfortunately, during such modernization often leaves behind artifacts that are difficult to understand for those other than its author. Thus, the Object Management Group (OMG) has defined standards in the modernization process, by creating the concept of Architecture-Driven Modernization (ADM). Nevertheless, to the best of our knowledge, there is no a systematic mapping study providing an overview of how researchers have been employing ADM. Thus, we assert that there is a need for a more systematic investigation of the topics encompassed by this research area. Objective: To describe a systematic mapping study on ADM, highlighting the main research thrusts in this field. Method: We undertook a systematic mapping study, emphasizing the most important electronic databases. Results: We identified 30 primary studies, which were classified by their contribution type, focus area, and research type. Conclusion: This systematic mapping can be seen as a valuable initial foray into ADM for those interested in doing research in this field. More specifically, our paper provides an overview of the current state of the art and future trends in software modernization area, which may serve as a road-map for researchers interested in coming up with new tools and processes to support the modernization of legacy systems. © 2014 IEEE.","<b>Authors:</b><br/>Durelli R.S., Santibanez D.S.M., Marinho B., Honda R., Delamaro M.E., Anquetil N., De Camargo V.V. <br/><b>Key words:</b><br/>ADM, Architecture-driven modernization, KDM, Knowledge discovery metamodel, Systematic mapping"
226,paper_226,"Basten D., Sunyaev A.",,"Software projects often do not meet their scheduling and budgeting targets. Inaccurate estimates are often responsible for this mismatch. This study investigates extant research on factors that affect accuracy of software development effort estimation. The purpose is to synthesize existing knowledge, propose directions for future research, and improve estimation accuracy in practice. A systematic mapping study (a comprehensive review of existing research) is conducted to identify such factors and their impact on estimation accuracy. Thirty-two factors assigned to four categories (estimation process, estimator's characteristics, project to be estimated, and external context) are identified in a variety of research studies. Although the significant impact of several factors has been shown, results are limited by the lack of insight into the extent of these impacts. Our results imply a shift in research focus and design to gather more in-depth insights. Moreover, our results emphasize the need to argue for specific design decisions to enable a better understanding of possible influences of the study design on the credibility of the results. For software developers, our results provide a useful map to check the assumptions that undergird their estimates, to build comprehensive experience databases, and to adequately staff design projects. © 2014 by the Association for Information Systems.","<b>Authors:</b><br/>Basten D., Sunyaev A. <br/><b>Key words:</b><br/>Effort estimation, Information system projects, Software development, Systematic mapping"
227,paper_227,"Santos I.S., Andrade R.M.C.C., Neto P.A.S.",,"In the Software Product Line (SPL) paradigm, the variability description is an important issue for the requirements engineering process. In this scenario, there are several approaches in the literature focusing on how to describe variability in use cases. However, to the best of our knowledge, no efforts have been made to collect and summarize the existing templates for textual use case description in the SPL paradigm. This paper addresses this gap, presenting a systematic mapping study about SPL variability description in textual use cases. We found with this mapping, nine use case templates and four different ways to describe SPL variabilities in a use case description. From these templates, only one deal with the five variability types identified and we did not find any experimental study comparing these templates in terms of ease of use or comprehensibility. © 2014 IEEE.","<b>Authors:</b><br/>Santos I.S., Andrade R.M.C.C., Neto P.A.S. <br/><b>Key words:</b><br/>software product line, systematic mapping study, use case"
228,paper_228,"Paternoster N., Giardino C., Unterkalmsteiner M., Gorschek T., Abrahamsson P.",,"Context Software startups are newly created companies with no operating history and fast in producing cutting-edge technologies. These companies develop software under highly uncertain conditions, tackling fast-growing markets under severe lack of resources. Therefore, software startups present a unique combination of characteristics which pose several challenges to software development activities. Objective This study aims to structure and analyze the literature on software development in startup companies, determining thereby the potential for technology transfer and identifying software development work practices reported by practitioners and researchers. Method We conducted a systematic mapping study, developing a classification schema, ranking the selected primary studies according their rigor and relevance, and analyzing reported software development work practices in startups. Results A total of 43 primary studies were identified and mapped, synthesizing the available evidence on software development in startups. Only 16 studies are entirely dedicated to software development in startups, of which 10 result in a weak contribution (advice and implications (6), lesson learned (3), tool (1)). Nineteen studies focus on managerial and organizational factors. Moreover, only 9 studies exhibit high scientific rigor and relevance. From the reviewed primary studies, 213 software engineering work practices were extracted, categorized and analyzed. Conclusion This mapping study provides the first systematic exploration of the state-of-art on software startup research. The existing body of knowledge is limited to a few high quality studies. Furthermore, the results indicate that software engineering work practices are chosen opportunistically, adapted and configured to provide value under the constrains imposed by the startup context. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Paternoster N., Giardino C., Unterkalmsteiner M., Gorschek T., Abrahamsson P. <br/><b>Key words:</b><br/>Software development, Startups, Systematic mapping study"
229,paper_229,"Santos R.E.S., De Magalhães C.V.C., Da Silva F.Q.B.",,"Context. A decade ago, Kitchenham, Dyb and Jørgensen argued that software engineering could benefit from an evidence-based research approach similar that that used in medicine, introducing the basis for Evidence Based Software Engineering (EBSE). Objective. Our main goal is to understand the evolution of the use of systematic reviews as the main research method in EBSE, as proposed by Kitchenham et al., by investigating primary and tertiary studies that explore any aspect, theory, or concept around the use of systematic reviews in software engineering. Method. A systematic mapping study protocol was used to find and selected studies about EBSE and systematic reviews in SE, published between 2004 and 2013. Results. We selected 52 unique papers classified as non-empirical studies (12), empirical studies (31), and tertiary studies (9). Conclusion. SLR has become an important component of software engineering research with nearly 200 unique reviews catalogued by the tertiary studies. Most important limitations are related to the industrial relevance and application of the results of reviews and the poor use of synthesis method to aggregate evidence © 2014 ACM.","<b>Authors:</b><br/>Santos R.E.S., De Magalhães C.V.C., Da Silva F.Q.B. <br/><b>Key words:</b><br/>evidence-based software engineering, mapping study, software engineering, systematic literature review"
230,paper_230,"Da Silva F.Q.B., Suassuna M., França A.C.C., Grubb A.M., Gouveia T.B., Monteiro C.V.F., Dos Santos I.E.",,"In this article, we present a systematic mapping study of replications in software engineering. The goal is to plot the landscape of current published replications of empirical studies in software engineering research. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported replications. Our search retrieved more than 16,000 articles, from which we selected 96 articles, reporting 133 replications performed between 1994 and 2010, of 72 original studies. Nearly 70 % of the replications were published after 2004 and 70 % of these studies were internal replications. The topics of software requirements, software construction, and software quality concentrated over 55 % of the replications, while software design, configuration management, and software tools and methods were the topics with the smallest number of replications. We conclude that the number of replications has grown in the last few years, but the absolute number of replications is still small, in particular considering the breadth of topics in software engineering. We still need incentives to perform external replications, better standards to report empirical studies and their replications, and collaborative research agendas that could speed up development and publication of replications. © 2012 Springer Science+Business Media, LLC.","<b>Authors:</b><br/>Da Silva F.Q.B., Suassuna M., França A.C.C., Grubb A.M., Gouveia T.B., Monteiro C.V.F., Dos Santos I.E. <br/><b>Key words:</b><br/>Empirical studies, Experiments, Mapping study, Replications, Software engineering, Systematic literature review"
231,paper_231,"Nye B.D.",,"Despite leading to strong learning outcomes, intelligent tutoring systems (ITS) have struggled to reach widescale adoption. However, recent increases in educational technology adoption are slowly leading to larger user bases. Such order-of-magnitude increases have significant research implications for the number and diversity of users. To better understand the problems and solutions that impact this transition, a review of barriers to ITS adoption was performed. This paper significantly extends a prior systematic mapping study of recent ITS literature (2009-2012) focusing on barriers in the developing world. The present study examines research published on possible barriers to adoption related to students, teachers, and school systems. The results indicate that while barriers related to students have received extensive attention, less attention has been given to barriers related to teachers and schools. Successful and innovative approaches to integrating ITS with teacher and school needs are reviewed, with consideration given to both published research papers and successful commercial systems. © 2014 Springer International Publishing Switzerland.","<b>Authors:</b><br/>Nye B.D. <br/><b>Key words:</b><br/>Barriers to Adoption, Big Data, Intelligent Tutoring Systems, ITS Architectures, Systematic Mapping Study"
232,paper_232,"Casteleyn S., Garrigós I., Mazón J.-N.",,"BACKGROUND. The term Rich Internet Applications (RIAs) is generally associated with Web applications that provide the features and functionality of traditional desktop applications. Ten years after the introduction of the term, an ample amount of research has been carried out to study various aspects of RIAs. It has thus become essential to summarize this research and provide an adequate overview. OBJECTIVE. The objective of our study is to assemble, classify, and analyze all RIA research performed in the scientific community, thus providing a consolidated overview thereof, and to identify well-established topics, trends, and open research issues. Additionally, we provide a qualitative discussion of the most interesting findings. This work therefore serves as a reference work for beginning and established RIA researchers alike, as well as for industrial actors that need an introduction in the field, or seek pointers to (a specific subset of) the state-of-the-art. METHOD. A systematic mapping study is performed in order to identify all RIA-related publications, define a classification scheme, and categorize, analyze, and discuss the identified research according to it. RESULTS. Our source identification phase resulted in 133 relevant, peer-reviewed publications, published between 2002 and 2011 in awide variety of venues. They were subsequently classified according to four facets: development activity, research topic, contribution type, and research type. Pie, stacked bar, and bubble charts were used to depict and analyze the results. A deeper analysis is provided for the most interesting and/or remarkable results. CONCLUSION. Analysis of the results shows that, although the RIA term was coined in 2002, the first RIA-related research appeared in 2004. From 2007 there was a significant increase in research activity, peaking in 2009 and decreasing to pre-2009 levels afterwards. All development phases are covered in the identified research, with emphasis on ""design"" (33%) and ""implementation"" (29%). The majority of research proposes a ""method"" (44%), followed by ""model"" (22%), ""methodology"" (18%), and ""tools"" (16%), no publications in the category ""metrics"" were found. The preponderant research topic is ""models, methods and methodologies"" (23%) and, to a lesser extent, ""usability and accessibility"" and ""user interface"" (11% each). On the other hand, the topic ""localization, internationalization and multilinguality"" received no attention at all, and topics such as ""deep Web"" (under 1%), ""business processing"", ""usage analysis"", ""data management"", ""quality and metrics"" (all under 2%), ""semantics"", and ""performance"" (slightly above 2%) received very little attention. Finally, there is a large majority of ""solution proposals"" (66%), few ""evaluation research"" (14%), and even fewer ""validation"" (6%), although the latter have been increasing in recent years. © 2014 ACM.","<b>Authors:</b><br/>Casteleyn S., Garrigós I., Mazón J.-N. <br/><b>Key words:</b><br/>Rich Internet applications, Systematic mapping study"
233,paper_233,"Dasanayake S., Markkula J., Oivo M.",,"Context: Successfully addressing stakeholder concerns that are related to software system development and operation is crucial to achieving development goals. The importance of using a systematic approach to addressing these concerns throughout the software development life cycle is growing as more and more systems are employed to handle critical tasks. Objective: The goal of this study is to provide an overview of addressing concerns across the software development life cycle. Method: A systematic mapping study was conducted using a pre-defined protocol. Four digital databases were searched for research literature and primary studies were selected after a three round selection process conducted by multiple researchers. Results: The extracted data are processed and the results are reported from different viewpoints. The results are also analyzed against our research goals. Conclusion: We show that there is a considerable variation in the use of terminologies and addressing concerns in different phases of the software development life cycle. Copyright 2014 ACM.","<b>Authors:</b><br/>Dasanayake S., Markkula J., Oivo M. <br/><b>Key words:</b><br/>Software development life cycle, Stakeholder concerns, Systematic mapping study"
234,paper_234,"Seriai A., Benomar O., Cerat B., Sahraoui H.",,"Software visualization as a research field focuses on the visualization of the structure, behavior, and evolution of software. It studies techniques and methods for graphically representing these different aspects of software. Interest in software visualization has grown in recent years, producing rapid advances in the diversity of research and in the scope of proposed techniques, and aiding the application experts who use these techniques to advance their own research. Despite the importance of evaluating software visualization research, there is little work studying validation methods. As a consequence, it is usually difficult producing compelling evidence about the effectiveness of software visualization contributions. The goal of this paper is to study the validation techniques performed in the software visualization literature. We conducted a systematic mapping study of validation methods in software visualization. We consider 752 articles from multiple sources, published between 2000 and 2012, and study the validation techniques of the software visualization articles. The main outcome of this study is the lack in rigor when validating software visualization tool and techniques. Although software visualization has grown in interest in the last decade, it still lacks the necessary maturity to be properly and thoroughly evaluating its claims. Most article evaluations studied in this paper are qualitative case studies, including discussions about the benefits of the proposed visualizations. The results help understand the needs in software visualization validation techniques. They identify the type of evaluations that should be performed to address this deficiency. The specific analysis of SOFTVIS series articles shows that the specialized conference suffers from the same shortage. © 2014 IEEE.","<b>Authors:</b><br/>Seriai A., Benomar O., Cerat B., Sahraoui H. <br/><b>Key words:</b><br/>Software visualization, Systematic mapping study, Validation techniques"
235,paper_235,"Tofan D., Galster M., Avgeriou P., Schuitema W.",,"Context The software architecture of a system is the result of a set of architectural decisions. The topic of architectural decisions in software engineering has received significant attention in recent years. However, no systematic overview exists on the state of research on architectural decisions. Objective The goal of this study is to provide a systematic overview of the state of research on architectural decisions. Such an overview helps researchers reflect on previous research and plan future research. Furthermore, such an overview helps practitioners understand the state of research, and how research results can help practitioners in their architectural decision-making. Method We conducted a systematic mapping study, covering studies published between January 2002 and January 2012. We defined six research questions. We queried six reference databases and obtained an initial result set of 28,895 papers. We followed a search and filtering process that resulted in 144 relevant papers. Results After classifying the 144 relevant papers for each research question, we found that current research focuses on documenting architectural decisions. We found that only several studies describe architectural decisions from the industry. We identified potential future research topics: domain-specific architectural decisions (such as mobile), achieving specific quality attributes (such as reliability or scalability), uncertainty in decision-making, and group architectural decisions. Regarding empirical evaluations of the papers, around half of the papers use systematic empirical evaluation approaches (such as surveys, or case studies). Still, few papers on architectural decisions use experiments. Conclusion Our study confirms the increasing interest in the topic of architectural decisions. This study helps the community reflect on the past ten years of research on architectural decisions. Researchers are offered a number of promising future research directions, while practitioners learn what existing papers offer. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Tofan D., Galster M., Avgeriou P., Schuitema W. <br/><b>Key words:</b><br/>Architectural decisions, Software architecture, Systematic mapping study"
236,paper_236,"Vargas J.A., García-Mundo L., Genero M., Piattini M.",,"This paper present a summary of the paper published in the 18th International Conference on Evaluation and Assessment in Software Engineering (EASE 2014). The aim of this research is to discover the current state of Serious Games (SGs) quality initiatives, identifying gaps that merit future investigation. For this purpose, we conducted a systematic mapping study (SMS) on SG quality. The main results are summarized in this paper.","<b>Authors:</b><br/>Vargas J.A., García-Mundo L., Genero M., Piattini M. <br/><b>Key words:</b><br/>ISO 25010, Quality, Serious games, Systematic mapping study"
237,paper_237,"Bajwa S.S., Gencel C., Abrahamsson P.",,"This study presents the results of a systematic mapping study on software size estimation metrics and methods. The methods are investigated based on the entity type(s) measured by the method, size attribute type, size measure(s), and theoretical/empirical validation status of the method, and restriction with respect to functional domain type(s) and software development methodology applicability. Based on the results of the survey, we provide a discussion on the current state of the art in software sizing. © 2014 IEEE.","<b>Authors:</b><br/>Bajwa S.S., Gencel C., Abrahamsson P. <br/><b>Key words:</b><br/>software size measurement, systematic mapping study"
238,paper_238,"Welter M., Benitti F.B.V., Thiry M.",,"Green IT and computing are key areas to contribute with the environment sustainability. Recent researches have been exploring tools, practices, process, frameworks, reference models in Green IT, as well as they are mitigating the impact of the technology on the user of raw materials, electrical energy costs, greenhouse gas emissions or generation e-waste. Sizing up earnings and costs of green practices and processes will allow that software organizations can invest more in environment actions. This paper presents a systematic mapping study about green metrics, with objective to discover the state-of-the art drived by software engineering. © 2014 IEEE.","<b>Authors:</b><br/>Welter M., Benitti F.B.V., Thiry M. <br/><b>Key words:</b><br/>Green Metrics, Measurement, Software Organization, Sustainability, Systematic Mapping"
239,paper_239,"Penzenstadler B., Raturi A., Richardson D., Calero C., Femmer H., Franch X.",,"Background/Context : The objective of achieving higher sustainability in our lifestyles by information and communication technology has lead to a plethora of research activities in related fields. Consequently, Software Engineering for Sustainability (SE4S) has developed as an active area of research. Objective/Aim: Though SE4S gained much attention over the past few years and has resulted in a number of contributions, there is only one rigorous survey of the field. We follow up on this systematic mapping study from 2012 with a more in-depth overview of the status of research, as most work has been conducted in the last 4 years. Method: The applied method is a systematic mapping study through which we investigate which contributions were made, which knowledge areas are most explored, and which research type facets have been used, to distill a common understanding of the state-of-the-art in SE4S. Results: We contribute an overview of current research topics and trends, and their distribution according to the research type facet and the application domains. Furthermore, we aggregate the topics into clusters and list proposed and used methods, frameworks, and tools. Conclusion: The research map shows that impact currently is limited to few knowledge areas and there is need for a future roadmap to fill the gaps. Copyright is held by the owner/author(s).","<b>Authors:</b><br/>Penzenstadler B., Raturi A., Richardson D., Calero C., Femmer H., Franch X. <br/><b>Key words:</b><br/>Software engineering, Sustainability, Systematic mapping study"
240,paper_240,"Torre D., Labiche Y., Genero M.",,"Context: The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented modeling and documentation. Since the various UML diagrams describe different aspects of one, and only one, software under development, they are not independent but strongly depend on each other in many ways. In other words, the UML diagrams describing a software product must be consistent. Inconsistencies between these diagrams may be a source of faults in software systems. It is therefore paramount that these inconsistencies be detected, analyzed and hopefully fixed. Objective: The aim of this article is to deliver a comprehensive summary of UML consistency rules as they are described in the literature to date to obtain an extensive and detailed overview of the current research in this area. Method: We performed a Systematic Mapping Study by following well-known guidelines. We selected 95 primary studies from a search with seven search engines performed in December 2012. Results: Different results are worth mentioning. First it appears that researchers tend to discuss very similar consistency rules, over and over again. Most rules are horizontal (98.10%) and syntactic (88.21%). The most used diagrams are the class diagram (71.58%), the sequence diagram (47.37%) and the state machine diagram (42.11%). Conclusion: The fact that many rules are duplicated in primary studies confirms the need for a well-accepted list of consistency rules. This paper is a first step in this direction. Results indicate that much more work is needed to develop consistency rules for all 14 UML diagrams, in all dimensions of consistency (e.g., semantic and syntactic on the one hand, horizontal, vertical and evolution on the other hand). Copyright 2014 ACM.","<b>Authors:</b><br/>Torre D., Labiche Y., Genero M. <br/><b>Key words:</b><br/>Systematic mapping study, UML consistency rules, Unified Modeling Language (UML)"
241,paper_241,"Vargas J.A., García-Mundo L., Genero M., Piattini M.",,"Context: A Serious Game (SG) is a game for purposes other than entertainment [12]. SGs are currently in widespread use and their popularity has begun to steadily increase, their application areas now extend not only to education, but also to military, health and corporate [9] [12] sectors. SGs are of vital importance at present, as they can be a means to achieve relevant goals from both a personal and an institutional point of view. This may take place in fields as diverse as defense, education, scientific exploration, health care, emergency management, city planning, engineering, religion, and politics. The number of users of these systems grows each day, signifying that their impact is very high, and it is precisely for this reason that more extensive research on SG quality is needed. Objective: The aim of this study is to discover the current state of SG quality initiatives, identifying gaps that merit future investigation. Method: We conducted a systematic mapping study (SMS) on SG quality, following the guidelines proposed by Kitchenham and Charters [7]. We selected 112 papers found in six digital libraries until April of 2013. Results: Since 2007, research on SG quality proves to have grown very significantly. Research has focused mostly on addressing the effectiveness of SGs (78.57%), in addition to several entertainment characteristics that are principally related to pleasure (62.50%). The most widely-researched software artifact was the final product (97.32%), with design coming very far behind (7.14%). Less than half of all the research reviewed had been validated by means of experiments, and in most of these cases, experiments were conducted by the same researchers who had proposed the SG. The majority of experiments have not been replicated. The most common research outcome was questionnaires, closely followed by the confirmation of knowledge. Most of these outcomes evaluated the quality of a particular SG. Conclusion: Results show that SG quality has undergone a very important growth, thus making SG quality an area of opportunity for future research. Researchers are mainly concerned with demonstrating or confirming the effectiveness of SGs, but very little research has been conducted as regards the characteristics of playability that make SGs more effective. Since effectiveness and playability are evaluated in the final product there is a need to provide quality assurance methods that incorporate quality issues from the early stages of SG development. Further empirical validation is also needed, and in particular, external replications must be performed in order to corroborate and generalize the findings obtained. Copyright 2014 ACM.","<b>Authors:</b><br/>Vargas J.A., García-Mundo L., Genero M., Piattini M. <br/><b>Key words:</b><br/>ISO 25010, Quality, Serious games, Systematic mapping study"
242,paper_242,"Febrero F., Calero C., Moraga M.Á.",,"Context Software Reliability (SR) is a highly active and dynamic research area. Published papers have approached this topic from various and heterogeneous points of view, resulting in a rich body of literature on this topic. The counterpart to this is the considerable complexity of this body of knowledge. Objective The objective of this study is to obtain a panorama and a taxonomy of Software Reliability Modeling (SRM). Method In order to do this, a Systematic Mapping Study (SMS) which analyzes and structures the literature on Software Reliability Modeling has been carried out. Results A total of 972 works were obtained as a result of the Systematic Mapping Study. On the basis of the more than 500 selected primary studies found, the results obtained show an increasing diversity of work. Conclusion Although it was discovered that Software Reliability Growth Models (SRGM) are still the most common modeling technique, it was also found that both the modeling based on static and architectural characteristics and the models based on Artificial Intelligence and automatic learning techniques are increasingly more apparent in literature. We have also observed that most Software Reliability Modeling efforts take place in the Pacific Rim area and in academic environments. Industrial initiatives are as yet marginal, and would appear to be primarily located in the USA. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Febrero F., Calero C., Moraga M.Á. <br/><b>Key words:</b><br/>Modeling, Software Reliability, Systematic Mapping, Taxonomy"
243,paper_243,"Filho P.C., Brasilino C., Duarte A.",,"BACKGROUND: There is a large body of literature on research about fault management in grid computing. Despite being a well-established research area, there are no systematic studies focusing on characterizing the sorts of research that have been conducted, identifying well-explored topics as well as opportunities for further research. OBJECTIVE: This study aims at surveying the existing research on fault management in grid computing in order to identify useful approaches and opportunities for future research. METHOD: We conducted a systematic mapping study to collect, classify and analyze the research literature on fault management in grid computing indexed by the main search engines in the field. RESULTS: Our study selected and classified 257 scientific papers and was able to answer five research questions regarding the distribution of the scientific production over the time and space. CONCLUSIONS: The majority of the selected studies focus on fault tolerance, with very few efforts towards fault prevention, prediction and removal. © 2013 IEEE.","<b>Authors:</b><br/>Filho P.C., Brasilino C., Duarte A. <br/><b>Key words:</b><br/>"
244,paper_244,"Pinto V.A., de Rezende Rohlfs C.L., Parreiras F.S.",,"Ontologies have been used in several fields as an engineering artefact with the main purpose of conceptualizing a specific object of study. Therefore, it is reasonable to think about using ontologies to support enterprise modelling. In this paper, we investigate the application of ontologies in enterprise modelling. We performed a comprehensive systematic mapping study in order to understand the usage of ontologies to enterprise modeling. We group the results by business areas, business segments, languages, environments and methodologies. We conclude that ontologies are applicable to assist enterprise modelling and have been used specially in Industry, Health and Environment and Government. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Pinto V.A., de Rezende Rohlfs C.L., Parreiras F.S. <br/><b>Key words:</b><br/>Enterprise Architecture, Enterprise Modelling, Ontology, Systematic Review"
245,paper_245,"Mohi-Aldeen S.M., Deris S., Mohamad R.",,"Test case generation is a painstaking task in software testing and has a strong influence on the efficiency and the effectiveness of software tests. It is an important subject in software testing research that has led to the development of several tools and approaches over the decades. This paper presented a systematic mapping study to get an overview about the current studies of distinct techniques for generation of test cases automatically. The techniques presented in this paper are random-based methods, search-based methods and data mining-based methods. Each technique is explored briefly to give the basic idea behind it. In general, the paper's objective is to give an up-to-date introduction and short review of the research in generation of test cases automatically. Systematic mapping study is the process of finding and collecting as much literature as possible, provides a structure of the type of research reports and the results that have been published by categorizing them depending on specific search questions to provide a background for further research. This study was based on a comprehensive set of 85 papers published in conference and journals between 2002 and 2013 obtained after using multistage selection criteria in the field of automatic test cases generation. The results from our systematic mapping study include information about the researches techniques used to generate test cases automatically and types of coverage within a specific period that can help researchers in this field through providing an overview of the current researches in this area. Furthermore, it may serve as a first step towards a great explanation of the topic with the help of systematic literature review. © 2014 The authors and IOS Press. All rights reserved.","<b>Authors:</b><br/>Mohi-Aldeen S.M., Deris S., Mohamad R. <br/><b>Key words:</b><br/>Automatic test case generation methods, Software testing, Test case generation, Test case generation methods"
246,paper_246,"González J.E., Juristo N., Vegas S.",,"Context Juristo et al. [7] published a literature review about testing technique expenments. The goal was to provide a picture of which techniques and aspects of techniques had been studied experimentally, and try to compile a body of knowledge on testing techniques. Goal: In this paper, we extend Junsto et al.'s study to cover the years from 2000 (where it ended) until 2013. Method: We have performed a systematic mapping study. Results: The situation in testing experimentation has not changed since Junsto et al.'s study. Conclusions: The research field has the same shortcomings. © 2014 ACM.","<b>Authors:</b><br/>González J.E., Juristo N., Vegas S. <br/><b>Key words:</b><br/>mapping study, software testing techniques, testing experiments"
247,paper_247,"Pinto V.A., Parreiras F.S.",,"Over the past years we have witnessed the Web becoming an established channel for learning. Nowadays, hundreds of repositories are freely available on the Web aiming at sharing and reusing learning objects, but lacking in interoperability. In this paper, we present a comprehensive literature review on the state-of-the-art in the research field of Linked Enterprise Data. More precisely, this Systematic Literature Review intends to answer the following research question: What are the applications of Linked Data for Corporate Environments? Studies point out that there is a pattern regarding the frameworks used for implementing Semantic Web in enterprises. This pattern enables interlinking of both internal and external data sources. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Pinto V.A., Parreiras F.S. <br/><b>Key words:</b><br/>"
248,paper_248,"Cruz W.M., Isotani S.",,"Group Formation is a complex and important step to design effective collaborative learning activities. Through the adequate selection of individuals to a group, it is possible to create environments that foster the occurrence of meaningful interactions, and thereby, increasing robust learning and intellectual growth. Many researchers indicate that the inadequate formation of groups can demotivate students and hinder the learning process. Thus, in the field of Computer-Supported Collaborative Learning (CSCL), there are several studies focusing on developing and testing group formation in collaborative learning contexts using best practices and other pedagogical approaches. Nevertheless, the CSCL community lacks a comprehensive understanding on which computational techniques (i.e. algorithms) has supported group formation. To the best of our knowledge, there is no study aimed at gathering and analyzing the research findings on this topic using a systematic method. To fill this gap, this research conducted a systematic mapping with the objective of summarizing the studies on algorithms for group formation in CSCL contexts. Initially, by searching on six digital libraries, we collected 256 studies. Then, after a careful analysis of each study, we verified that only 48 were related to group formation applied to collaborative learning contexts. Finally, we categorized the contributions of these studies to present an overview of the findings produced by the community. This overview shows that: (i) there is a gradual increase on research published in this topic, (ii) 41% of the algorithms for group formation area based on probabilistic models, (iii) most studies presented the evaluation of tools that implement these algorithms, but (iv) only 2% of the studies provide their source code, and finally, (v) there is no tool or guideline to compare the benefits, differences and specificities of group formation algorithms available to date. As a result of this work an infographic is also available at: http://infografico.caed-lab.com/mapping/gf. © 2014 Springer International Publishing.","<b>Authors:</b><br/>Cruz W.M., Isotani S. <br/><b>Key words:</b><br/>Algorithms, CSCL, Group Formation, Systematic Mapping"
249,paper_249,"Fotrousi F., Fricker S.A., Fiedler M., Le-Gall F.",,"To create value with a software ecosystem (SECO), a platform owner has to ensure that the SECO is healthy and sustainable. Key Performance Indicators (KPI) are used to assess whether and how well such objectives are met and what the platform owner can do to improve. This paper gives an overview of existing research on KPI-based SECO assessment using a systematic mapping of research publications. The study identified 34 relevant publications for which KPI research and KPI practice were extracted and mapped. It describes the strengths and gaps of the research published so far, and describes what KPI are measured, analyzed, and used for decision-making from the researcher's point of view. For the researcher, the maps thus capture stateof- knowledge and can be used to plan further research. For practitioners, the generated map points to studies that describe how to use KPI for managing of a SECO. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Fotrousi F., Fricker S.A., Fiedler M., Le-Gall F. <br/><b>Key words:</b><br/>Digital ecosystem, KPI, Performance indicator, Software ecosystem, Success factor, Systematic mapping"
250,paper_250,"Ichrak T., Nawal S., Mustapha A.",,"Traffic congestion of the Internet is one of the major communication problems lived by millions of users. Many works have been devoted to improve the internet congestion control performance [1]. In this paper, we will apply a Systematic Mapping Study to the congestion control problem in TCP/IP, in order to build a classification scheme and measure the field interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different filters of the scheme can also be combined to answer more specific research Questions. © 2014Tolaimate Ichrak, Sbiti Nawal and Amghar Mustapha.","<b>Authors:</b><br/>Ichrak T., Nawal S., Mustapha A. <br/><b>Key words:</b><br/>Active queue management(AQM), Congestion control, Content type, Controller, Internet protocol(IP), Paper, Publication, Publisher, Relevance, SMS (systematic mapping study), Transmission control protocol(TCP)"
251,paper_251,"Ouhbi S., Idri A., Aleman J.L.F., Toval A.",,"Evaluating software product quality (SPQ) is an important task to ensure the quality of software products. In this paper a systematic mapping study was performed to summarize the existing SPQ evaluation (SPQE) approaches in literature and to classify the selected studies according to seven classification criteria: SPQE approaches, research types, empirical types, data sets used in the empirical evaluation of these studies, artifacts, SQ models, and SQ characteristics. Publication channels and trends were also identified. 57 papers were selected. The results show that the main publication sources of the papers identified were journals. Data mining techniques are the most frequently approaches reported in literature. Solution proposals were the main research type identified. The majority of the selected papers were history-based evaluations using existing data, which were mainly obtained from open source software projects and domain specific projects. Source code was the main artifacts used by SPQE approaches. Well-known SQ models were mentioned by half of the selected papers and reliability is the SQ characteristic through which SPQE was mainly achieved. SPQE-related subjects seem to attract more interest from researchers since the past years. © 2014 IEEE.","<b>Authors:</b><br/>Ouhbi S., Idri A., Aleman J.L.F., Toval A. <br/><b>Key words:</b><br/>"
252,paper_252,"Souza F.C.M., Papadakis M., Durelli V.H.S., Delamaro M.E.",,"Mutation testing is regarded as an effective testing criterion. However, its main downside is that it is an expensive and time consuming technique, which has hampered its widespread adoption in industrial settings. Automatic test data generation is an option to deal with this problem. The successful automation of this activity requires the generation of a small number of tests that can achieve good mutation score. In this paper, we describe the process, results and discussions of a systematic mapping conducted to gather and identify evidence with respect to the techniques and approaches for test data generation in mutation testing. We selected 19 primary studies that focus on techniques based in search, code coverage, restrictions, and hybrid techniques. Our results indicate that most of the reported approaches are not industry-ready, which underscores the importance of further research in the area so that automatic test data generation can reach an acceptable level of maturity and be used in realistic settings.","<b>Authors:</b><br/>Souza F.C.M., Papadakis M., Durelli V.H.S., Delamaro M.E. <br/><b>Key words:</b><br/>Mutation testing, Systematic mapping, Test data generation"
253,paper_253,"Khan M.J.",,"Domain knowledge for various decision-making activities of Software Engineering (SE) is rarely available in a structured or well-formalised form. Owing to lack of the well-informed knowledge, decision making for different kinds of predictions and estimations in SE domain is a challenge. Maintenance and elicitation of domain knowledge is an overwhelming task and causes the knowledge acquisition bottleneck. Most of the artificial intelligence techniques of prediction and estimation do not work in absence of complete and structured knowledge. Case-based reasoning (CBR) is a lazy learning paradigm of artificial intelligence which takes care of this challenge and helps to reduce the knowledge availability bottleneck. This technique exploits the similar experience of past which may be available in unstructured form, and improves its learning curve with passage of time. In literature, CBR has been successfully applied in various areas of SE, but there is lack of single systematic panoramic picture which might have highlighted the potential research questions in this direction. In this study, the author has presented a comprehensive and panoramic systematic mapping study of various CBR applications in SE domain, and identified some promising future research directions. © The Institution of Engineering and Technology 2014.","<b>Authors:</b><br/>Khan M.J. <br/><b>Key words:</b><br/>"
254,paper_254,"Oriol M., Marco J., Franch X.",,"Context Quality of Service (QoS) is a major issue in various web service related activities. Quality models have been proposed as the engineering artefact to provide a common framework of understanding for QoS, by defining the quality factors that apply to web service usage. Objective The goal of this study is to evaluate the current state of the art of the proposed quality models for web services, specifically: (1) which are these proposals and how are they related, (2) what are their structural characteristics, (3) what quality factors are the most and least addressed, and (4) what are their most consolidated definitions. Method We have conducted a systematic mapping by defining a robust protocol that combines automatic and manual searches from different sources. We used a rigorous method to elicitate the keywords from the research questions and a selection criteria to retrieve the final papers to evaluate. We have adopted the ISO/IEC 25010 standard to articulate our analysis. Results We have evaluated 47 different quality models from 65 papers that fulfilled the selection criteria. By analyzing in depth these quality models, we have: (1) distributed the proposals along the time dimension and identified their relationships, (2) analyzed their size (visualizing the number of nodes and levels) and definition coverage (as indicator of quality of the proposals), (3) quantified the coverage of the different ISO/IEC 25010 quality factors by the proposals, (4) identified the quality factors that appeared in at least 30% of the surveyed proposals and provided the most consolidated definitions for them. Conclusions We believe that this panoramic view on the anatomy of the quality models for web services may be a good reference for prospective researchers and practitioners in the field and especially may help avoiding the definition of new proposals that do not align with current research. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Oriol M., Marco J., Franch X. <br/><b>Key words:</b><br/>Quality model, Quality of service, Systematic mapping, Web service"
255,paper_255,"Alcañiz L.M., Rosado D.G., Mellado D., Fernández-Medina E.",,"While cloud computing emerges as a major trend in IT industry, early providers and adopters are paving the path with concerns and solutions. One of the most worrisome challenges that face the corporate clients of this new form of IT provision is how to maintain the security of their most important every day apps in the new environment, that is how to migrate securely their legacy systems that run on data centres fully controlled by the organization's IT department to a less clearly controlled infrastructure that is managed at least partly outside the scope of the clients premises and even completely off-shore. This paper presents a Systematic Mapping Study on the issue as the first step to analyze the different existing approaches in the literature about migration process to Cloud computing where taking into account the security aspects that have to be also moved to Cloud. We propose four research questions dealing with the existing strategies to migrate legacy, how they relate to common security issues as well as security issues specific to the cloud environment, and how the proposals are aligned with security standards.","<b>Authors:</b><br/>Alcañiz L.M., Rosado D.G., Mellado D., Fernández-Medina E. <br/><b>Key words:</b><br/>"
256,paper_256,"Nissola F., Benitti F.B.V.",,"The research scenario concerned about software usability addresses a wide range of characteristics, attributes and evaluation models. As result, we can observe the evaluators difficult in order to select the characteristics that best apply to the product that is being evaluated. By this way, this paper presents a systematic mapping in order to identify the main characteristics of usability evaluation related to web, desktop and mobile devices environments. As results, we selected 31 papers in order to perform data extraction and way possible to produce a list of 28 evaluation characteristics. Copyright © IARIA, 2014.","<b>Authors:</b><br/>Nissola F., Benitti F.B.V. <br/><b>Key words:</b><br/>Systematic Mapping, Usability Evaluation"
257,paper_257,"Francisco L., Benitti F.B.V.",,"This paper examines the Brazilian research on techniques for usability evaluation with regard to the scope of use. Through a systematic mapping this study identified 116 studies that reported empirical evidence on the different technical and evaluations in different environments. Results indicate that the empirical technique is the most used and the web environment was chosen by 60% of the studies. This study noted that assessments of an educational product are the most applied ones (33 studies). In addition to mapping research in the area, this study also contributes to point out areas for further research. © 2014 AISTI.","<b>Authors:</b><br/>Francisco L., Benitti F.B.V. <br/><b>Key words:</b><br/>systematic mapping, usability"
258,paper_258,"Maia P., Cavalcante E., Gomes P., Batista T., Delicato F.C., Pires P.F.",,"The Internet of Things (IoT) has emerged as a paradigm in which smart things actively collaborate among them and with other physical and virtual objects available in the Web in order to perform high level tasks. These things can be engaged in complex relationships including the composition and collaboration with other independent and heterogeneous systems in order to provide new functionalities, thus leading to the so-called systems-of-systems (SoS). In the context of integrating IoT-based systems in order to compose complex, large-scale SoS, this paper presents a systematic mapping aimed to discuss current scenarios and approaches in the development of IoT-based SoS, as well as some challenges and research opportunities in this context. © 2014 ACM.","<b>Authors:</b><br/>Maia P., Cavalcante E., Gomes P., Batista T., Delicato F.C., Pires P.F. <br/><b>Key words:</b><br/>Internet of Things, IoT, SoS, Systematic Mapping, Systems-of-Systems"
259,paper_259,"De Sousa Borges S., Durelli V.H.S., Reis H.M., Isotani S.",,"Gamification is a term that refers to the use of game elements in non-game contexts with the goal of engaging people in a variety of tasks. There is a growing interest in gamification as well as its applications and implications in the field of Education since it provides an alternative to engage and motivate students during the process of learning. Despite this increasing interest, to the best of our knowledge, there are no studies that cover and classify the types of research being published and the most investigated topics in the area. As a first step towards bridging this gap, we carried out a systematic mapping to synthesize an overview of the area. We went through 357 papers on gamification. Among them, 48 were related to education and only 26 met the criteria for inclusion and exclusion of articles defined in this study. These 26 papers were selected and categorized according to their contribution. As a result, we provide an overview of the area. Such an overview suggests that most studies focus on investigating how gamification can be used to motivate students, improve their skills, and maximize learning. Copyright 2014 ACM.","<b>Authors:</b><br/>De Sousa Borges S., Durelli V.H.S., Reis H.M., Isotani S. <br/><b>Key words:</b><br/>Education, Engagement, Gamification, Literature review, Motivation, Persuasive computing, Survey, Systematic mapping"
260,paper_260,"Jurca G., Hellmann T.D., Maurer F.",,"Agile software engineering and user-centered design are two important development processes for ensuring that an application has good user experience. However, integrating these two different processes into a single Agile-UX approach remains difficult. We performed a systematic mapping study to identify relevant research and understand what the field of Agile-UX looks like at present. This mapping discovered that there were only a few evaluation and validation papers published to date, so we performed a review of these papers to better understand the recommendations of these types of papers. Based on this, we are able both to provide a discussion of common trends in these papers that should be of interest to practitioners and to identify gaps in existing literature that indicate strong opportunities for future work. © 2014 IEEE.","<b>Authors:</b><br/>Jurca G., Hellmann T.D., Maurer F. <br/><b>Key words:</b><br/>agile, empirical studies, literature review, systematic mapping, user experience, user-centered design"
261,paper_261,"Qiu D., Li B., Ji S., Leung H.",,"Web service is a widely used implementation technique under the paradigm of Service-Oriented Architecture (SOA). A service-based system is subjected to continuous evolution and regression testing is required to check whether new faults have been introduced. Based on the current scientific work of web service regression testing, this survey aims to identify gaps in current research and suggests some promising areas for further study. To this end, we performed a broad automatic search on publications in the selected electronic databases published from 2000 to 2013. Through our careful review and manual screening, a total of 30 papers have been selected as primary studies for answering our research questions. We presented a qualitative analysis of the findings, including stakeholders, challenges, standards, techniques, and validations employed in these primary studies. Our main results include the following: (1) Service integrator is the key stakeholder that largely impacts how regression testing is performed. (2) Challenges of cost and autonomy issues have been studied heavily. However, more emphasis should be put on the other challenges, such as test timing, dynamics, privacy, quota constraints, and concurrency issues. (3) Orchestration-based services have been largely studied, while little attention has been paid to either choreography-based services or semantic-based services. (4) An appreciable amount of web service regression testing techniques have been proposed, including 48 test case prioritization techniques, 10 test selection techniques, two test suite minimization techniques, and another collaborative technique. (5) Many regression test techniques have not been theoretically proven or experimentally analyzed, which limits their application in large-scale systems. We believe that our survey has identified gaps in current research work and reveals new insights for the future work. © 2014 ACM.","<b>Authors:</b><br/>Qiu D., Li B., Ji S., Leung H. <br/><b>Key words:</b><br/>Regression testing, Test case prioritization, Test case selection, Test suite minimization, Web service"
262,paper_262,"Borg M., Runeson P., Ardö A.",,"Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based trace recovery. © 2013, Springer Science+Business Media New York.","<b>Authors:</b><br/>Borg M., Runeson P., Ardö A. <br/><b>Key words:</b><br/>Information retrieval, Software artifacts, Systematic mapping study, Traceability"
263,paper_263,"Fernández-Diego M., González-Ladrón-De-Guevara F.",,"Context The International Software Benchmarking Standards Group (ISBSG) maintains a software development repository with over 6000 software projects. This dataset makes it possible to estimate a project's size, effort, duration, and cost. Objective The aim of this study was to determine how and to what extent, ISBSG has been used by researchers from 2000, when the first papers were published, until June of 2012. Method A systematic mapping review was used as the research method, which was applied to over 129 papers obtained after the filtering process. Results The papers were published in 19 journals and 40 conferences. Thirty-five percent of the papers published between years 2000 and 2011 have received at least one citation in journals and only five papers have received six or more citations. Effort variable is the focus of 70.5% of the papers, 22.5% center their research in a variable different from effort and 7% do not consider any target variable. Additionally, in as many as 70.5% of papers, effort estimation is the research topic, followed by dataset properties (36.4%). The more frequent methods are Regression (61.2%), Machine Learning (35.7%), and Estimation by Analogy (22.5%). ISBSG is used as the only support in 55% of the papers while the remaining papers use complementary datasets. The ISBSG release 10 is used most frequently with 32 references. Finally, some benefits and drawbacks of the usage of ISBSG have been highlighted. Conclusion This work presents a snapshot of the existing usage of ISBSG in software development research. ISBSG offers a wealth of information regarding practices from a wide range of organizations, applications, and development types, which constitutes its main potential. However, a data preparation process is required before any analysis. Lastly, the potential of ISBSG to develop new research is also outlined. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Fernández-Diego M., González-Ladrón-De-Guevara F. <br/><b>Key words:</b><br/>ISBSG, Research methods, Software cost prediction, Software effort estimation, Software engineering, Systematic mapping study"
264,paper_264,"Ardini A., Hosseini M., Alrobai A., Shahri A., Phalp K., Ali R.",,"There is a continual growth in the use of social computing within a breadth of business domains, such as marketing, public engagement and innovation management. Software engineering research, like other similar disciplines, has recently started to harness the power of social computing throughout the various development phases, from requirements elicitation to validation and maintenance and for the various methods of development and structures of development teams. However, despite this increasing effort, we still lack a clear picture of the current status of this research. To address that lack of knowledge, we conduct a systematic mapping study on the utilisation of social computing for software engineering. This will inform researchers and practitioners about the current status and progress of the field including the areas of current focus and the geographical and chronological distribution of the research. We do the mapping across a diversity of dimensions including the activities of software engineering, the types of research, the characteristics of social computing and the demographic attributes of the published work. Our study results show a growing interest in the field, mainly in academia, and a general trend towards developing designated social computing platforms and utilising them in mainly four software engineering areas: management, coding, requirements engineering, and maintenance and enhancement. © 2014 Elsevier Inc.","<b>Authors:</b><br/>Ardini A., Hosseini M., Alrobai A., Shahri A., Phalp K., Ali R. <br/><b>Key words:</b><br/>Social computing, Software engineering, Systematic mapping"
265,paper_265,"Kähkönen T., Maglyas A., Smolander K.",,"Enterprise Resource Planning (ERP) systems have been the major interest of companies to improve the business performance with integrated business systems during the last 15 years. As demands for collaborative business through supply chain increased, so did the integration requirements for ERPs that are today connected externally with customers, suppliers and business partners and internally with continuously changing system landscape of the enterprise. We conducted a systematic mapping study to investigate how ERP integration has been studied by the academia from 1998 to 2012. Studies about technological issues are mostly dealing with systems inside a company whereas studies on methodological issues focus on the integration of the supply chain management and e-business. However, these studies are often either carried out without a rigorous empirical research method or they are based on single cases only. Quantitative methods have been mainly used to investigate quality attributes of ERPs but issues related to ERP integration in terms of a network of stakeholders in an ERP project still need more research in the future. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Kähkönen T., Maglyas A., Smolander K. <br/><b>Key words:</b><br/>Enterprise resource planning, Integration, Literature review, Systematic mapping study"
266,paper_266,"Bano M., Zowghi D., Ikram N.",,"There has been an increasing interest in conducting Systematic Literature Reviews (SLR) among Requirements Engineering (RE) researchers in recent years. However, so far there have been no tertiary studies conducted to provide a comprehensive overview of these published SLR in RE. In this paper we present a tertiary study of SLR that focus solely on RE related topics by following the guidelines of Evidence Based Software Engineering. We have conducted both automated search of major online sources and manual search of the RE and SLR related conferences and journals. Our tertiary study has identified 53 distinct systematic reviews published from 2006 to 2014 and reported in 64 publications. We have assessed the resulting SLR for their quality, and coverage of specific RE related topics thus identifying some gaps. We have observed that the quality of SLR in RE has been decreasing over the recent years. There is a strong need to replicate some of these SLR to increase the reliability of their results for future RE research. © 2014 IEEE.","<b>Authors:</b><br/>Bano M., Zowghi D., Ikram N. <br/><b>Key words:</b><br/>Mapping Study, Requirements Engineering, Systematic Literature Review, Tertiary Study"
267,paper_267,"Hasheela V.T., Smolander K.",,"Enterprise Resource Planning (ERP) was originally meant for large enterprises (LEs), hence most attention on ERP research focuses on LEs. However, ERP vendors have diverted their attention also to small and medium sized enterprises (SMEs) in order to cater for their needs. This paper presents a systematic mapping study of ERP in the context of SMEs. The study concentrated on the ERP adoption and implementation issues that have already been studied and identified gaps where further research is needed. It also investigated what research methodologies have been employed in the studies.","<b>Authors:</b><br/>Hasheela V.T., Smolander K. <br/><b>Key words:</b><br/>Enterprise resource planning, ERP, Small and medium-sized enterprises, SME, Systematic mapping"
268,paper_268,"Fernández D.M., Ognawala S., Wagner S., Daneva M.",,"Context: Requirements engineering process improvement (REPI) approaches have gained much attention in research and practice. Goal: So far, there is no comprehensive view on the research in REPI in terms of solutions and current state of reported evidence. We aims to provide an overview on the existing solutions, their underlying principles and their research type facets, i.e. their state of empirical evidence. Method: To this end, we conducted a systematic mapping study of the REPI publication space. Results: This paper reports on the first findings regarding research type facets of the contributions as well as selected methodological principles. We found a strong focus in the existing research on solution proposals for REPI approaches that concentrate on normative assessments and benchmarks of the RE activities rather than on holistic RE improvements according to individual goals of companies. Conclusions: We conclude, so far, that there is a need to broaden the work and to investigate more problem-driven REPI which also targets the improvement of the quality of the underlying RE artefacts, which currently seem out of scope. © 2014 ACM.","<b>Authors:</b><br/>Fernández D.M., Ognawala S., Wagner S., Daneva M. <br/><b>Key words:</b><br/>requirements engineering, software process improvement, systematic mapping study"
269,paper_269,"Sulaman S.M., Orucevic-Alagic A., Borg M., Wnuk K., Host M., De La Vara J.L.",,"The popularity of Open Source Software (OSS) has increased the interest in using it in safety critical applications. The aim of this study is to review research carried out on usage of open source code in development of safety-critical software and systems. We conducted a systematic mapping study through searches in library databases and manual identification of articles from open source conferences. We have identified 22 studies about using open source software, mainly in automotive, aerospace, medical and nuclear domains. Moreover, only a few studies present complete safety systems that are released as OSS in full. The most commonly used OSS functionalities are operating systems, imaging, control and data management. Finally most of the integrated OSS have mature code bases and a commit history of more than five years. © 2014 IEEE.","<b>Authors:</b><br/>Sulaman S.M., Orucevic-Alagic A., Borg M., Wnuk K., Host M., De La Vara J.L. <br/><b>Key words:</b><br/>mapping study, Open source, safety critical"
270,paper_270,"Wohlin C.",,"Context: Systematic literature reviews have become common in software engineering in the last decade, but challenges remain. Goal: Given the challenges, the objective is to describe improvement areas in writing primary studies, and hence provide a good basis for researchers aiming at synthesizing research evidence in a specific area. Method: The results presented are based on a literature review with respect to synthesis of research results in software engineering with a particular focus on empirical software engineering. The literature review is complemented and exemplified with experiences from conducting systematic literature reviews and working with research methodologies in empirical software engineering. Results: The paper presents three areas where improvements are needed to become more successful in synthesizing empirical evidence. These three areas are: terminology, paper content and reviewing. Conclusion: It is concluded that it must be possible to improve the primary studies, but it requires that researchers start having synthesis in mind when writing their research papers. © 2014 ACM.","<b>Authors:</b><br/>Wohlin C. <br/><b>Key words:</b><br/>evidence, guidelines, research methodology, research synthesis, systematic literature reviews, systematic mapping studies"
271,paper_271,"Da SilvaNeto M.G., Lins W.C.B., Mariz E.B.P.",,"Systematic Reviews and systematic mappings are widely used in medicine in an area called evidence based studies. These techniques have been adapted and used on secondary studies in the area of Software Engineering and Systems. Sort and synthesize information in a particular research area by analysis of their primary studies, both involve extensive work and researcher dedication. By adapting techniques applied in evidence based studies from the medical field to software engineering led to an approach which divid the systematic review tasks into three main phases: planning, execution and reporting of results. Unlike the area of medicine where there is a large number of groups and methodologies to support these tasks, the area of software engineering still lacks tools and methods that support the implementation of these activities, which generally require researchers to use software that were not designed for this purpose. This paper presents an approach based on Biolchini's proccess, based on checkpoints techniques, to assist in the maintenance of the main objectives of the review process, this tasks was supported by a management software. We used a case study to validate the approach and supporting tool. © ISCA, SEDE 2014.","<b>Authors:</b><br/>Da SilvaNeto M.G., Lins W.C.B., Mariz E.B.P. <br/><b>Key words:</b><br/>Approach, Method, Support tool, Systematic mapping, Systematic review"
272,paper_272,"Gonzalez-Ladron-De-Guevara F., Fernández-Diego M.",,"Background: The International Software Benchmarking Standards Group (ISBSG) dataset makes it possible to estimate a project's size, effort, duration, and cost. Aim: The aim was to analyze the ISBSG variables that have been used by researchers for software effort estimation from 2000, when the first papers were published, until the end of 2013. Method: A systematic mapping review was applied to over 167 papers obtained after the filtering process. From these, it was found that 133 papers produce effort estimation and only 107 list the independent variables used in the effort estimation models. Results: Seventy-one out of 118 ISBSG variables have been used at least once. There is a group of 20 variables that appear in more than 50% of the papers and include Functional Size (62%), Development Type (58%), Language Type (53%), and Development Platform (52%) following ISBSG recommendations. Sizing and Size attributes altogether represent the most relevant group along with Project attributes that includes 24 technical features of the project and the development platform. All in all, variables that have more missing values are used less frequently. Conclusions: This work presents a snapshot of the existing usage of ISBSG variables in software development estimation. Moreover, some insights are provided to guide future studies. © 2014 ACM.","<b>Authors:</b><br/>Gonzalez-Ladron-De-Guevara F., Fernández-Diego M. <br/><b>Key words:</b><br/>ISBSG, missing values, software effort estimation, software engineering, systematic mapping study"
273,paper_273,"Borges A., Ferreira W., Barreiros E., Almeida A., Fonseca L., Teixeira E., Silva D., Alencar A., Soares S.",,"Context: Empirical studies are gaining recognition in the Software Engineering (SE) research community. In order to foster empirical research, it is essential understand the environments, guidelines, process, and other mechanisms available to support these studies in SE. Goal: Identifying the mechanisms used to support the empirical strategies adopted by the researches in the major Empirical Software Engineering (ESE) scientific venues. Method: We performed a systematic mapping study that included all full papers published at EASE, ESEM and ESEJ since their first editions. A total of 898 studies were selected. Results: We provide the full list of identified support mechanisms and the strategies that uses them. The most commonly mechanisms used to support the empirical strategies were two sets of guidelines, one to secondary studies and another to experiments. The most reported empirical strategies are experiments and case studies. Conclusions: The use of empirical methods in SE has increased over the years but many studies do not apply these methods nor use mechanisms to guide their research. Therefore, the list of support mechanisms, where and how they were applied is a major asset to the SE community. Such asset can foster empirical studies aiding the choice regarding which strategies and mechanisms to use in a research. Also, we identified new perspectives and gaps that foster the development of resources to aid empirical studies. © 2014 ACM.","<b>Authors:</b><br/>Borges A., Ferreira W., Barreiros E., Almeida A., Fonseca L., Teixeira E., Silva D., Alencar A., Soares S. <br/><b>Key words:</b><br/>EASE, empirical software engineering, empirical strategies, ESEJ, ESEM, support mechanisms, systematic mapping study"
274,paper_274,"Diebold P.",,"Background: Agile software development methods are commonly customized to a specific need, such as usage or adaptation of agile practices. In order to identify which agile practices organizations are using, we performed a systematic mapping study. Objective: In this paper, our goal is to present the generalizable state of the practice in agile practices usage. Method: We used triangulation of these study data and the data of the state of agile development survey related to agile practices usage. Results: This comparison shows similar results, obvious deviations and contradictions are discussed. Conclusion: The results of the triangulation of the two studies can be considered as an initial generalizable state of the practice for the usage of agile practices. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Diebold P. <br/><b>Key words:</b><br/>Agile practices, Agile software development, Software processes"
275,paper_275,"Erich F., Amrit C., Daneva M.",,"DevOps is a conceptual framework for reintegrating development and operations of Information Systems.We performed a Systematic Mapping Study to explore DevOps. 26 articles out of 139 were selected, studied and summarized. Based on this a concept table was constructed. We discovered that DevOps has not been adequately studied in scientific literature. There is relatively little research available on DevOps and the studies are often of low quality. We also found that DevOps is supported by a culture of collaboration, automation, measurement, information sharing and web service usage. DevOps benefits IS development and operations performance. It also has positive effects on web service development and quality assurance performance. Finally, our mapping study suggests that more research is needed to quantify these effects. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Erich F., Amrit C., Daneva M. <br/><b>Key words:</b><br/>"
276,paper_276,"Wiese I.S., Côgo F.R., Ré R., Steinmacher I., Gerosa M.A.",,"Context: Previous work that used prediction models on Software Engineering included few social metrics as predictors, even though many researchers argue that Software Engineering is a social activity. Even when social metrics were considered, they were classified as part of other dimensions, such as process, history, or change. Moreover, few papers report the individual effects of social metrics. Thus, it is not clear yet which social metrics are used in prediction models and what are the results of their use in different contexts. Objective: To identify, characterize, and classify social metrics included in prediction models reported in the literature. Method: We conducted a mapping study (MS) using a snowballing citation analysis. We built an initial seed list adapting strings of two previous systematic reviews on software prediction models. After that, we conducted backward and forward citation analysis using the initial seed list. Finally, we visited the profile of each distinct author identified in the previous steps and contacted each author that published more than 2 papers to ask for additional candidate studies. Results: We identified 48 primary studies and 51 social metrics. We organized the metrics into nine categories, which were divided into three groups-communication, project, and commit-related. We also mapped the applications of each group of metrics, indicating their positive or negative effects. Conclusions: This mapping may support researchers and practitioners to build their prediction models considering more social metrics Copyright 2014 ACM.","<b>Authors:</b><br/>Wiese I.S., Côgo F.R., Ré R., Steinmacher I., Gerosa M.A. <br/><b>Key words:</b><br/>Mapping study, Prediction models, Social metrics, Social network analysis"
277,paper_277,"Diebold P., Dahlem M.",,"Background: Agile software development has been increasingly adopted during the last two decades. Nonetheless, many studies show that using agile methods as defined in the literature does not work very well. Thus, companies adapt these methods by just using parts of them (called agile practices). Objective: The goal of the literature study was to understand which agile practices are used in industry under different circumstances, such as different project types, domains, or processes. Method: We conducted a mapping study of empirical studies using agile practices in industry. The search strategy identified 1110 studies, of which 24 studies including 68 projects were analyzed. Results: The results of this study show that there are practices that are used more often and that the domain and the process also influence the application of different practices. Additionally, the findings confirm the assumption of Ken Schwaber that in most cases, agile methods are not used ""completely"" but that rather certain practices are adopted. Conclusions: Our results can be used by researchers to get a better idea of where and how to follow up research as well as by practitioners to get a better idea of which practices fit their needs and which are used by others. Therefore, our contribution increases the body of knowledge in agile practices usage. Copyright 2014 ACM.","<b>Authors:</b><br/>Diebold P., Dahlem M. <br/><b>Key words:</b><br/>Agile methods, Agile practices, Agile software development, Empirical SE, Industrial usage, Mapping study, Systematic review"
278,paper_278,"De Lima Salgado A., Freire A.P.",,"The mobile devices market has grown substantially, along with significant developments in mobile interactive technologies. Devices such as tablets, smartphones and others have increasingly become more popular and helped improve the way people interact and exchange information. The aim of this paper is to perform a systematic mapping of the literature regarding the use of heuristic evaluation methods applied on mobile applications. The aims of this research were twofold: analysing what are the most used sets of usability heuristics on usability evaluations of mobile devices, providing a common base to improve mobile design and usability evaluation, analysing details of how usability inspections of mobile applications have been conducted. The results show that different heuristics have been reported in research papers to evaluate usability of mobile devices. The study identified a total of 9 different heuristics sets means of the literature mapping. The traditional set of heuristics proposed by Nielsen and Molich was still the most used set of heuristics in heuristic usability evaluations of mobile devices, but the proposal of new specific heuristics for mobile interfaces has grown substantially. © 2014 Springer International Publishing.","<b>Authors:</b><br/>De Lima Salgado A., Freire A.P. <br/><b>Key words:</b><br/>heuristic evaluation, mobile usability, usability inspection"
279,paper_279,"Wohlin C.",,"Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably. Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review. Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches. Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review. Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches. Copyright 2014 ACM.","<b>Authors:</b><br/>Wohlin C. <br/><b>Key words:</b><br/>Replication, Snowball search, Snowballing, Systematic literature review, Systematic mapping studies"
280,paper_280,"Hernandes E.M., Belgamo A., Fabbri S.",,"Software inspection process is an effective activity to find defects on software artifacts as soon as they are introduced. The development of experimental knowledge on this area is useful to everyone who needs to make decisions about inspection activity. This paper aims to map the empirical studies conducted in the software inspection process area. The steps of the Systematic Mapping (SM) process was performed with the support of the StArt tool. Seventy nine papers were accepted in this SM and attributes related to inspection process, techniques, tools, inspected artifacts, research groups and universities were extracted. The results show different inspection processes, which have been experimentally investigated. Fagans process is the most investigated of them. In relation to inspected artifacts, requirements document and source code were the most used. Moreover, different tools and techniques have been used to support these processes. © Springer International Publishing Switzerland 2014.","<b>Authors:</b><br/>Hernandes E.M., Belgamo A., Fabbri S. <br/><b>Key words:</b><br/>Empirical software engineering, Experimental software engineering, Experiments, Software inspection process, Systematic mapping"
281,paper_281,"Türkcan S., Richly M.U., Le Gall A., Fiszman N., Masson J.-B., Westbrook N., Perronet K., Alexandrou A.",,"We present a new method for calibrating an optical-tweezer setup that is based on Bayesian inference1. This method employs an algorithm previously used to analyze the confined trajectories of receptors within lipid rafts2,3. The main advantages of this method are that it does not require input parameters and is insensitive to systematic errors like the drift of the setup. Additionally, it exploits a much larger amount of the information stored in the recorded bead trajectory than standard calibration approaches. The additional information can be used to detect deviations from the perfect harmonic potential or detect environmental influences on the bead. The algorithm infers the diffusion coefficient and the potential felt by a trapped bead, and only requires the bead trajectory as input. We demonstrate that this method outperforms the equipartition method and the power-spectrum method in input information required (bead radius and trajectory length) and in output accuracy. Furthermore, by inferring a higher order potential our method can reveal deviations from the assumed second-order potential. More generally, this method can also be used for magnetic-tweezer calibration. © 2014 SPIE.","<b>Authors:</b><br/>Türkcan S., Richly M.U., Le Gall A., Fiszman N., Masson J.-B., Westbrook N., Perronet K., Alexandrou A. <br/><b>Key words:</b><br/>Bayesian inference, Calibrating Optical-tweezers, Calibration method, Magnetic-tweezers, Optical-tweezers, Potential mapping, Trapping potential"
282,paper_282,"Fauzi S.S.M.",,"Majority of the current literature in GSD today only present the issues and solutions related to the social difficulties, cultural, language, time zone and others. There is only little investigation related to technical difficulties, particularly on Software Configuration Management (SCM). SCM is a practice which has been used widely, neither in collocated environment nor in GSD itself. SCM practices can be done without major problems in collocated environment. However, a lot of challenges faced by the team members in GSD environment, for the reason that the coordination and synchronization become more complex. This paper presents our findings of a systematic mapping study of the literature related to the issues concerning SCM and SCM areas of responsibility which have major problems.","<b>Authors:</b><br/>Fauzi S.S.M. <br/><b>Key words:</b><br/>Configuration management, Distributed software development, Global software development, Software configuration management"
283,paper_283,"Assunção W.K.G., Vergilio S.R.",,"Developing software from scratch is a high cost and errorprone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches. Copyright 2014 ACM.","<b>Authors:</b><br/>Assunção W.K.G., Vergilio S.R. <br/><b>Key words:</b><br/>Evolution, Reengineering, Reuse, Software product line"
284,paper_284,"Silva E., Cavalcante E., Batista T., Oquendo F., Delicato F.C., Pires P.F.",,"A system-of-systems (SoS) can be understood as a set of complex, independent, heterogeneous constituent systems, which are composed to form a larger and more complex system aiming at accomplishing a given mission. Each constituent system accomplishes its own individual mission and is able to collaboratively contribute to the accomplishment of the global mission of the SoS. Despite the importance and central role played by missions in SoS, the current literature lacks of studies focused on analyzing such missions and their specificities. The existing initiatives are domain-specific and are still in an initial stage of development. In this context, the main goal of this paper is to present a study about how missions of SoS can be defined, specified, represented, and implemented. Due to the limitations of the existing proposals for SoS, we have performed a systematic mapping on missions of systems, in a broader extent, aiming at identifying elements that can be brought to SoS. In this paper, we discuss such elements related to missions and other important issues that must be considered when shifting from single and monolithic systems to SoS. © 2014 ACM.","<b>Authors:</b><br/>Silva E., Cavalcante E., Batista T., Oquendo F., Delicato F.C., Pires P.F. <br/><b>Key words:</b><br/>Missions, SoS, Systematic mapping, Systems-of-systems"
285,paper_285,"Paredes J., Anslow C., Maurer F.",,"Understanding information about software artifacts is key to successful Agile software development projects, however, sharing information about artifacts is difficult to achieve amongst team members. There are many information visualization techniques used to help address the difficulties of knowledge sharing, but it is not clear what is the most effective technique. This paper presents the results of a systematic mapping study of existing literature on information visualization techniques used by Agile software development teams. The results of the systematic mapping show that Agile teams use visualization techniques for designing, developing, communicating, and tracking progress. Our findings show that visualization techniques help Agile teams increase knowledge sharing and raise awareness about software artifacts amongst team members. © 2014 IEEE.","<b>Authors:</b><br/>Paredes J., Anslow C., Maurer F. <br/><b>Key words:</b><br/>Agile software development, big visible charts, information radiator, information visualization, knowledge sharing, software visualization, systematic mapping"
286,paper_286,"Neto C.B.L., Filho P.B.D.C., Duarte A.N.",,"Background: The large computational infrastructures required to provide the on-demand services that most users are now used to are more prone to failures than any single computational device. Thus, fault management is a essential activity in the realization of the cloud computing model. Aims: This work aims at identifying well-explored topics on fault management in cloud computing as well as pin-pointing gaps in the scientific literature that may represent opportunities for further research and development in this area. Method: We conducted a systematic mapping study to collect, filter and classify scientific works in this area. The 4535 scientific papers found on major search engines were filtered and the remaining 166 papers were classified according to a taxonomy described in this work. Results: We found that IaaS is most explored in the selected studies. The main dependability functions explored were Tolerance and Removal, and the attributes were Reliability and Availability. Most papers had been classified by research type as Solution Proposal. Conclusion: This work summarizes and classifies the research effort conducted on fault management in cloud computing, providing a good starting point for further research in this area. © 2013 IEEE.","<b>Authors:</b><br/>Neto C.B.L., Filho P.B.D.C., Duarte A.N. <br/><b>Key words:</b><br/>Cloud Computing, Dependability, Evidence-Based Research, Fault Management, Mapping Study, Secondary Study"
287,paper_287,"Cavalcanti Y.C., Da Mota Silveira Neto P.A., Machado I.D.C., Vale T.F., De Almeida E.S., Meira S.R.D.L.",,"Software maintenance starts as soon as the first artifacts are delivered and is essential for the success of the software. However, keeping maintenance activities and their related artifacts on track comes at a high cost. In this respect, change request (CR) repositories are fundamental in software maintenance. They facilitate the management of CRs and are also the central point to coordinate activities and communication among stakeholders. However, the benefits of CR repositories do not come without issues, and commonly occurring ones should be dealt with, such as the following: duplicate CRs, the large number of CRs to assign, or poorly described CRs. Such issues have led researchers to an increased interest in investigating CR repositories, by considering different aspects of software development and CR management. In this paper, we performed a systematic mapping study to characterize this research field. We analyzed 142 studies, which we classified in two ways. First, we classified the studies into different topics and grouped them into two dimensions: challenges and opportunities. Second, the challenge topics were classified in accordance with an existing taxonomy for information retrieval models. In addition, we investigated tools and services for CR management, to understand whether and how they addressed the topics identified. Copyright © 2013 John Wiley & Sons, Ltd. Change request repositories are fundamental for software maintenance. However, their benefits do not come without issues. We analyzed 142 studies to characterize the research on these issues and provide directions for future investigation. The studies were classified into topics and grouped into two dimensions: challenges and opportunities. Then, the challenges were classified in accordance with an existing taxonomy for information retrieval models. Additionally, we investigated different change request repositories to understand whether and how they addressed the topics identified. Copyright © 2013 John Wiley & Sons, Ltd.","<b>Authors:</b><br/>Cavalcanti Y.C., Da Mota Silveira Neto P.A., Machado I.D.C., Vale T.F., De Almeida E.S., Meira S.R.D.L. <br/><b>Key words:</b><br/>bug report, bug tracking, change request repository, software evolution, software maintenance, software quality assurance"
288,paper_288,"Daneva M., Damian D., Marchetto A., Pastor O.",,"Since the inception of the RE conference series (1992), both researchers and practitioners in the RE community have acknowledged the significance of empirical evaluation as an instrument to gain knowledge about various aspects of RE phenomena and the validity of our research results. A significant number of empirical studies have been conducted in the search for knowledge about RE problems as well as evidence of successful and less successful application of proposed solutions. This editorial presents the progress empirical RE research has made since 1992. Based on a search in the Scopus digital library, we report from an analysis of peer-reviewed systematic literature reviews and mapping studies to showcase major areas of RE research that use methods from the Empirical Software Engineering paradigm. We summarize prior empirical research in RE and introduce the contributors to this special issue on empirical research methodologies and studies in RE. © 2014 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Daneva M., Damian D., Marchetto A., Pastor O. <br/><b>Key words:</b><br/>"
289,paper_289,"Malavolta I., Muccini H.",,"Model-Driven Engineering (MDE) can be considered as the right tool to reduce the complexity of Wireless Sensor Network (WSN) development through its principles of abstraction, separation of concerns, reuse and automation. In this paper we present the results of a systematic mapping study we performed for providing an organized view of existing MDE approaches for designing WSNs. A total number of 780 studies were analysed, among them, we selected 16 papers as primary studies relevant for review. We setup a comparison framework for these studies, and classified them based on a set of common parameters. The main objective of our research is to give an overview about the state-of-the-art of MDE approaches dedicated to WSN design, and finally, discuss emerging challenges that have to be considered in future MDE approaches for engineering WSNs. © 2014 IEEE.","<b>Authors:</b><br/>Malavolta I., Muccini H. <br/><b>Key words:</b><br/>"
290,paper_290,"De La Vega De Leõn A., Hu Y., Bajorath J.",,"Matching molecular series (MMS) have originally been introduced as an extension of the matched molecular pair (MMP) concept to facilitate the design of substructure-based structure-activity relationship (SAR) networks. An MMP is defined as a pair of compounds that only differ by a structural change at a single site. In addition, an MMS is defined as an MMP-based series of compounds that have a conserved structural core and are distinguished by modifications at a single site. Systematic generation of MMS from specifically active compounds generalizes the search for series of structural analogs. Potency-ordered MMS provide series associated with SAR information. We have systematically extracted MMS from publicly available compounds with well-defined activity measurements and generated a large database with approx. 40 000 single- and 13 600 multi-target series, which provide a rich source of SAR information. As an application, we introduce MMP-based mapping of screening hits to MMS to search for initial SAR information and determine all SAR environments available for such hits. The MMS database is made freely available to the scientific community. © 2014 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.","<b>Authors:</b><br/>De La Vega De Leõn A., Hu Y., Bajorath J. <br/><b>Key words:</b><br/>Bioinformatics, Computational chemistry, Drug design, Matching molecular series (MMS), Structure-activity relationship (SAR) networks"
291,paper_291,"Carroll L.N., Au A.P., Detwiler L.T., Fu T.-C., Painter I.S., Abernethy N.F.",,"Background: A myriad of new tools and algorithms have been developed to help public health professionals analyze and visualize the complex data used in infectious disease control. To better understand approaches to meet these users' information needs, we conducted a systematic literature review focused on the landscape of infectious disease visualization tools for public health professionals, with a special emphasis on geographic information systems (GIS), molecular epidemiology, and social network analysis. The objectives of this review are to: (1) identify public health user needs and preferences for infectious disease information visualization tools, (2) identify existing infectious disease information visualization tools and characterize their architecture and features, (3) identify commonalities among approaches applied to different data types, and (4) describe tool usability evaluation efforts and barriers to the adoption of such tools. Methods: We identified articles published in English from January 1, 1980 to June 30, 2013 from five bibliographic databases. Articles with a primary focus on infectious disease visualization tools, needs of public health users, or usability of information visualizations were included in the review. Results: A total of 88 articles met our inclusion criteria. Users were found to have diverse needs, preferences and uses for infectious disease visualization tools, and the existing tools are correspondingly diverse. The architecture of the tools was inconsistently described, and few tools in the review discussed the incorporation of usability studies or plans for dissemination. Many studies identified concerns regarding data sharing, confidentiality and quality. Existing tools offer a range of features and functions that allow users to explore, analyze, and visualize their data, but the tools are often for siloed applications. Commonly cited barriers to widespread adoption included lack of organizational support, access issues, and misconceptions about tool use. Discussion and conclusion: As the volume and complexity of infectious disease data increases, public health professionals must synthesize highly disparate data to facilitate communication with the public and inform decisions regarding measures to protect the public's health. Our review identified several themes: consideration of users' needs, preferences, and computer literacy, integration of tools into routine workflow, complications associated with understanding and use of visualizations, and the role of user trust and organizational support in the adoption of these tools. Interoperability also emerged as a prominent theme, highlighting challenges associated with the increasingly collaborative and interdisciplinary nature of infectious disease control and prevention. Future work should address methods for representing uncertainty and missing data to avoid misleading users as well as strategies to minimize cognitive overload. © 2014 The Authors.","<b>Authors:</b><br/>Carroll L.N., Au A.P., Detwiler L.T., Fu T.-C., Painter I.S., Abernethy N.F. <br/><b>Key words:</b><br/>Disease surveillance, GIS, Infectious disease, Public health, Social network analysis, Visualization"
292,paper_292,"Jacobsen K.",,"Height information is a basic part of topographic mapping. Only in special areas frequent update of height models is required, usually the update cycle is quite lower as for horizontal map information. Some height models are available free of charge in the internet, for commercial height models a fee has to be paid. Mostly digital surface models (DSM) with the height of the visible surface are given and not the bare ground height, as required for standard mapping. Nevertheless by filtering of DSM, digital terrain models (DTM) with the height of the bare ground can be generated with the exception of dense forest areas where no height of the bare ground is available. These height models may be better as the DTM of some survey administrations. In addition several DTM from national survey administrations are classified, so as alternative the commercial or free of charge available information from internet can be used. The widely used SRTM DSM is available also as ACE-2 GDEM corrected by altimeter data for systematic height errors caused by vegetation and orientation errors. But the ACE-2 GDEM did not respect neighbourhood information. With the worldwide covering TanDEM-X height model, distributed starting 2014 by Airbus Defence and Space (former ASTRIUM) as WorldDEM, higher level of details and accuracy is reached as with other large area covering height models. At first the raw-version of WorldDEM will be available, followed by an edited version and finally as WorldDEM-DTM a height model of the bare ground. With 12m spacing and a relative standard deviation of 1.2m within an area of 1° × 1° an accuracy and resolution level is reached, satisfying also for larger map scales. For limited areas with the HDEM also a height model with 6m spacing and a relative vertical accuracy of 0.5m can be generated on demand. By bathymetric LiDAR and stereo images also the height of the sea floor can be determined if the water has satisfying transparency. Another method of getting bathymetric height information is an analysis of the wave structure in optical and SAR-images. An overview about the absolute and relative accuracy, the consistency, error distribution and other characteristics as influence of terrain inclination and aspects is given. Partially by post processing the height models can or have to be improved.","<b>Authors:</b><br/>Jacobsen K. <br/><b>Key words:</b><br/>Analysis, Bathymetry, DEM/DTM, Image, SAR"
293,paper_293,"Oliveira K., Pimentel J., Santos E., Dermeval D., Guedes G., Souza C., Soares M., Castro J., Alencar F., Silva C.",,"The celebration of 25th anniversary of the Brazilian Symposium of Software Engineering (SBES) as well as the forthcoming Requirements Engineering Conference to be held in Brazil for the first time, has led us to have a closer look at the local Requirements Engineering (RE) Community. A systematic mapping was performed in order to find out the main Brazilian research groups, authors as well as their topics of interest and publications with greatest impact. This information may be useful for those that do not know well the local requirements engineering community, such as local newcomers or foreign researchers. It may also help to identify potential groups for collaboration. Similarly, it may provide valuable information to assist local agencies when granting research funds.","<b>Authors:</b><br/>Oliveira K., Pimentel J., Santos E., Dermeval D., Guedes G., Souza C., Soares M., Castro J., Alencar F., Silva C. <br/><b>Key words:</b><br/>Requirements engineering, Systematic mapping"
294,paper_294,"Maretto C.X., Barcellos M.P.",,"During the execution of software projects, it is necessary to collect, store and analyze data to support project and organizational decisions. Software measurement is a fundamental practice for project management and process improvement. It is present in the main models and standards that address software process improvement, such as ISO/IEC 12207, CMMI and MR MPS.BR. In order to effectively perform software measurement, it is necessary an infrastructure to support data collection, storage and analysis. This article presents a study that investigated measurement architectures described in the literature. As a result, eight architectures were found. Their main characteristics were analyzed and are presented in this paper.","<b>Authors:</b><br/>Maretto C.X., Barcellos M.P. <br/><b>Key words:</b><br/>Measurement architecture, Measurement repository, Software measurement, Systematic mapping study"
295,paper_295,"Villar A., Matalonga S.",,"[No abstract available]","<b>Authors:</b><br/>Villar A., Matalonga S. <br/><b>Key words:</b><br/>"
296,paper_296,"Cedillo P., Fernandez A., Insfran E., Abrahão S.",,"Web mashups are a new generation of applications based on the composition of ready-to-use, heterogeneous components. They are gaining momentum thanks to their lightweight composition approach, which represents a new opportunity for companies to leverage on past investments in SOA, Web services, and public APIs. Although several studies are emerging in order to address mashup development, no systematic mapping studies have been reported on how quality issues are being addressed. This paper reports a systematic mapping study on which and how the quality of Web mashups has been addressed and how the product quality-aware approaches have been defined and validated. The aim of this study is to provide a background in which to appropriately develop future research activities. A total of 38 research papers have been included from an initial set of 187 papers. Our results provided some findings regarding how the most relevant product quality characteristics have been addressed in different artifacts and stages of the development process. They have also been useful to detect some research gaps, such as the need of more controlled experiments and more quality-aware mashup development proposals for other characteristics which being important for the Web domain have been neglected such as Usability and Reliability. © Springer International Publishing 2013.","<b>Authors:</b><br/>Cedillo P., Fernandez A., Insfran E., Abrahão S. <br/><b>Key words:</b><br/>Product quality, Systematic mapping study, Web mashups"
297,paper_297,"Bandi A., Williams B.J., Allen E.B.",,"Code decay is a gradual process that negatively impacts the quality of a software system. Developers need trusted measurement techniques to evaluate whether their systems have decayed. The research aims to find what is currently known about code decay detection techniques and metrics used to evaluate decay. We performed a systematic mapping study to determine which techniques and metrics have been empirically evaluated. A review protocol was developed and followed to identify 30 primary studies with empirical evidence of code decay. We categorized detection techniques into two broad groups: human-based and metric-based approaches. We describe the attributes of each approach and distinguish features of several subcategories of both high-level groups. A tabular overview of code decay metrics is also presented. We exclude studies that do not use time (i.e., do not use evaluation of multiple software versions) as a factor when evaluating code decay. This limitation serves to focus the review. We found that coupling metrics are the most widely used at identifying code decay. Researchers use various terms to define code decay, and we recommend additional research to operationalize the terms to provide more consistent analysis. © 2013 IEEE.","<b>Authors:</b><br/>Bandi A., Williams B.J., Allen E.B. <br/><b>Key words:</b><br/>Architecture Violations, Code Decay, Coupling, Design Rules, Metrics, Software Evolution"
298,paper_298,"Marshall C., Brereton P.",,"Background: Systematic literature reviews (SLRs) have become an established methodology in software engineering (SE) research however they can be very time consuming and error prone. Aim: The aims of this study are to identify and classify tools that can help to automate part or all of the SLR process within the SE domain. Method: A mapping study was performed using an automated search strategy plus snowballing to locate relevant papers. A set of known papers was used to validate the search string. Results: 14 papers were accepted into the final set. Eight presented text mining tools and six discussed the use of visualisation techniques. The stage most commonly targeted was study selection. Only two papers reported an independent evaluation of the tool presented. The majority were evaluated through small experiments and examples of their use. Conclusions: A variety of tools are available to support the SLR process although many are in the early stages of development and usage. © 2013 IEEE.","<b>Authors:</b><br/>Marshall C., Brereton P. <br/><b>Key words:</b><br/>automated tool, systematic literature review"
299,paper_299,"Nascimento D.M., Cox K., Almeida T., Sampaio W., Bittencourt R.A., Souza R., Chavez C.",,"Context: It is common practice in academia to have students work with ""toy"" projects in software engineering courses. One way to make such courses more realistic and reduce the gap between academic courses and industry needs is getting students involved in Open Source Projects with faculty supervision. Objective: This study aims to summarize existing information on how open source projects have been used to facilitate students' learning of software engineering. Method: A systematic mapping study was undertaken by identifying, filtering and classifying primary studies using a predefined strategy. Results: 53 papers were selected and classified. The main results were: a) most studies focus on comprehensive software engineering courses, although some papers deal with specific areas, b) the most prevalent approach was the traditional project method, c) surveys are the main learning assessment instrument, especially for student self-assessment, d) conferences are the typical publication venue, and e) more than half of the studies were published in the last five years. Conclusions: The resulting map gives an overview of the existing initiatives in this context and shows gaps where further research can be pursued. © 2013 IEEE.","<b>Authors:</b><br/>Nascimento D.M., Cox K., Almeida T., Sampaio W., Bittencourt R.A., Souza R., Chavez C. <br/><b>Key words:</b><br/>"
300,paper_300,"Kitchenham B., Brereton P.",,"Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research. Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process. Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools. Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult. Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Kitchenham B., Brereton P. <br/><b>Key words:</b><br/>Mapping study, Systematic literature review, Systematic review, Systematic review methodology"
301,paper_301,"Silva C.M.R.D., Silva J.L.C.D., Rodrigues R.B., Campos G.M.M., Nascimento L.M.D., Garcia V.C.",,"Today, Cloud Computing is rising strongly, presenting itself to the market by its main service models, known as IaaS, PaaS and SaaS, that offer advantages in operational investments by means of on-demand costs, where consumers pay by resources used. In face of this growth, security threats also rise, compromising the Confidentiality, Integrity and Availability of the services provided. Our work is a Systematic Mapping where we hope to present metrics about publications available in literature that deal with some of the seven security threats in Cloud Computing, based in the guide entitled 'Top Threats to Cloud Computing' from the Cloud Security Alliance (CSA). In our research we identified the more explored threats, distributed the results between fifteen Security Domains and identified the types of solutions proposed for the threats. In face of Those results, we observed that publications in the literature mostly show a certain tendency as the proposals presented for Threats involved. However, in some cases there is some variation, it motivated us to carry out an analysis of standard deviation in the results obtained in our protocol. Based on these data, we present our conception regarding this behavior. © 2013 IEEE.","<b>Authors:</b><br/>Silva C.M.R.D., Silva J.L.C.D., Rodrigues R.B., Campos G.M.M., Nascimento L.M.D., Garcia V.C. <br/><b>Key words:</b><br/>Cloud Computing, Security, Systematic Map, Threats"
302,paper_302,"Pereira Da Silva F.A., Da Mota Silveira Neto P.A., Garcia V.C., Trinta F.A.M., Assad R.E.",,"Cloud Accounting refers to how cloud usage is recorded and charged. It is present at all commercialized cloud services (SaaS, PaaS and IaaS), since they have to monitor the consumption aiming to charge the customers. In a previous research we performed a systematic mapping study regarding cloud accounting and identified a set of shortcomings in the existing accounting models for IaaS. One of these limitations was related to charging policy specification. Charging policies are mechanisms that establish rules to convert usage records into monetary information. This paper presents a Domain Specific Language (DSL) for charging policy specification: VeloZ. Such DSL enables cloud providers to write charging policies following the economic model RaaS. According to RaaS, the IaaS must be provisioned in a high granularity in terms of resource types and such granularity is profitable for both, provider and customer. We evaluated the DSL under a real cloud platform called JiTCloud. The experiment results evidences that VeloZ can be considered feasible in terms of flexibility and reliability. © 2013 IEEE.","<b>Authors:</b><br/>Pereira Da Silva F.A., Da Mota Silveira Neto P.A., Garcia V.C., Trinta F.A.M., Assad R.E. <br/><b>Key words:</b><br/>Accounting, Billing, Cloud Computing, Domain Specific Language, Federated Cloud, IaaS"
303,paper_303,"Borg M., Runeson P.",,"Background. Several researchers have proposed creating after-the-fact structure among software artifacts using trace recovery based on Information Retrieval (IR). Due to significant variation points in previous studies, results are not easily aggregated. Aim. We aim at an overview picture of the outcome of previous evaluations. Method. Based on a systematic mapping study, we perform a synthesis of published research. Results. Our synthesis shows that there are no empirical evidence that any IR model outperforms another model consistently. We also display a strong dependency between the Precision and Recall (P-R) values and the input datasets. Finally, our mapping of P-R values on the possible output space highlights the difficulty of recovering accurate trace links using naïve cut-off strategies. Conclusion. Based on our findings, we stress the need for empirical evaluations beyond the basic P-R 'race'. © 2013 IEEE.","<b>Authors:</b><br/>Borg M., Runeson P. <br/><b>Key words:</b><br/>empirical software engineering, information retrieval, secondary study, software traceability"
304,paper_304,"Oliveira A.M.C.A., Dos Santos S.C., Garcia V.C.",,"In computing courses, the teaching and learning approach normally emphasizes theoretical knowledge at the expense of practical knowledge. The major disadvantages of this approach are learners' lack of motivation during class and their quickly forgetting the knowledge they have acquired. With a view to overcoming these difficulties, Problem Based Learning (PBL), an institutional method of teaching, has been applied to teaching computing disciplines. Despite the growth of the practice of PBL in various disciplines of Computing, there is little evidence of its specific characteristics in this area, the effectiveness of different PBL methodological approaches, or of benefits and challenges encountered. In this context, this paper presents a systematic mapping study in order to identify studies which involve best practices when using the PBL method in Computing between 1997 and 2011, answering five research questions: ""What are the main characteristics of PBL that support teaching in Computing?"", ""What are the criteria for applying PBL effectively in this area?"", ""How is the PBL methodology applied?"", ""What are the advantages and benefits of applying PBL in Computing?"" and, finally, ""What are the main challenges about learning in PBL in Computing?"". © 2013 IEEE.","<b>Authors:</b><br/>Oliveira A.M.C.A., Dos Santos S.C., Garcia V.C. <br/><b>Key words:</b><br/>Computing, Education, Problem Based Learning"
305,paper_305,"Kähkönen T., Smolander K.",,"Companies have been adopting Enterprise Resource Planning (ERP) systems for decades in order to integrate business functions to increase their competitiveness. The original goal of ERP was to provide an all-in-one integrated suite for the enterprise. However, in a modern business environment, ERPs are integrated externally with customers, suppliers and business partners and internally with continuously changing system landscape of the enterprise. In this paper we present a systematic mapping study that investigates how ERP integration-related issues have been studied by the academia between 1998 and 2012. Studies about technological issues are mostly dealing with systems inside a company whereas studies on methodological issues focus on the integration of the supply chain management and e-business. However, these studies are often either carried out without a rigorous research method or they are based on single cases only. Quantitative methods have been mainly used to investigate quality attributes of ERPs. It is still unclear, how integration issues are effectively solved by a network of stakeholders in an ERP project. This requires more research in the future.","<b>Authors:</b><br/>Kähkönen T., Smolander K. <br/><b>Key words:</b><br/>Enterprise resource planning, Integration, Literature review, Systematic mapping study"
306,paper_306,"Hernandes E.M., Belgamo A., Fabbri S.",,"Background: The interest in produce experimental knowledge about verification and validation techniques increases over the years. This kind of knowledge can be useful for researchers who develop studies in that area as well as for industry that can make decisions about verification and validation activities (V&V) on the basis of experimental results. Aim: This paper aims to map the empirical studies conducted in the software inspection process area. Method: Each step of the Systematic Mapping (SM) process was performed with the support of the StArt tool, and the papers from major databases, journals, conferences, and workshops were covered. Results: Seventy nine papers were accepted in this mapping and helped identifying the inspection processes, techniques and tools commonly referenced in that papers, as well as the artifacts usually inspected and the research groups and universities frequently involved in these papers. Conclusion: Different inspection processes have been investigated through experimental studies, and the Fagan's process is the most investigated of them. To evaluate these different processes requirements document and source code were the artifacts more used. Besides, different tools and techniques have been used to support these processes. Some important lessons were learned, which are in accordance to explanations of others authors.","<b>Authors:</b><br/>Hernandes E.M., Belgamo A., Fabbri S. <br/><b>Key words:</b><br/>Empirical software engineering, Experimental software engineering, Experiments, Software inspection process, Systematic mapping"
307,paper_307,"Abelein U., Sharp H., Paech B.",,"Researchers have studied how best to involve users in software development for a long time, primarily in the area of information systems and human-computer interaction. The authors consider the effects of both user participation and user involvement, which they abbreviate to UPI. Existing research describes several benefits of UPI, such as improved quality due to more precise requirements, the prevention of unneeded and expensive features, and an increase in user satisfaction, which leads to higher system use. But even though some researchers consider it to be essential to system success, other studies have found contradicting results. Furthermore, it's not a common practice in today's IT projects to involve users to a large extent. To clarify UPI's effects on system success and to get a deeper understanding of the differences between user participation and user involvement, the authors reviewed the existing UPI literature in software development and conducted a systematic mapping study. © 2013 IEEE.","<b>Authors:</b><br/>Abelein U., Sharp H., Paech B. <br/><b>Key words:</b><br/>literature review, meta analysis, software development, systematic mapping study, user involvement, user participation"
308,paper_308,"Raza B., Macdonell S.G., Clear T.",,"This study presents an analysis of the most recent literature addressing global software engineering (GSE). The primary purpose is to understand what issues are being addressed and how research is being carried out in GSE - And comparatively, what work is not being conducted. We examine the current state of GSE research using a new Systematic Snapshot Mapping (SSM) technique. We analysed 275 papers published between January 2011 and June 2012 in peer-reviewed conferences, journals and workshops. Our results provide a coarse-grained overview of the very recent literature addressing GSE, by classifying studies into predefined categories. We also follow and extend several prior classifications to support our synthesis of the data. Our results reveal that, currently, GSE studies are focused on Management and Infrastructure related factors rather than Human or Distance related factors, using principally evaluative research approaches. Most of the studies are conducted at the organizational level, mainly using methods such as interviews, surveys, field studies and case studies. We use inter-country network analysis to confirm that the USA and India are major players in GSE, with USA-India collaborations being the most frequently studied, followed by USA-China. Specific groups of countries have dominated the reported GSE project locations (and the locations of research authors). In contrast, regions including Central Asia, South Asia (except India), Africa and South East Asia have not been covered in these studies. While a considerable number of GSE-related studies have been published they are currently quite narrowly focused on exploratory research and explanatory theories. The critical research paradigm has been untouched, perhaps due to a lack of criteria and principles for carrying out such research in GSE. An absence of formulative research, experimentation and simulation, and a comparative focus on evaluative approaches, all suggest that existing tools, methods and approaches from related fields are being tested in the GSE context. However, these solutions may not scale to cover GSE-related issues or may overlook factors/facets specific to GSE. Copyright © 2013 SCITEPRESS.","<b>Authors:</b><br/>Raza B., Macdonell S.G., Clear T. <br/><b>Key words:</b><br/>Distributed software development, Global software engineering (GSE), Systematic mapping"
309,paper_309,"Catal C., Atalay M.",,"We conduct a Systematic Mapping Study to categorize the primary research papers in architectural analysis. Systematic mapping studies can change the research perspective in an area and they can help to find the ignored researched areas. The objective of this study is to investigate the techniques used in papers on architectural analysis, identify the current trends, and provide suggestions for the future research studies. We search for papers published within the last ten years in the following databases: IEEE Explorer, ACM Digital Library, Science Direct, and Wiley. We conclude that more validation and evaluation research is needed to provide a better foundation for architectural analysis. © 2013 IEEE.","<b>Authors:</b><br/>Catal C., Atalay M. <br/><b>Key words:</b><br/>ATAM, Evidence-based software engineering, literature review, software architecture, survey"
310,paper_310,"Mohabbati B., Asadi M., Gaevi? D., Hatala M., Müller H.A.",,"Context Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Mohabbati B., Asadi M., Gaevi? D., Hatala M., Müller H.A. <br/><b>Key words:</b><br/>Service-oriented architecture, Software product lines, Systematic mapping"
311,paper_311,"Novais R.L., Torres A., Mendes T.S., Mendonça M., Zazworka N.",,"Background Software evolution is an important topic in software engineering. It generally deals with large amounts of data, as one must look at whole project histories as opposed to their current snapshot. Software visualization is the field of software engineering that aims to help people to understand software through the use of visual resources. It can be effectively used to analyze and understand the large amount of data produced during software evolution. Objective This study investigates Software Evolution Visualization (SEV) approaches, collecting evidence about how SEV research is structured, synthesizing current evidence on the goals of the proposed approaches and identifying key challenges for its use in practice. Methods A mapping study was conducted to analyze how the SEV area is structured. Selected primary studies were classified and analyzed with respect to nine research questions. Results SEV has been used for many different purposes, especially for change comprehension, change prediction and contribution analysis. The analysis identified gaps in the studies with respect to their goals, strategies and approaches. It also pointed out to a widespread lack of empirical studies in the area. Conclusion Researchers have proposed many SEV approaches during the past years, but some have failed to clearly state their goals, tie them back to concrete problems, or formally validate their usefulness. The identified gaps indicate that there still are many opportunities to be explored in the area. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Novais R.L., Torres A., Mendes T.S., Mendonça M., Zazworka N. <br/><b>Key words:</b><br/>Software evolution, Software visualization, Systematic mapping study"
312,paper_312,"Pernstål J., Feldt R., Gorschek T.",,"Lean approaches to product development (LPD) have had a strong influence on many industries and in recent years there have been many proponents for lean in software development as it can support the increasing industry need of scaling agile software development. With it's roots in industrial manufacturing and, later, industrial product development, it would seem natural that LPD would adapt well to large-scale development projects of increasingly software-intensive products, such as in the automotive industry. However, it is not clear what kind of experience and results have been reported on the actual use of lean principles and practices in software development for such large-scale industrial contexts. This was the motivation for this study as the context was an ongoing industry process improvement project at Volvo Car Corporation and Volvo Truck Corporation. The objectives of this study are to identify and classify state of the art in large-scale software development influenced by LPD approaches and use this established knowledge to support industrial partners in decisions on a software process improvement (SPI) project, and to reveal research gaps and proposed extensions to LPD in relation to its well-known principles and practices. For locating relevant state of the art we conducted a systematic mapping study, and the industrial applicability and relevance of results and said extensions to LPD were further analyzed in the context of an actual, industrial case. A total of 10,230 papers were found in database searches, of which 38 papers were found relevant. Of these, only 42 percent clearly addressed large-scale development. Furthermore, a majority of papers (76 percent) were non-empirical and many lacked information about study design, context and/or limitations. Most of the identified results focused on eliminating waste and creating flow in the software development process, but there was a lack of results for other LPD principles and practices. Overall, it can be concluded that research in the much hyped field of lean software development is in its nascent state when it comes to large scale development. There is very little support available for practitioners who want to apply lean approaches for improving large-scale software development, especially when it comes to inter-departmental interactions during development. This paper explicitly maps the area, qualifies available research, and identifies gaps, as well as suggests extensions to lean principles relevant for large scale development of software intensive systems. © 2013 Elsevier Inc.","<b>Authors:</b><br/>Pernstål J., Feldt R., Gorschek T. <br/><b>Key words:</b><br/>Agile software development, Automotive software development, Lean product development, Lean software development, Software engineering, Systematic mapping study"
313,paper_313,"Vanhala E., Smolander K.",,"Although business model is a decades old concept, it has been a part of scientific research especially after the burst of the dot-com bubble. Business model is an abstraction of the firm's business logic. It describes the basic revenue stream, value propositions, customers and key resources. This article presents a systematic mapping study of the research on software business models, how the concept is applied in literature. We found out that the business model concept is not well-defined. The definitions of business models include varying relations to other similar concepts, like revenue model, business logic and business process. We also found out that there is very little, if any, research done in the industry level to see how companies utilize business modeling. These issues require further research. © 2013 IADIS.","<b>Authors:</b><br/>Vanhala E., Smolander K. <br/><b>Key words:</b><br/>Business model, Software business, Success factor, Systematic mapping study"
314,paper_314,"Wanderley F., Araujo J.",,"The KAOS framework aims to avoid eliciting ambiguous or irrelevant requirements and allows efficient and easy communication between stakeholders. Nevertheless, KAOS is designed mainly for requirements engineers, not so much for other important stakeholders, that understand better other kinds of models, such as mind maps. Thus, this paper proposes an approach for generating KAOS goal models from mind maps by adopting model-driven techniques. The use of mind maps, as a creative and agile requirements technique, aims to encourage the construction of KAOS goal models more effectively and in a simpler way by involving all kinds of stakeholders that do need to know about KAOS. Our approach also contains the definition of a systematic process and is applied to an industrial case application. © 2013 IEEE.","<b>Authors:</b><br/>Wanderley F., Araujo J. <br/><b>Key words:</b><br/>Goal-Oriented Requirements, Mind Mapping Modelling, Model-Driven Engireering"
315,paper_315,"Banerjee I., Nguyen B., Garousi V., Memon A.",,"Context GUI testing is system testing of a software that has a graphical-user interface (GUI) front-end. Because system testing entails that the entire software system, including the user interface, be tested as a whole, during GUI testing, test cases - modeled as sequences of user input events - are developed and executed on the software by exercising the GUI's widgets (e.g., text boxes and clickable buttons). More than 230 articles have appeared in the area of GUI testing since 1991. Objective In this paper, we study this existing body of knowledge using a systematic mapping (SM). Method The SM is conducted using the guidelines proposed by Petersen et al. We pose three sets of research questions. We define selection and exclusion criteria. From the initial pool of 230 articles, published in years 1991-2011, our final pool consisted of 136 articles. We systematically develop a classification scheme and map the selected articles to this scheme. Results We present two types of results. First, we report the demographics and bibliometrics trends in this domain, including: top-cited articles, active researchers, top venues, and active countries in this research area. Moreover, we derive the trends, for instance, in terms of types of articles, sources of information to derive test cases, types of evaluations used in articles, etc. Our second major result is a publicly-accessible repository that contains all our mapping data. We plan to update this repository on a regular basis, making it a ""live"" resource for all researchers. Conclusion Our SM provides an overview of existing GUI testing approaches and helps spot areas in the field that require more attention from the research community. For example, much work is needed to connect academic model-based techniques with commercially available tools. To this end, studies are needed to compare the state-of-the-art in GUI testing in academic techniques and industrial tools. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Banerjee I., Nguyen B., Garousi V., Memon A. <br/><b>Key words:</b><br/>Bibliometrics, GUI application, Paper repository, Systematic mapping, Testing"
316,paper_316,"Wohlin C., Runeson P., Da Mota Silveira Neto P.A., Engström E., Do Carmo Machado I., De Almeida E.S.",,"Background Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies. Objective This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering. Method The research is based on an in-depth case study of two published mapping studies on software product line testing. Results We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies. Conclusions From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. © 2013 Elsevier Inc.","<b>Authors:</b><br/>Wohlin C., Runeson P., Da Mota Silveira Neto P.A., Engström E., Do Carmo Machado I., De Almeida E.S. <br/><b>Key words:</b><br/>Review of reviews, Software product lines, Software testing, Systematic literature review, Systematic mapping study"
317,paper_317,"Li Y., Liu Z., Han L., Li C., Wang R.",,"Protein-protein interactions are observed in various biological processes. They are important for understanding the underlying molecular mechanisms and can be potential targets for developing small-molecule regulators of such processes. Previous studies suggest that certain residues on protein-protein binding interfaces are ?hot spots?. As an extension to this concept, we have developed a residue-based method to identify the characteristic interaction patterns (CIPs) on protein-protein binding interfaces, in which each pattern is a cluster of four contacting residues. Systematic analysis was conducted on a nonredundant set of 1,222 protein-protein binding interfaces selected out of the entire Protein Data Bank. Favored interaction patterns across different protein-protein binding interfaces were retrieved by considering both geometrical and chemical conservations. As demonstrated on two test tests, our method was able to predict hot spot residues on protein-protein binding interfaces with good recall scores and acceptable precision scores. By analyzing the function annotations and the evolutionary tree of the protein-protein complexes in our data set, we also observed that protein-protein interfaces sharing common characteristic interaction patterns are normally associated with identical or similar biological functions. © 2013 American Chemical Society.","<b>Authors:</b><br/>Li Y., Liu Z., Han L., Li C., Wang R. <br/><b>Key words:</b><br/>"
318,paper_318,"Catal C., Mishra D.",,"Test case prioritization techniques, which are used to improve the cost-effectiveness of regression testing, order test cases in such a way that those cases that are expected to outperform others in detecting software faults are run earlier in the testing phase. The objective of this study is to examine what kind of techniques have been widely used in papers on this subject, determine which aspects of test case prioritization have been studied, provide a basis for the improvement of test case prioritization research, and evaluate the current trends of this research area. We searched for papers in the following five electronic databases: IEEE Explorer, ACM Digital Library, Science Direct, Springer, and Wiley. Initially, the search string retrieved 202 studies, but upon further examination of titles and abstracts, 120 papers were identified as related to test case prioritization. There exists a large variety of prioritization techniques in the literature, with coverage-based prioritization techniques (i.e., prioritization in terms of the number of statements, basic blocks, or methods test cases cover) dominating the field. The proportion of papers on model-based techniques is on the rise, yet the growth rate is still slow. The proportion of papers that use datasets from industrial projects is found to be 64 %, while those that utilize public datasets for validation are only 38 %. On the basis of this study, the following recommendations are provided for researchers: (1) Give preference to public datasets rather than proprietary datasets, (2) develop more model-based prioritization methods, (3) conduct more studies on the comparison of prioritization methods, (4) always evaluate the effectiveness of the proposed technique with well-known evaluation metrics and compare the performance with the existing methods, (5) publish surveys and systematic review papers on test case prioritization, and (6) use datasets from industrial projects that represent real industrial problems. © 2012 Springer Science+Business Media, LLC.","<b>Authors:</b><br/>Catal C., Mishra D. <br/><b>Key words:</b><br/>Regression testing, Systematic literature review, Systematic mapping study, Test case prioritization"
319,paper_319,"Ruiz-Rube I., Dodero J.M., Palomo-Duarte M., Ruiz M., Gawn D.",,"Software process engineering is a discipline, which aims to study and improve software development and maintenance processes. The explicit definition of software processes is essential. To this end, the Object Management Group consortium proposed the Software and Systems Process Engineering Meta-Model (SPEM) that exploits the benefits of theModel Driven Architecture paradigm applied to software process models, instead of software specification models. The aim of this study is to discover evidence clusters and evidence deserts in the use and application of SPEM from a business process management point of view. To reach the proposed objective, we have undertaken a systematic mapping study of the existing scientific literature. The reviewed literature deals mainly with process modeling and, to a lesser extent, with process adaptability, verification, and validation, enactment and evaluation. Wide agreement exists in using the SPEMmeta-model to develop different types of methods and processes. Further research efforts are needed in areas related to enactment and evaluation of software processes. There is a need to evolve to a new version of the meta-model that incorporates the improvements proposed by different authors. Copyright © 2013 John Wiley and Sons, Ltd. Received 5 July 2011, Revised 2 November 2012, Accepted 19 February 2013.","<b>Authors:</b><br/>Ruiz-Rube I., Dodero J.M., Palomo-Duarte M., Ruiz M., Gawn D. <br/><b>Key words:</b><br/>Business process management, Model-driven engineering, Software process engineering, SPEM, Systematic mapping study"
320,paper_320,"Markan C.M., Gupta P., Bansal M.",,"A novel analogue CMOS design of a cortical cell, that computes weighted sum of inputs, is presented. The cell?s feedback regime exploits the adaptation dynamics of floating gate pFET 'synapse' to perform competitive learning amongst input weights as time-staggered winner take all. A learning rate parameter regulates adaptation time and a bias enforces resource limitation by restricting the number of input branches and winners in a competition. When learning ends, the cell?s response favours one input pattern over others to exhibit feature selectivity. Embedded in a 2-D RC grid, these feature selective cells are capable of performing a symmetry breaking pattern formation, observed in some reaction?diffusion models of cortical feature map formation, e.g. ocular dominance. Close similarity with biological networks in terms of adaptability and long term memory indicates that the cell?s design is ideally suited for analogue VLSI implementation of Self-Organizing Feature Map (SOFM) models of cortical feature maps. &copy, 2013 Elsevier Ltd.","<b>Authors:</b><br/>Markan C.M., Gupta P., Bansal M. <br/><b>Key words:</b><br/>Cortical feature maps, Feature selectivity, Floating gate'synapse'Winner-Take-All (WTA), Ocular dominance, Reaction?diffusion"
321,paper_321,"De Carvalho J.V., Ruiz D.D.",,"In this paper, we describe the development of a systematic review about the topic ""Discovering Frequent Itemsets on Uncertain Data"". To the best of our knowledge, this work seems to be the first systematic review addressing the topic. We show the whole process executed and its findings. Initially we define a rigorous protocol to lead the process. In the first phase, we create a systematic mapping of the area. In addition, from the complete reading of each article, a panorama of this area is presented. We reveal the search engines that most publicize this topic and which publishing types, authors and research institutions are involved in these papers. Moreover we identify the algorithms and the classes of these algorithms most compared over the years, how are made these comparisons, as well as their availabilities. Therefore this systematic review becomes a rich material for understanding this knowledge area. © 2013 Springer-Verlag.","<b>Authors:</b><br/>De Carvalho J.V., Ruiz D.D. <br/><b>Key words:</b><br/>Frequent Itemsets, Frequent Patterns, Probabilistic Databases, Systematic Mapping, Systematic Review, Uncertain Data"
322,paper_322,"Garousi V., Mesbah A., Betin-Can A., Mirshokraie S.",,"Context: The Web has had a significant impact on all aspects of our society. As our society relies more and more on the Web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 147 papers in the area of web application testing, which have appeared between 2000 and 2011. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends in this specialized field. Method We review and structure the body of knowledge related to web application testing through a systematic mapping (SM) study. As part of this study, we pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a classification schema. In addition, we conduct a bibliometrics analysis of the papers included in our study. Results Our study includes a set of 79 papers (from the 147 retrieved papers) published in the area of web application testing between 2000 and 2011. We present the results of our systematic mapping study. Our mapping data is available through a publicly-accessible repository. We derive the observed trends, for instance, in terms of types of papers, sources of information to derive test cases, and types of evaluations used in papers. We also report the demographics and bibliometrics trends in this domain, including top-cited papers, active countries and researchers, and top venues in this research area. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our systematic mapping can help researchers to obtain an overview of existing web application testing approaches and indentify areas in the field that require more attention from the research community. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Garousi V., Mesbah A., Betin-Can A., Mirshokraie S. <br/><b>Key words:</b><br/>Bibliometrics, Paper repository, Systematic mapping, Testing, Web application"
323,paper_323,"De Sousa E Silva G., Guimarães A.P.N., De Oliveira H.N., Tavares T.A., Dos Anjos E.G.",,"The use of telemedicine systems is becoming increasingly common these days. Telemedicine systems exist for the purposes of education, improving the accuracy of medical diagnoses through the provision of a second opinion, and remote patient monitoring. Accordingly, the number of software solutions is increasing. Software Architecture is a subarea of Software Engineering whose goal is to study the system components, their external properties, and their relationships with other software. A good architecture can allow a system meets the mainly requirements of a project, such as performance, reliability, portability, easy maintenance, interoperability, etc. Aiming to find out what architectural styles that proposes a better performance in systems for telemedicine, a systematic mapping was done. With this mapping, it was possible to find taxonomies related to telemedicine systems, architectural styles commonly used in these systems and technologies relevant to the area. © 2013 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>De Sousa E Silva G., Guimarães A.P.N., De Oliveira H.N., Tavares T.A., Dos Anjos E.G. <br/><b>Key words:</b><br/>mapping study, software architecture, telemedicine"
324,paper_324,"Laguna M.A., Crespo Y.",,"Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements, specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Laguna M.A., Crespo Y. <br/><b>Key words:</b><br/>Evolution, Legacy system, Reengineering, Refactoring, Software product line"
325,paper_325,"Nye B.D.",,"This paper analyzes the state of current intelligent tutoring systems (ITS) research for applications in the developing world. Recent data shows a rapidly narrowing digital divide, with internet and computing device access rising sharply in less developed countries. Tutoring systems could be a transformative technology in these areas, where shortages of teachers and materials are persistent problems. However, the unique challenges and opportunities for ITS in this context are not well-explored. This paper identifies barriers to adoption distinct to the developing world, then presents the results of a systematic mapping study of recent ITS literature (2009-2012) that looks at the level of focus given to each barrier. This study finds that only a small percentage of peer-reviewed publications and architectures address even one of the barriers preventing adoption in these contexts. Implications and strategies being used to target these barriers are discussed. © 2013 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Nye B.D. <br/><b>Key words:</b><br/>Barriers to Adoption, Digital Divide, Intelligent Tutoring Systems, Mobile Learning, Systematic Mapping Study"
326,paper_326,"Agrawal P., Raghavan P., Hartman M., Sharma N., Van Der Perre L., Catthoor F.",,"We present a systematic methodology for exploring application partitioning and assignment together with platform architecture instantiation. Streaming applications with multiple runtime modes are considered. The platform architecture is based on a domain specific MPSoC architecture template. We show results using complete inner modem physical layer processing of wireless applications, WLAN and LTE. We show that the proposed methodology obtains up to 30% energy improvement in energy with negligible area overheads as compared to straight-forward mapping to one processor, while meeting performance constraints, for a multi-mode WLAN 11n system and single-mode LTE system. Copyright © 2013 ACM.","<b>Authors:</b><br/>Agrawal P., Raghavan P., Hartman M., Sharma N., Van Der Perre L., Catthoor F. <br/><b>Key words:</b><br/>Co-Design, Heterogeneous MPSoC, Mapping, Partition- ing, Streaming applications"
327,paper_327,"Fernández-Sáez A.M., Genero M., Chaudron M.R.V.",,"Context: The Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile. Method: We have carried out a systematic mapping study to collect the empirical studies published in order to discover ""What is the current existing empirical evidence with regard to the use of UML diagrams in source code maintenance and the maintenance of the UML diagrams themselves? Results: We found 38 papers, which contained 63 experiments and 3 case studies. Conclusion: Although there is common belief that the use of UML is beneficial for source code maintenance, since the quality of the modifications is greater when UML diagrams are available, only 3 papers concerning this issue have been published. Most research (60 empirical studies) concerns the maintainability and comprehensibility of the UML diagrams themselves which form part of the system's documentation, since it is assumed that they may influence source code maintainability, although this has not been empirically validated. Moreover, the generalizability of the majority of the experiments is questionable given the material, tasks and subjects used. There is thus a need for more experiments and case studies to be performed in industrial contexts, i.e., with real systems and using maintenance tasks conducted by practitioners under real conditions that truly show the utility of UML diagrams in maintaining code, and that the fact that a diagram is more comprehensible or modifiable influences the maintainability of the code itself. This utility should also be studied from the viewpoint of cost and productivity, and the consistent and simultaneous maintenance of diagrams and code must also be considered in future empirical studies. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Fernández-Sáez A.M., Genero M., Chaudron M.R.V. <br/><b>Key words:</b><br/>Empirical studies, Software maintenance, Systematic literature review, Systematic mapping study, UML"
328,paper_328,"Giuffrida R., Dittrich Y.",,"Background: In Global Software Development (GSD), informal communication and knowledge sharing play an important role. Social Software (SoSo) has the potential to support and foster this key responsibility. Research on the use of SoSo in GSD is still at an early stage: although a number of empirical studies on the usage of SoSo are available in related fields, there exists no comprehensive overview of what has been investigated to date across them. Objective The aim of this review is to map empirical studies on the usage of SoSo in Software Engineering projects and in distributed teams, and to highlight the findings of research works which could prove to be beneficial for GSD researchers and practitioners. Method A Systematic Mapping Study is conducted using a broad search string that allows identifying a variety of studies which can be beneficial for GSD. Papers have been retrieved through a combination of automatic search and snowballing, hence a wide quantitative map of the research area is provided. Additionally, text extracts from the studies are qualitatively synthesised to investigate benefits and challenges of the use of SoSo. Results SoSo is reported as being chiefly used as a support for collaborative work, fostering awareness, knowledge management and coordination among team members. Contrary to the evident high importance of the social aspects offered by SoSo, socialisation is not the most important usage reported. Conclusions This review reports how SoSo is used in GSD and how it is capable of supporting GSD teams. Four emerging themes in global software engineering were identified: the appropriation and development of usage structures, understanding how an ecology of communication channels and tools are used by teams, the role played by SoSo either as a subtext or as an explicit goal, and finally, the surprising low percentage of observational studies. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Giuffrida R., Dittrich Y. <br/><b>Key words:</b><br/>Computer-supported cooperative work, Distributed teams, Global software development, Social media, Social software, Systematic mapping study"
329,paper_329,"Da Silva F.Q.B., França A.C.C., Suassuna M., De Sousa Mariz L.M.R., Rossiley I., De Miranda R.C.G., Gouveia T.B., Monteiro C.V.F., Lucena E., Cardozo E.S.F., Espindola E.",,"Context: The internal composition of a work team is an important antecedent of team performance and the criteria used to select team members play an important role in determining team composition. However, there are only a handful of empirical studies about the use of team building criteria in the software industry. Objective: The goal of this article is to identify criteria used in industrial practice to select members of a software project team, and to look for relationships between the use of these criteria and project success. In addition, we expect to contribute with findings about the use of replication in empirical studies involving human factors in software engineering. Method: Our research was based on an iterative mix-method, replication strategy. In the first iteration, we used qualitative research to identify team-building criteria interviewing software project managers from industry. Then, we performed a cross-sectional survey to assess the correlations of the use of these criteria and project success. In the second iteration, we used the results of a systematic mapping study to complement the set of team building criteria. Finally, we performed a replication of the survey research with variations to verify and improve the results. Results: Our results showed that the consistent use team building criteria correlated significantly with project success, and the criteria related to human factors, such as personality and behavior, presented the strongest correlations. The results of the replication did not reproduce the results of the original survey with respect to the correlations between criteria and success goals. Nevertheless, the variations in the design and the difference in the sample of projects allowed us to conclude that the two results were compatible, increasing our confidence on the existence of the correlations. Conclusion: Our findings indicated that carefully selecting team member for software teams is likely to positively influence the projects in which these teams participate. Besides, it seems that the type of development method used can moderate (increase or decrease) this influence. In addition, our study showed that the choice of sampling technique is not straightforward given the many interacting factors affecting this type of investigation. © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Da Silva F.Q.B., França A.C.C., Suassuna M., De Sousa Mariz L.M.R., Rossiley I., De Miranda R.C.G., Gouveia T.B., Monteiro C.V.F., Lucena E., Cardozo E.S.F., Espindola E. <br/><b>Key words:</b><br/>People management, Software engineering, Software teams, Team building criteria, Team effectiveness"
330,paper_330,"Kasoju A., Petersen K., Mäntylä M.V.",,"Context: Evidence-based software engineering (EBSE) provides a process for solving practical problems based on a rigorous research approach. The primary focus so far was on mapping and aggregating evidence through systematic reviews. Objectives: We extend existing work on evidence-based software engineering by using the EBSE process in an industrial case to help an organization to improve its automotive testing process. With this we contribute in (1) providing experiences on using evidence based processes to analyze a real world automotive test process and (2) provide evidence of challenges and related solutions for automotive software testing processes. Methods: In this study we perform an in-depth investigation of an automotive test process using an extended EBSE process including case study research (gain an understanding of practical questions to define a research scope), systematic literature review (identify solutions through systematic literature), and value stream mapping (map out an improved automotive test process based on the current situation and improvement suggestions identified). These are followed by reflections on the EBSE process used. Results: In the first step of the EBSE process we identified 10 challenge areas with a total of 26 individual challenges. For 15 out of those 26 challenges our domain specific systematic literature review identified solutions. Based on the input from the challenges and the solutions, we created a value stream map of the current and future process. Conclusions: Overall, we found that the evidence-based process as presented in this study helps in technology transfer of research results to industry, but at the same time some challenges lie ahead (e.g. scoping systematic reviews to focus more on concrete industry problems, and understanding strategies of conducting EBSE with respect to effort and quality of the evidence). © 2013 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Kasoju A., Petersen K., Mäntylä M.V. <br/><b>Key words:</b><br/>Automotive software testing, Evidence-based software engineering, Process assessment"
331,paper_331,"Garousi V., Shahnewaz S., Krishnamurthy D.",,"Performance is critical to the success of every software system. As a sub-area of software engineering, Software Performance Engineering (SPE) is a systematic and quantitative discipline to construct software systems that meet performance objectives. A family of SPE approaches that has become popular in the last decade is SPE based on models developed using the Unified Modeling Language (UML), referred to as UML-Driven Software Performance Engineering (UML-SPE). This particular research area has emerged and grown since late 1990s when the UML was proposed. More than 100 papers have been published so far in this area. As this research area matures and the number of related papers increases, it is important to systematically summarize and categorize the current state-of-the-art and to provide an overview of the trends in this specialized field. The authors systematically map the body of knowledge related to UML-SPE through a Systematic Mapping (SM) study. As part of this study, they pose two sets of research questions, define selection and exclusion criteria, and systematically develop and refine a systematic map (classification schema). In addition, the authors conduct bibliometric, demographic, and trend analysis of the included papers. The study pool includes a set of 90 papers (from 114 identified papers) published in the area of UML-SPE between 1998 and 2011. The authors derive the trends in terms of types of papers, types of SPE activities, and types of evaluations. They also report the demographics and bibliometrics trends in this domain and discuss the emerging trends in UML-SPE and the implications for researchers and practitioners in this area. © 2013, IGI Global.","<b>Authors:</b><br/>Garousi V., Shahnewaz S., Krishnamurthy D. <br/><b>Key words:</b><br/>"
332,paper_332,"Oliveira K., Pimentel J., Santos E., Dermeval D., Guedes G., Souza C., Soares M., Castro J., Alencar F., Silva C.",,"The celebration of 25th anniversary of the Brazilian Symposium of Software Engineering (SBES) as well as the forthcoming Requirements Engineering Conference to be held in Brazil for the first time, has led us to have a closer look at the local Requirements Engineering (RE) Community. A systematic mapping was performed in order to find out the main Brazilian research groups, authors as well as their topics of interest and publications with greatest impact. This information may be useful for those that do not know well the local requirements engineering community, such as local newcomers or foreign researchers. It may also help to identify potential groups for collaboration. Similarly, it may provide valuable information to assist local agencies when granting research funds. © Springer-Verlag Montevideo 2013.","<b>Authors:</b><br/>Oliveira K., Pimentel J., Santos E., Dermeval D., Guedes G., Souza C., Soares M., Castro J., Alencar F., Silva C. <br/><b>Key words:</b><br/>Requirements engineering, Systematic mapping"
333,paper_333,"Cobb W.E., Baldwin R.O., Laspe E.D.",,"We propose a generalized framework to evaluate the side-channel information leakage of symmetric block ciphers. The leakage mapping methodology enables the systematic and efficient identification and mitigation of problematic information leakages by exhaustively considering relevant leakage models. The evaluation procedure bounds the anticipated resistance of an implementation to the general class of univariate differential side-channel analysis techniques. Typical applications are demonstrated using the well-known Hamming weight and Hamming distance leakage models, with recommendations for the incorporation of more accurate models. The evaluation results are empirically validated against correlation-based differential side-channel analysis attacks on two typical unprotected implementations of the Advanced Encryption Standard. © 2013 ACM.","<b>Authors:</b><br/>Cobb W.E., Baldwin R.O., Laspe E.D. <br/><b>Key words:</b><br/>Advanced encryption standard, Block cipher, Cryptanalysis, Cryptography, Differential power analysis, Encryption, Hardware security, Information leakage, Physical-layer security, Side-channel analysis"
334,paper_334,"Jung J., Bae S.-H., Lee J.H., Kim M.-S.",,"Human communication significantly relies on the expressivity of their body movements. Based on these body language experiences, humans tend to extract meanings even from movements of objects. This paper begins with the above human tendencies to create a design method that can help product designers make their products move to communicate. As a research vehicle, we created a robotic torso prototype and utilized it to collaborate with movement experts, and listed up possible expressive movement components. We then built a mapping matrix that links these movements to general product messages. A method which utilizes this mapping matrix was developed to help designers determine a set of effective movements that can communicate specific product messages. Lastly, a design work-shop was conducted to identify the usefulness of the proposed method. We expect the procedures and findings of this study to help researchers and designers approach affective user experience through product movement design. Copyright © 2013 ACM.","<b>Authors:</b><br/>Jung J., Bae S.-H., Lee J.H., Kim M.-S. <br/><b>Key words:</b><br/>Design method, Product movement"
335,paper_335,"Borges A., Soares S., Meira S., Tomaz H., Rocha R., Costa C.",,"Background: Along the last decade, there has been a significant increase in the adoption of the approaches based on Distributed Software Development (DSD). This approach has brought several competitive advantages, as well as new challenges such as communication and information sharing. In this context, the ontologies can provide benefits such as the definition, standardization and sharing of knowledge involved in the project, allowing a uniform understanding of information and facilitating the collaboration among distributed software development teams. Aim: Identifying evidence to determine which tools, models, techniques and best practices that use ontologies to support the DSD projects, and which ontologies proposed in this context. Method: This paper presents a systematic mapping study conducted in order to investigate how ontologies are being applied as a support to the DSD. The research protocol was based on Kitchenham's, and Travassos and Biolchini's guidelines. Searches were performed both in manual and automatic way in a set of digital libraries engines and leading conferences in the Software Engineering field. Results: From the initial set of 1588 studies, it was selected a total of 38 primary studies that answer the two research questions. Conclusions: This work presents evidences from each paper collected and an analysis of results reached. The results support the foundation for proposing and developing a feature based on ontologies to support the DSD, besides encouraging further researches that may promote advancements in this area and fostering the adoption of these types of resources by the global software industry. Copyright 2013 ACM.","<b>Authors:</b><br/>Borges A., Soares S., Meira S., Tomaz H., Rocha R., Costa C. <br/><b>Key words:</b><br/>Distributed software development, Empirical Software Engineering, Ontology, Systematic mapping study"
336,paper_336,"Kuhrmann M., Fernández D.M., Tiessler M.",,"Context: Software processes have become inherently complex to cope with the various situations we face in industrial project environments. In response to this problem, the research area of Method Engineering arose in the 1990s aiming at the systematization of process construction. Objective: Although the research area has gained much attention and offered a plethora of contributions so far, we still have little knowledge about the feasibility of Method Engineering. To overcome this shortcoming, necessary is a systematic investigation of the respective publication flora. Method: We conduct a systematic mapping study and investigate, inter alia, which contributions were made over time and which research type facet they address to distill a common understanding of the state-of-the-art. Results: Based on the review of 64 publications, our results show that most of those contributions only repeat and discuss formerly introduced concepts, whereas empirically sound evidence on the feasibility of Method Engineering, is still missing. Conclusion: Although the research area constitutes many contributions, yet missing are empirically sound investigations that would allow for practical application and experience extraction. Copyright 2013 ACM.","<b>Authors:</b><br/>Kuhrmann M., Fernández D.M., Tiessler M. <br/><b>Key words:</b><br/>Mapping study, Situational method engineering, Systematic literature review"
337,paper_337,"Sulaman S.M., Weyns K., Höst M.",,"Context: At the same time as our dependence on IT systems increases, the number of reports of problems caused by failures of critical IT systems has also increased. This means that there is a need for risk analysis in the development of this kind of systems. Risk analysis of technical systems has a long history in mechanical and electrical engineering. Objective: Even if a number of methods for risk analysis of technical systems exist, the failure behavior of information systems is typically very different from mechanical systems. Therefore, risk analysis of IT systems requires different risk analysis techniques, or at least adaptations of traditional approaches. This means that there is a need to understand what types of methods are available for IT systems and what research that has been conducted on these methods. Method: In this paper we present a systematic mapping study on risk analysis for IT systems. 1086 unique papers were identified in a database search and 57 papers were identified as relevant for this study. These papers were classified based on 5 different criteria. Results: This classification, for example, shows that most of the discussed risk analysis methods are qualitative and not quantitative and that most of the risk analysis methods that are presented in these papers are developed for IT systems in general and not for specific types of IT system. Conclusions: The results show that many new risk analysis methods have been proposed in the last decade but even more that there is a need for more empirical evaluations of the different risk analysis methods. Many papers were identified that propose new risk analysis methods, but few papers discuss a systematic evaluation of these methods or a comparison of different methods based on empirical data. Copyright 2013 ACM.","<b>Authors:</b><br/>Sulaman S.M., Weyns K., Höst M. <br/><b>Key words:</b><br/>IT systems, Mapping study, Risk analysis"
338,paper_338,"Li Z., Liang P., Avgeriou P.",,"Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which ""Embedded software"" has received the most attention. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Li Z., Liang P., Avgeriou P. <br/><b>Key words:</b><br/>Architecting activity, Knowledge-based approach, Software architecture, Systematic mapping study"
339,paper_339,"Farhoodi R., Garousi V., Pfahl D., Sillito J.",,"Scientific and engineering research is heavily dependent on effective development and use of software artifacts. Many of these artifacts are produced by the scientists themselves, rather than by trained software engineers. To address the challenges in this area, a research community often referred to as ""Development of Scientific Software"" has emerged in the last few decades. As this research area has matured, there has been a sharp increase in the number of papers and results made available, and it has thus become important to summarize and provide an overview about those studies. Through a systematic mapping and bibliometrics study, we have reviewed 130 papers in this area. We present the results of our study in this paper. Also we have made the mapping data available on an online repository which is planned to be updated on a regular basis. The results of our study seem to suggest that many software engineering techniques and activities are being used in the development of scientific software. However, there is still a need for further exploration of the usefulness of specific software engineering techniques (e.g., regarding software maintenance, evolution, refactoring, re(v)-engineering, process and project management) in the scientific context. It is hoped that this article will help (new) researchers get an overview of the research space and help them to understand the trends in the area. © 2013 World Scientific Publishing Company.","<b>Authors:</b><br/>Farhoodi R., Garousi V., Pfahl D., Sillito J. <br/><b>Key words:</b><br/>Bibliometrics study, Development of scientific software, Paper repository, Systematic mapping"
340,paper_340,"Lobato L.L., Bittar T.J., Neto P.A.D.M.S., MacHado I.D.C., De Almeida E.S., Meira S.R.D.L.",,"Software Product Line (SPL) Engineering focuses on systematic software reuse, which has benefits such as reductions in time-to-market and effort, and improvements in the quality of products. However, establishing a SPL is not a simple matter, and can affect all aspects of the organization, since the approach is complex and involves major investment and considerable risk. These risks can have a negative impact on the expected ROI for an organization, if SPL is not sufficiently managed. This paper presents a mapping study of Risk Management (RM) in SPL Engineering. We analyzed a set of thirty studies in the field. The results points out the need for risk management practices in SPL, due to the little research on RM practices in SPL and the importance of identifying insight on RM in SPL. Most studies simply mention the importance of RM, however the steps for managing risk are not clearly specified. Our findings suggest that greater attention should be given, through the use of industrial case studies and experiments, to improve SPL productivity and ensure its success. This research is a first attempt within the SPL community to identify, classify, and manage risks, and establish mitigation strategies. © 2013 World Scientific Publishing Company.","<b>Authors:</b><br/>Lobato L.L., Bittar T.J., Neto P.A.D.M.S., MacHado I.D.C., De Almeida E.S., Meira S.R.D.L. <br/><b>Key words:</b><br/>Mapping study, Risk management, Software product lines"
341,paper_341,"Kritikakou A., Catthoor F., Athanasiou G.S., Kelefouras V., Goutis C.",,"A systematic methodology for near-optimal software/hardware codesign mapping onto an FPGA platform with microprocessor and HW accelerators is proposed. The mapping steps deal with the inter-organization, the foreground memory management, and the datapath mapping. A step is described by parameters and equations combined in a scalable template. Mapping decisions are propagated as design constraints to prune suboptimal options in next steps. Several performance-area Pareto points are produced by instantiating the parameters. To evaluate our methodology we map a real-time bio-imaging application and loop-dominated benchmarks. © 2013 ACM.","<b>Authors:</b><br/>Kritikakou A., Catthoor F., Athanasiou G.S., Kelefouras V., Goutis C. <br/><b>Key words:</b><br/>Area reduction, FPGA, Near optimal, Real-time behavior"
342,paper_342,"Sadeghi A., Fröhlich H.",,"Background: Analysis and interpretation of biological networks is one of the primary goals of systems biology. In this context identification of sub-networks connecting sets of seed proteins or seed genes plays a crucial role. Given that no natural node and edge weighting scheme is available retrieval of a minimum size sub-graph leads to the classical Steiner tree problem, which is known to be NP-complete. Many approximate solutions have been published and theoretically analyzed in the computer science literature, but far less is known about their practical performance in the bioinformatics field.Results: Here we conducted a systematic simulation study of four different approximate and one exact algorithms on a large human protein-protein interaction network with ~14,000 nodes and ~400,000 edges. Moreover, we devised an own algorithm to retrieve a sub-graph of merged Steiner trees. The application of our algorithms was demonstrated for two breast cancer signatures and a sub-network playing a role in male pattern baldness.Conclusion: We found a modified version of the shortest paths based approximation algorithm by Takahashi and Matsuyama to lead to accurate solutions, while at the same time being several orders of magnitude faster than the exact approach. Our devised algorithm for merged Steiner trees, which is a further development of the Takahashi and Matsuyama algorithm, proved to be useful for small seed lists. All our implemented methods are available in the R-package SteinerNet on CRAN (http://www.r-project.org) and as a supplement to this paper. © 2013 Sadeghi and Fröhlich, licensee BioMed Central Ltd.","<b>Authors:</b><br/>Sadeghi A., Fröhlich H. <br/><b>Key words:</b><br/>"
343,paper_343,"Wautelet Y., Kolp M.",,"[Context and Motivation] Business modeling is nowadays a common approach in huge enterprise software developments. It notably allows to align business processes and supporting IT solutions at best, to produce a documentation of the company's ""savoir-faire"" and to look for possible optimizations. The business modeling discipline of the Rational Unified Process (RUP) has enriched the semantic of the Unified Modeling Language's (UML) use case diagrams for the special purpose of representing the organization's processes with accurate elements. [Question/Problem] RUP/UML business use case scemantics are nevetheless only intended to further stereotype use case models and not to be used for reasoning. In parallel and in line with artificial intelligence concepts, researchers have developed the i* framework enabling the evaluation and decomposition of multiple design opportunities. RUP/UML business use case scemantics could be used more efficiently to integrate the latter benefits. [Principal ideas/results] Through a systematic mapping of elements from i* on the one side and of the RUP/UML business use case model on the other, we have set up a RUP/UML graphical notation for i* elements. Applicability has been shown on an illustrative example. [Contribution] The main contribution of the framework is allowing to model in an i* fashion using CASE-tools meant for RUP/UML and proposing an interface for forward engineering the produced model in a classical UML requirements model. Future work is required to fully validate the proposal, notably to measure the method's efficacy. © 2013 Springer-Verlag.","<b>Authors:</b><br/>Wautelet Y., Kolp M. <br/><b>Key words:</b><br/>Business Modeling, i*, RUP/UML Business Use Case Model"
344,paper_344,"Omer I.",,"In this article we report results from an investigation conducted to ascertain how the potential for perception-based rotation distortion in direction estimation is affected by regional categories. To do so we tested subjects' knowledge about the direction of geographical locations in the northern part of Israel and in hypothetical maps. The results clearly indicate that regional categories do affect rotation distortion when direction estimation involves cities belonging to different regional categories. Based on these findings, we conclude that categorical representation has priority over perception-based representation in estimation when conditions appropriate to each type of representation simultaneously appear in a geographical environment. © 2013 Copyright Taylor and Francis Group, LLC.","<b>Authors:</b><br/>Omer I. <br/><b>Key words:</b><br/>cognitive mapping, direction estimation, regional categories, rotation, spatial representation, systematic distortions"
345,paper_345,"Durelli V.H.S., Araujo R.F., Silva M.A.G., Oliveira R.A.P.D., Maldonado J.C., Delamaro M.E.",,"Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we have synthesized its 25-year history of research on software testing. Using information drawn from this overview we highlighted which software testing topics have been the most extensively surveyed in SBES literature. We have also devised a co-authorship network to depict the most prolific research groups and researchers. Moreover, by performing a citation analysis of the selected studies we have tried to ascertain the importance of SBES in a wider scenario. Finally, using the information extracted from the studies, we have shed light on the state-of-the-art of software testing in Brazil and provided an outlook on its foreseeable future. © 2012 Elsevier Inc.","<b>Authors:</b><br/>Durelli V.H.S., Araujo R.F., Silva M.A.G., Oliveira R.A.P.D., Maldonado J.C., Delamaro M.E. <br/><b>Key words:</b><br/>Brazilian research, Software testing, Systematic mapping"
346,paper_346,"Abdellatief M., Sultan A.B.M., Ghani A.A.A., Jabar M.A.",,"A component-based software system (CBSS) is a software system that is developed by integrating components that have been deployed independently. In the last few years, many researchers have proposed metrics to evaluate CBSS attributes. However, the practical use of these metrics can be difficult. For example, some of the metrics have concepts that either overlap or are not well defined, which could hinder their implementation. The aim of this study is to understand, classify and analyze existing research in component-based metrics, focusing on approaches and elements that are used to evaluate the quality of CBSS and its components from a component consumer's point of view. This paper presents a systematic mapping study of several metrics that were proposed to measure the quality of CBSS and its components. We found 17 proposals that could be applied to evaluate CBSSs, while 14 proposals could be applied to evaluate individual components in isolation. Various elements of the software components that were measured are reviewed and discussed. Only a few of the proposed metrics are soundly defined. The quality assessment of the primary studies detected many limitations and suggested guidelines for possibilities for improving and increasing the acceptance of metrics. However, it remains a challenge to characterize and evaluate a CBSS and its components quantitatively. For this reason, much effort must be made to achieve a better evaluation approach in the future. © 2012 Elsevier Inc.","<b>Authors:</b><br/>Abdellatief M., Sultan A.B.M., Ghani A.A.A., Jabar M.A. <br/><b>Key words:</b><br/>Component-based software system, Software components, Software metrics, Software quality, Systematic mapping study"
347,paper_347,"Mehmood A., Jawawi D.N.A.",,"Context: Model-driven code generation is being increasingly applied to enhance software development from perspectives of maintainability, extensibility and reusability. However, aspect-oriented code generation from models is an area that is currently underdeveloped. Objective: In this study we provide a survey of existing research on aspect-oriented modeling and code generation to discover current work and identify needs for future research. Method: A systematic mapping study was performed to find relevant studies. Classification schemes have been defined and the 65 selected primary studies have been classified on the basis of research focus, contribution type and research type. Results: The papers of solution proposal research type are in a majority. All together aspect-oriented modeling appears being the most focused area divided into modeling notations and process (36%) and model composition and interaction management (26%). The majority of contributions are methods. Conclusion: Aspect-oriented modeling and composition mechanisms have been significantly discussed in existing literature while more research is needed in the area of model-driven code generation. Furthermore, we have observed that previous research has frequently focused on proposing solutions and thus there is need for research that validates and evaluates the existing proposals in order to provide firm foundations for aspect-oriented model-driven code generation. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Mehmood A., Jawawi D.N.A. <br/><b>Key words:</b><br/>Aspect-oriented software development, Code generation, Model-driven engineering, Systematic map"
348,paper_348,"Vanhanen J., Mäntylä M.V.",,"Previous systematic literature reviews on pair programming (PP) lack in their coverage of industrial PP data as well as certain factors of PP such as infrastructure. Therefore, we conducted a systematic mapping study on empirical, industrial PP research. Based on 154 research papers, we built a new PP framework containing 18 factors. We analyzed the previous research on each factor through several research properties. The most thoroughly studied factors in industry are communication, knowledge of work, productivity and quality. Many other factors largely lack comparative data, let alone data from reliable data collection methods such as measurement. Based on these gaps in research further studies would be most valuable for development process, targets of PP, developers' characteristics, and feelings of work. We propose how they could be studied better. If the gaps had been commonly known, they could have been covered rather easily in the previous empirical studies. Our results help to focus further studies on the most relevant gaps in research and design them based on the previous studies. The results also help to identify the factors for which systematic reviews that synthesize the findings of the primary studies would already be feasible. © 2013 World Scientific Publishing Company.","<b>Authors:</b><br/>Vanhanen J., Mäntylä M.V. <br/><b>Key words:</b><br/>empirical studies, gaps in research, industrial studies, Pair programming, systematic mapping study"
349,paper_349,"Ouhbi S., Idri A., Fernández-Alemán J.L., Toval A.",,"Software quality requirements (SQR) play a central role in software quality (SQ) success. The importance of mastering SQR can be seen as obvious, however, when it comes to customer satisfaction, end-users are often dissatisfied with SQ. In this paper, a systematic mapping study aims to summarize SQR research by answering nine mapping questions. In total, 51 articles were selected and classified according to multiple criteria: publication source, publication year, research type, research approach, contribution type of SQR literature, requirements engineering activity, well-known SQ model, software artifact and SQR type. The results show an increased interest in SQR research in recent years and reveal that conferences are the main SQR publication target. Most SQR research has used case studies. The dominant contribution type of SQR research is method while specification is the main requirements engineering activity identified. SQ models need to be more used for SQR identification. Design module and requirements documentation are the principal artifacts reported in SQR literature. External and internal SQR were the main SQR types addressed in literature. Identifying empirical solutions to address SQR is a promising research direction for researchers. © 2013 IEEE.","<b>Authors:</b><br/>Ouhbi S., Idri A., Fernández-Alemán J.L., Toval A. <br/><b>Key words:</b><br/>Requirement engineering, Software quality requirements, Systematic mapping study"
350,paper_350,"Costa C., Murta L.",,"Along the last decade, many companies started using Distributed Software Development (DSD). The distribution of the software development teams over the globe has become almost a rule in large companies. However, in this context, new problems arise, which mainly involve the physical and temporal distance among the participants. Some studies show that deploying a version control system to alleviate this problem is a big challenge for distributed teams. This paper presents a systematic mapping study about works about version control that focus on DSD. We found 29 studies related to DSD version control, published between 2002 and 2012. Using the systematically extracted data from these works, we present challenges, tools, and other solutions proposed to version control in DSD. These results can support practitioners and researchers to better understand and overcome the challenges related do DSD version control, and devise more effective solutions to improve version control in a distributed setting. © 2013 IEEE.","<b>Authors:</b><br/>Costa C., Murta L. <br/><b>Key words:</b><br/>Configuration management, Distributed Software Development, Systematic mapping study, Version control"
351,paper_351,"Steinmacher I., Chaves A.P., Gerosa M.A.",,"The developers' physical dispersion in Distributed Software Development (DSD) imposes challenges related to awareness support during collaboration in such scenario. In this paper, we present a systematic literature review and mapping that gathered, analyzed, and classified studies that improve awareness support in DSD, providing an overview of the area. Our initial search returned 1967 papers, of which 91 were identified as reporting some awareness support to DSD. These papers were then analyzed, and classified according to the 3 C collaboration model and to the Gutwin et al. Awareness Framework. Our findings suggest that awareness in DSD is gaining increasingly attention, 71 out of 91 papers were published from 2006 to 2010. Most part of the papers presented tools with some awareness support. The classification showed that the coordination is by far the most supported dimension of the 3C model, while communication is the less explored. It also showed that workspace awareness elements play a central role on DSD collaboration. © 2012 Springer.","<b>Authors:</b><br/>Steinmacher I., Chaves A.P., Gerosa M.A. <br/><b>Key words:</b><br/>Awareness, Communication, Cooperation, Coordination, Distributed software development, Systematic mapping, Systematic review"
352,paper_352,"Souza E.F., Falbo R.A., Vijaykumar N.L.",,"With the growth of data from several different sources of knowledge within an organization, it becomes necessary to provide computerized support for tasks of acquiring, processing, analyzing and disseminating knowledge. In the software process, testing is a critical factor for product quality, and thus there is an increasing concern in how to improve the accomplishment of this task. In software testing, finding relevant knowledge to reuse can be a difficult and complex task, due to the lack of a strategy to represent or to associate semantics to a large volume of test data, including test cases, testing techniques to be applied and so on. This paper aims to investigate, through a Systematic Mapping of the Literature, some aspects associated with applying Knowledge Management to Software Testing. Copyright © 2013 by Knowledge Systems Institute Graduate School.","<b>Authors:</b><br/>Souza E.F., Falbo R.A., Vijaykumar N.L. <br/><b>Key words:</b><br/>Knowledge management, Software testing, Systematic mapping"
353,paper_353,"Hellmann T.D., Chokshi A., Abad Z.S.H., Pratte S., Maurer F.",,"Unit and acceptance testing are central to agile software development, but is that all there is to agile testing? We build on previous work to provide a systematic mapping of agile testing publications at major agile conferences. The analysis presented in this paper allows us to answer research questions like: what is agile testing used for, what types of studies on agile testing have been published, what problems do people have when performing agile testing, and what benefits do these publications offer? We additionally explore topics such as: who are the major authors in this field, in which countries do these authors work, what tools are mentioned, and is the field driven by academics, practitioners, or collaborations? This paper presents our analysis of these topics in order to better structure future work in the field of agile testing and to provide a better understanding of what this field actually entails. © 2013 IEEE.","<b>Authors:</b><br/>Hellmann T.D., Chokshi A., Abad Z.S.H., Pratte S., Maurer F. <br/><b>Key words:</b><br/>agile software development, empirical, software testing, systematic mapping, test-driven development, testing tools"
354,paper_354,"Mendonça D.F., Rodrigues G.N., Favacho A., Holanda M.",,"In the last years, the field of service oriented computing (SOC) has received a growing interest from researchers and practitioners, particularly with respect to quality of service (QoS). Aim: This paper presents a mapping study to aggregate literature in this field in order to find trends and research opportunities regarding QoS in SOC. Method: Following well established mapping study protocol, we collected data from major digital libraries and analysed 364 papers aided by a tool developed for this purpose. Results: With respect to SOC contributions dealing with QoS properties, we were able to find out which SOC as well as which QoS facets are the focus of research. Our mapping was also able to identify those research groups that have mostly published in the context of our study. Conclusions: Most of the studies concentrate on runtime issues, such as monitoring and adaptation. Besides, an expressive amount of papers focused on metrics, computational models or languages for the context of Qos in SOC. Regarding quality attributes, a vast majority of the papers use generic models, so that the proposed solutions are independent of the particularities of a quality attribute. In spite of that, our study reveal that availability, performance and reliability were the major highlights. With respect to research type, many of the reviewed studies propose new solutions, instead of evaluating and validating existing proposals - a symptom of a field that still needs established research paradigms. © 2013 IEEE.","<b>Authors:</b><br/>Mendonça D.F., Rodrigues G.N., Favacho A., Holanda M. <br/><b>Key words:</b><br/>"
355,paper_355,"Pergher M., Rossi B.",,"In this paper, we report about a systematic mapping study in software requirements prioritization with a specific focus on empirical studies.The results show that the interest from the research community is clustered around the more recent years. The majority of the studies are about the validation of research or solution proposals. We report the prevalence of studies on techniques and methodologies while there is a scarce interest in the strict evaluation of tools that could be beneficial to industry. In most of the empirical studies we found a bottom-up approach, centering on the techniques and on accuracy as the dependent variable, as well as on functional requirements as the main research focus. Based on the results, we provide recommendations for future research directions. © 2013 IEEE.","<b>Authors:</b><br/>Pergher M., Rossi B. <br/><b>Key words:</b><br/>"
356,paper_356,"Rocha R.G.C., Azevedo R.R., Mendonça S., Borges A.N., Costa C., Meira S.",,"The distributed Software Development (DSD) has brought several competitive advantages in software industry, as well as new challenges such as communication and information sharing. In this context, the ontologies can provide benefits such as the definition, standardization and sharing of knowledge involved in the project, allowing a uniform understanding of information and facilitating the collaboration among distributed software development teams. This paper presents a systematic mapping study conducted in order to investigate which ontologies proposed for this context. This work presents evidences from each paper collected and an brief analysis of results reached. The results support the foundation for proposing and developing a feature based on ontologies to support the DSD. The Searches were performed both in manual and automatic way in a set of digital libraries engines and leading conferences in the Software Engineering field. The results support the foundation for proposing and developing a feature based on ontologies to support DSD, besides encouraging further researches that may promote advancements in this area and fostering the adoption these resources by the global software industry. © 2013 by Knowledge Systems Institute Graduate School.","<b>Authors:</b><br/>Rocha R.G.C., Azevedo R.R., Mendonça S., Borges A.N., Costa C., Meira S. <br/><b>Key words:</b><br/>Distributed software development, Empirical software engineering, Ontology, Systematic mapping study"
357,paper_357,"Rosli M.M., Tempero E., Luxton-Reilly A.",,"Background: The quality of data sets used in software engineering research is of the utmost importance. To ensure credibility of results obtained from use of data sets, the quality of the data must be examined. Objective: This study provides an overview of recent research (2008-2012) involving data quality in software engineering data sets, with the goal of generally understanding what research there is that addresses data quality, and in particular to determine to what degree researchers have addressed any data quality issues in order to evaluate the trustworthiness of their results. Method: We performed a systematic mapping study to investigate treatment of data quality issues in software engineering research. A total of 64 papers published from 2008 to 2012 explicitly address issues with the quality of data and use software engineering data sets. These studies were classified according to the data quality topic, data set and data quality problem. Results: We found only 31 studies gave serious consideration for how the quality of the data affected their results. We observed that there is a lack of clear and consistent terminology regarding data quality, especially with respect to the kinds of quality problems a data set might have. As a first step to address this problem, we propose a model that describes the lifecycle that research data goes through when used in research. Conclusions: The results suggest that researchers should give more attention to the quality of data sets in order to produce trustworthy data for reliable empirical research, and that the research community needs to better understand and communicate issues with data quality. © 2013 IEEE.","<b>Authors:</b><br/>Rosli M.M., Tempero E., Luxton-Reilly A. <br/><b>Key words:</b><br/>Data quality, Empirical studies, Software engineering data sets, Systematic mapping"
358,paper_358,"Nardi J.C., De Almeida Falbo R., Almeida J.P.A.",,"Enterprise Application Integration (EAI) plays an important role by linking heterogeneous applications in order to support business processes within and across organizations. In this context, semantic conflicts often arise and have to be dealt with to ensure successful interoperation. In recent years, many EAI initiatives have aimed at addressing semantic interoperability challenges by employing ontologies in various ways. This paper aims to reveal, through a systematic review method, some aspects associated with semantic EAI initiatives and the adoption of ontologies by them, namely: (i) the business application domains in which these initiatives have been conducted, (ii) the focus of the initiatives regarding integration layers (data, message/service, and process), (iii) the adoption of ontologies by EAI research along the years, and (iv) the characteristics of these ontologies. We provide a panorama of these aspects and identify gaps and trends that may guide further research. © 2013 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Nardi J.C., De Almeida Falbo R., Almeida J.P.A. <br/><b>Key words:</b><br/>enterprise application integration, ontology, semantics, systematic mapping"
359,paper_359,"Kayastha P., Dhital M.R., De Smedt F.",,"Landslide problems are abundant in the mountainous areas of Nepal due to a unique combination of adverse geological conditions, abundant rainfall and anthropogenic factors, which leads to enormous loss of life and property every year. To control such problems, systematic studies of landslides are necessary, including inventory mapping and risk assessment. Analytical hierarchy process method in the Tinau watershed, Nepal. A landslide susceptibility map is prepared on the basis of available digital data of topography, geology, land-use and hydrology. The landslide susceptibility map is validated through physical and statistical methods. The results reveal that the predicted susceptibility levels are found to be in good agreement with the past landslide occurrences, and, hence, the map is trustworthy for future land-use planning. © 2012 Elsevier Ltd.","<b>Authors:</b><br/>Kayastha P., Dhital M.R., De Smedt F. <br/><b>Key words:</b><br/>Analytical hierarchy process, Geographic information system (GIS), Landslide susceptibility, Nepal, Tinau watershed"
360,paper_360,"Raza B., MacDonell S.G., Clear T.",,"This paper reports our extended analysis of the recent literature addressing global software engineering (GSE), using a new Systematic Snapshot Mapping (SSM) technique. The primary purpose of this work is to understand what issues are being addressed and how research is being carried out in GSE - and comparatively, what work is not being conducted. We carried out the analysis in two stages. In the first stage we analyzed 275 papers published between January 2011 and June 2012, and in the second stage we augmented our analysis by considering a further 26 papers (from the 2013 International Conference on Global Software Engineering (ICGSE'13). Our results reveal that, currently, GSE studies are focused on management- and infrastructure-related factors, using principally evaluative research approaches. Most of the studies are conducted at the organizational level, mainly using methods such as interviews, surveys, field studies and case studies. The USA, India and China are major players in GSE, with USA-India collaborations being the most frequently studied, followed by USA-China. While a considerable number of GSE-related studies have been published since January 2011 they are currently quite narrowly focused, on exploratory research and explanatory theories, and the critical research paradigm has been untouched. An absence of formulative research, experimentation and simulation, and a related focus on evaluative approaches, all suggest that existing tools, methods and approaches from related fields are being tested in the GSE context, even though these may not be inherently applicable to the additional scale and complexity of GSE. © Springer-Verlag Berlin Heidelberg 2013.","<b>Authors:</b><br/>Raza B., MacDonell S.G., Clear T. <br/><b>Key words:</b><br/>Classification, Distributed Software Development, Global Software Engineering (GSE), Systematic Mapping"
361,paper_362,"Ramos C.S., Rocha A.R., De Oliveira K.M.",,"Software processes improvement (SPI) program have been used by several organizations that aim to have better competitive advantage. Since it involves high costs (time and money), it is essential to have some estimate about the return on investment when applying SPI programs. After a large study of the literature, by performing a systematic mapping, we concluded that despite of the existence of several initiatives, the estimate of return on investment for SPI is not very applied in practice, and it is still an open and challenging issue. In general, the existing approaches propose to perform the estimation using a monetary unit, which makes the process difficult to be applied. Based on the results of this study, we propose a light approach to estimate the return on investment for SPI, that considers not only economics aspects but also any kind of benefits obtained by the organization aligned to their business strategy. In this paper, we present the systematic mapping, our conclusions and this approach that we called a strategy for analyzing benefits of SPI programs, defined based on the results of the systematic mapping. Copyright © 2013 by Knowledge Systems Institute Graduate School.","<b>Authors:</b><br/>Ramos C.S., Rocha A.R., De Oliveira K.M. <br/><b>Key words:</b><br/>Return on investment, Software process improvement"
362,paper_363,"Ravindran K., Wu J., Adiththan A.",,"The paper deals with service abstractions that allow managing the dependability features of networked application systems. Component-level faults in the infrastructure are mapped onto degradations in the service quality space. Fault recovery mechanisms in a service-layer algorithm, if any, purport to maintain either the same level of service as in the no-fault case or a reduced level of service proportionate to the fault severity. Treating the occurrence of component faults as a normal case rather than as an exception, a virtualization mechanism provides a systematic mapping of faults onto canonical service quality spaces in a measurable way (e.g., service availability and agility). Our paper describes the generic mechanisms and programming interfaces in the virtualization layer to manage the infrastructure in a flexible way that allows attaining an optimal service quality. Such service layer abstractions are useful for applications to infuse adaptive fault-tolerance and quality assurance. © 2013 IEEE.","<b>Authors:</b><br/>Ravindran K., Wu J., Adiththan A. <br/><b>Key words:</b><br/>"
363,paper_364,"Marques A.B., Carvalho J.R., Rodrigues R., Conte T., Prikladnicki R., Marczak S.",,"An adequate task allocation plan is an effective strategy to reduce collaboration issues in distributed software development. Practitioners adopt distinct processes to allocate tasks as well as diverse labels for the same activities and artifacts. This diversity is also found in literature. Task allocation proposals consider different elements and use distinct names for the same concepts. The lack of a standardized vocabulary and of an understanding of the elements involved impairs knowledge acquisition and sharing. Our paper presents a domain ontology to represent concepts related to task allocation in distributed teams. The ontology was defined based on a literature systematic mapping and on the opinion of experts. Preliminary evaluation suggests that the relationships among concepts are valid in real projects. The ontology brings awareness to managers regarding the factors related to task allocation planning and provides researchers with a framework to define processes and design tools to support such activity. © 2013 IEEE.","<b>Authors:</b><br/>Marques A.B., Carvalho J.R., Rodrigues R., Conte T., Prikladnicki R., Marczak S. <br/><b>Key words:</b><br/>Distributed software development, Domain ontology, Task allocation"
364,paper_365,"Zapata L., Moreno A.M., Eduardo F.-M.",,"Keeping information systems secure is costly. Organizations allocate financial and human resources in order to prevent security incidents having an impact on software applications. There is evidence that information systems security has in some cases been affected by human errors that might be caused by a poor usability design. There is clearly a link between security and usability. To clarify this, we have conducted a systematic mapping study of the literature produced over the last decade. We identified five relationship types: inverse, direct, relative, one-way inverse, and no-relationship. Most authors agree that there is an inverse relationship between security and usability, which means that increasing usability leads to a decrease in security issues in a product and vice versa. However, this is not a unanimous finding, and this study unveils a number of open questions, like application domain dependency and the need to explore lower level relationships between attribute sub-characteristics. Copyright © 2013 SCITEPRESS.","<b>Authors:</b><br/>Zapata L., Moreno A.M., Eduardo F.-M. <br/><b>Key words:</b><br/>"
365,paper_366,"Copeland M., Gonçalves R.S., Parsia B., Sattler U., Stevens R.",,"Understanding ontology evolution is becoming an active topic of interest to ontology engineers, e.g., we have large collaborative developed ontologies but, unlike software engineering, comparatively little is understood about the dynamics of historical changes, especially at a fine level of granularity. Only recently has there been a systematic analysis of changes across ontology versions, but still at a coarse-grained level. The National Cancer Institute (NCI) Thesaurus (NCIt) is a large, collaboratively-developed ontology, used for various Web and research-related purposes, e.g., as a medical research controlled vocabulary. The NCI has published ten years worth of monthly versions of the NCIt as Web Ontology Language (OWL) documents, and has also published reports on the content of, development methodology for, and applications of the NCIt. In this paper, we carry out a fine-grained analysis of the asserted axiom dynamics throughout the evolution of the NCIt from 2003 to 2012. From this, we are able to identify axiomatic editing patterns that suggest significant regression editing events in the development history of the NCIt.","<b>Authors:</b><br/>Copeland M., Gonçalves R.S., Parsia B., Sattler U., Stevens R. <br/><b>Key words:</b><br/>"
366,paper_367,"Burns C., Ferreira J., Hellmann T.D., Maurer F.",,"Modern software development often involves the use of complex, reusable components called Application Programming Interfaces (APIs). Developers use APIs to complete tasks they could not otherwise accomplish in a reasonable time. These components are now vital to mainstream software development. But as APIs have become more important, understanding how to make them more usable is becoming a significant research question. To assess the current state of research in the field, we conducted a systematic mapping. A total of 28 papers were reviewed and categorized based on their research type and on the evaluation method employed by its authors. We extended the analysis of a subset of the papers we reviewed beyond the usual limits of a systematic map in order to more closely examine details of their evaluations - such as their structure and validity - and to summarize their recommendations. Based on these results, common problems in the field are discussed and future research directions are suggested. © 2012 IEEE.","<b>Authors:</b><br/>Burns C., Ferreira J., Hellmann T.D., Maurer F. <br/><b>Key words:</b><br/>API usability, application programming interface, meta-analysis, systematic map, systematic review"
367,paper_368,"Chi H.-C., Ferng Jr. F., Hsieh Y.-C.",,"Network-on-chip (NoC) architectures have been recently proposed as the communication framework for large-scale chips. A well-designed NoC architecture can facilitate the IP cores to communicate with each other efficiently. In this paper, we propose a systematic mapping scheme, called area utilization based mapping (AUBM), to map the IP cores from the communication core graph to the mesh network. In AUBM, the IP cores can be of various sizes. Extensive experiments have been conducted for evaluating the mapping schemes. AUBM is compared with previously proposed schemes for different applications as well as synthetic workloads. Our experiment results show that AUBM outperforms others in almost all cases in terms of the mapping cost involving traffic volume and chip area. © 2012 IEEE.","<b>Authors:</b><br/>Chi H.-C., Ferng Jr. F., Hsieh Y.-C. <br/><b>Key words:</b><br/>communication core graph, IP mapping, network-on-chip architectures, routing switches, SoC design"
368,paper_369,"Quintana G., Solari M.",,"In the literature there are many empirical studies to evaluate techniques for automatic structural test case generation. However, there isn't a systematic study about the kind of experiments that are conducted to automate the process. The main objective of this paper is the classification and thematic analysis of the experiments reported in the literature to increase the efficacy and efficiency of the automation of structural testing and a classification of the techniques for generating test data. The methodology is a systematic mapping study. The results indicate that the experiments are mainly focused in three areas (test data generation, reduction of test suites and techniques for dealing with complex structures of programs), and that there are different combined approaches to make the generation more effective and efficient. © 2012 IEEE.","<b>Authors:</b><br/>Quintana G., Solari M. <br/><b>Key words:</b><br/>Automatic Software Testing, Automatic Test Case Generation, Empirical Software Engineering, Systematic Mapping Study"
369,paper_370,"Wendler R.",,"Context: Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective: The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method: A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results: The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion: The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Wendler R. <br/><b>Key words:</b><br/>Design-oriented research, Maturity models, Software management, Systematic mapping study"
370,paper_371,"Tahir A., MacDonell S.G.",,"Several important aspects of software product quality can be evaluated using dynamic metrics that effectively capture and reflect the software's true runtime behavior. While the extent of research in this field is still relatively limited, particularly when compared to research on static metrics, the field is growing, given the inherent advantages of dynamic metrics. The aim of this work is to systematically investigate the body of research on dynamic software metrics to identify issues associated with their selection, design and implementation. Mapping studies are being increasingly used in software engineering to characterize an emerging body of research and to identify gaps in the field under investigation. In this study we identified and evaluated 60 works based on a set of defined selection criteria. These studies were further classified and analyzed to identify their relativity to future dynamic metrics research. The classification was based on three different facets: research focus, research type and contribution type. We found a strong body of research related to dynamic coupling and cohesion metrics, with most works also addressing the abstract notion of software complexity. Specific opportunities for future work relate to a much broader range of quality dimensions. © 2012 IEEE.","<b>Authors:</b><br/>Tahir A., MacDonell S.G. <br/><b>Key words:</b><br/>dynamic analysis, dynamic metrics, mapping study, software metrics, software quality"
371,paper_372,"MacChi D., Solari M.",,"In Software Engineering technical literature the references about the benefits of software inspections are abundant. In contrast, some authors raise the problem of low adoption of this process. From this issue a literature review is made to produce a map on most researched topics in the area, factors causing low adoption and possible solutions. Results showed a list of 64 articles selected using a search protocol, which were classified according to a defined taxonomy. The founded factors were codified and a list of solutions founded in the reviewed papers was made. The main conclusion was that most of the factors causing low adoption are related to developers perceptions about the process, lack of training and some characteristics of the process as the rigidity, complexity and the difficulty of connecting the effort made with the final product quality. These factors should be studied in future works. © 2012 IEEE.","<b>Authors:</b><br/>MacChi D., Solari M. <br/><b>Key words:</b><br/>Factors, Low adoption, Quality, Software inspections, Systematic mapping study"
372,paper_373,"Mehmood A., Jawawi D.N.A.",,"An integration of aspect orientation and model-driven engineering is expected to enhance software development from many perspectives. Different approaches for this integration have already appeared in literature. In general, all such approaches use aspect-oriented model as the primary artifact and apply different techniques to obtain an executable from it. In this study we have provided a survey of existing research in this context by conducting a systematic mapping study. Classification schemes were defined and 38 selected primary studies were classified on the basis of research focus, contribution type and research type. Results show that solutions proposals are in a majority and current research has mainly focused on using model weavers for the integration (81% versus 19% code generation approaches). The majority of contributions are methods. © 2012 IEEE.","<b>Authors:</b><br/>Mehmood A., Jawawi D.N.A. <br/><b>Key words:</b><br/>aspect-oriented software development, code generation, model-driven engineering, systematic map"
373,paper_374,"Bräutigam B., Rizzoli P., Martone M., Bachmann M., Kraus T., Krieger G.",,"TanDEM-X is an interferometric SAR (InSAR) mission acquiring bistatic images with two satellites. Systematic mapping of the Earth's land masses will provide individual interferometric data sets which will be mosaicked and calibrated into a global Digital Elevation Model (DEM). The concept of InSAR and DEM quality monitoring throughout the acquisition and processing phase is presented in this paper. © 2012 IEEE.","<b>Authors:</b><br/>Bräutigam B., Rizzoli P., Martone M., Bachmann M., Kraus T., Krieger G. <br/><b>Key words:</b><br/>DEM, SAR Interferometry, TanDEM-X"
374,paper_375,"Matturro G., Saavedra J.",,"Initiatives to improve software processes are a particular case of organizational change and, as such, their implementations are influenced by what are often called critical success factors. These change initiatives, in turn, are not exempt from difficulties and often face barriers and obstacles to individual and organizational levels. Knowing what these factors are positively or negatively affecting the software process improvement is of importance for organizations that are implementing or planning to establish an initiative of this kind. The purpose of this paper is to report systematic mapping of literature whose objectives were to a) identify existing research on factors affecting software process improvement, and b) identify and categorize the factors reported in the studies reviewed. It describes the process for mapping and identifying existing items, the types of factors reported and the research methods used to identify them. It also presents the 23 categories constructed with the factors extracted from the studies reviewed. These categories will be used as a basis for further study of the factors affecting the process improvement initiatives from a group of software companies Uruguay, certified CMMI Level 3.","<b>Authors:</b><br/>Matturro G., Saavedra J. <br/><b>Key words:</b><br/>Barreras, Desmotivadores, Factores de éxito, Mapeo sistemático de literatura, Mejora de procesos software, Motivadores, Obstáculos"
375,paper_376,"Hellmann T.D., Sharma A., Ferreira J., Maurer F.",,"Testing has been a cornerstone of agile software development methodologies since early in the history of the field. However, the terminology used to describe the field - as well as the evidence in existing literature - is largely inconsistent. In order to better structure our understanding of the field and to guide future work, we conducted a systematic mapping of agile testing. We investigated five research questions: which authors are most active in agile testing, what is agile testing used for, what types of paper tend to be published in this field, how do practitioners and academics contribute to research in this field, and what tools are used to conduct agile testing? Of particular interest is our investigation into the source of these publications, which indicates that academics and practitioners focus on different types of publication and, disturbingly, that the number of practitioner papers in the sources we searched is strongly down since 2010. © 2012 IEEE.","<b>Authors:</b><br/>Hellmann T.D., Sharma A., Ferreira J., Maurer F. <br/><b>Key words:</b><br/>agile software development, empirical, software testing, systematic mapping, test-driven development, testing tools"
376,paper_377,"Mermillod M., Bonin P., Méot A., Ferrand L., Paindavoine M.",,"According to the age-of-acquisition hypothesis, words acquired early in life are processed faster and more accurately than words acquired later. Connectionist models have begun to explore the influence of the age/order of acquisition of items (and also their frequency of encounter). This study attempts to reconcile two different methodological and theoretical approaches (proposed by Lambon Ralph & Ehsan, 2006 and Zevin & Seidenberg, 2002) to age-limited learning effects. The current simulations extend the findings reported by Zevin and Seidenberg (2002) that have shown that frequency trajectories (FTs) have limited and specific effects on word-reading tasks. Using the methodological framework proposed by Lambon Ralph and Ehsan (2006), which makes it possible to compare word-reading and picture-naming tasks in connectionist networks, we were able to show that FT has a considerable influence on age-limited learning effects in a picture naming task. The findings show that when the input-output mappings are arbitrary (simulating picture naming tasks), the links formed by the network become entrenched as a result of early experience and that subsequent variations in frequency of exposure of the items have only a minor impact. In contrast, when the mappings between input-output are quasi-systematic or systematic (simulating word-reading tasks), the training of new items was generalized and resulted in the suppression of age-limited learning effects. At a theoretical level, we suggest that FT, which simultaneously takes account of time and the level of exposure across time, represents a more precise and modulated measure compared with the order of introduction of the items and may lead to innovative hypotheses in the field of age-limited learning effects. © 2012 Cognitive Science Society, Inc.","<b>Authors:</b><br/>Mermillod M., Bonin P., Méot A., Ferrand L., Paindavoine M. <br/><b>Key words:</b><br/>Age of acquisition, Arbitrary mappings, Frequency trajectory, Quasi-systematic/systematic mappings"
377,paper_378,"Yousefzadeh M., Mojaradi B.",,"The accessibility of global data, such as the digital elevation model (DEM), and the development of global visualisations allow promising new methods for minimising the distortion in the online generation of rough orthorectified products to be developed. Direct georeferencing (DG) has attracted a considerable amount of attention in the applications of pushbroom raw images in orthorectification or mono-plotting using ancillary satellite data. This study builds on recent DG studies to achieve an "" orthoimage"" from raw data and to determine potential mapping errors due to the DG procedure. Thus, this paper focuses on establishing a simple method for mitigating the misalignments of space-borne imageries to be used in direct orthorectification. Towards this goal, instead of image resectioning, affine transformation in different coordinate systems is employed in the orthorectification algorithm to compensate for the systematic DG errors. For a given point, the elevation corresponding to the obtained planimetric coordinate is extracted using available topographic maps and global DEMs, such as SRTM's and ASTER's DEMs. As a result, parameters no longer need to be updated, as in the conventional orthophoto generation methods. To evaluate the proposed procedures, experiments were conducted over three different IRS-p6 sensors in five datasets with different swath widths and tilt angles. The obtained results also demonstrate that the geographic coordinate system and a simple 2D affine transformation can efficiently correct misalignments. © 2012 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","<b>Authors:</b><br/>Yousefzadeh M., Mojaradi B. <br/><b>Key words:</b><br/>Ancillary data, DEM, Direct georeferencing, Google Earth, Orthoimage, Pushbroom"
378,paper_379,"Zhou H., Jiang A., Bruck J.",,"The rank modulation scheme has been proposed recently for efficiently writing and storing data in nonvolatile memories. Error-correcting codes are very important for rank modulation, and they have attracted interest among researchers. In this work, we explore a new approach, systematic error-correcting codes for rank modulation. In an (n, k) systematic code, we use the permutation induced by the levels of n cells to store data, and the permutation induced by the first k cells (k < n) has a one-to-one mapping to information bits. Systematic codes have the benefits of enabling efficient information retrieval and potentially supporting more efficient encoding and decoding procedures. We study systematic codes for rank modulation equipped with the Kendall's ?-distance. We present (k + 2, k) systematic codes for correcting one error, which have optimal sizes unless perfect codes exist. We also study the design of multi-error-correcting codes, and prove that for any 2 ? k < n, there always exists an (n, k) systematic code of minimum distance n-k. Furthermore, we prove that for rank modulation, systematic codes achieve the same capacity as general error-correcting codes. © 2012 IEEE.","<b>Authors:</b><br/>Zhou H., Jiang A., Bruck J. <br/><b>Key words:</b><br/>"
379,paper_380,"Nik Daud N.M., Kadir W.M.N.W.",,"Background: Service oriented architecture (SOA) promotes software reuse and interoperability as its niche. Developer usually expects that services being used are performing as promised. Capability to measure quality of services helps developer to predict the software behavior. Measuring quality attributes of software can be treated as a way to ensure that the developed software will/does meet expected qualities. Objectives: This study investigates the state-of-art in quality attributes measurement for service oriented architecture. It will try to answer on type of studies that have been done regarding quality attributes measurement over service oriented architecture. Method: Systematic mapping study is selected as method for this study where primary studies related to quality attributes measurement are chosen and analyzed based on research questions. Result: Based on research questions, results of this mapping study shows, performance as highly seek quality attributes in this fields, where measurement technique that mostly used is metric and finally the measurement usually done on service level. Conclusion: this study is initiated to seek for how studies on quality attributes measurement in service oriented architecture has mature during these recent years. The result shows an up and down trends regarding number of studies whilst a lot of research gap can still be covered for future researches. © 2012 AICIT.","<b>Authors:</b><br/>Nik Daud N.M., Kadir W.M.N.W. <br/><b>Key words:</b><br/>Quality attributes measurement, service oriented architecture, systematic mapping study"
380,paper_381,"Da Silva F.Q.B., Prikladnicki R., França A.C.C., Monteiro C.V.F., Costa C., Rocha R.",,"Distributed software development (DSD) has intensified over the past few years, and DSD project management is more complex than collocated project management. However, no systematic research effort has focused on aggregating evidence from the scientific literature to build models to support project management on DSD context. For these reasons, the goal of this paper is to build an evidence-based model of DSD project management from the research findings about challenges of DSD and the practices, models and tools proposed and used to overcome these challenges. We based the construction of this model on the evidence collected and synthesized by a comprehensive systematic mapping study of 70 research papers published between 1997 and 2009. We believe that our results can help practitioners and researchers to better understand the challenges and implement more effective solutions to improve project management within distributed project management teams. These results also provide a mapping of the research about DSD project management, identifying areas where further research is needed. Copyright © 2011 John Wiley & Sons, Ltd.","<b>Authors:</b><br/>Da Silva F.Q.B., Prikladnicki R., França A.C.C., Monteiro C.V.F., Costa C., Rocha R. <br/><b>Key words:</b><br/>Distributed software development, Project management, Software engineering, Systematic literature review, Systematic mapping studies"
381,paper_382,"Elberzhager F., Rosbach A., Münch J., Eschbach R.",,"Context: Quality assurance effort, especially testing effort, is often a major cost factor during software development, which sometimes consumes more than 50% of the overall development effort. Consequently, one major goal is often to reduce testing effort. Objective: The main goal of the systematic mapping study is the identification of existing approaches that are able to reduce testing effort. Therefore, an overview should be presented both for researchers and practitioners in order to identify, on the one hand, future research directions and, on the other hand, potential for improvements in practical environments. Method: Two researchers performed a systematic mapping study, focusing on four databases with an initial result set of 4020 articles. Results: In total, we selected and categorized 144 articles. Five different areas were identified that exploit different ways to reduce testing effort: approaches that predict defect-prone parts or defect content, automation, test input reduction approaches, quality assurance techniques applied before testing, and test strategy approaches. Conclusion: The results reflect an increased interest in this topic in recent years. A lot of different approaches have been developed, refined, and evaluated in different environments. The highest attention was found with respect to automation and prediction approaches. In addition, some input reduction approaches were found. However, in terms of combining early quality assurance activities with testing to reduce test effort, only a small number of approaches were found. Due to the continuous challenge of reducing test effort, future research in this area is expected. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Elberzhager F., Rosbach A., Münch J., Eschbach R. <br/><b>Key words:</b><br/>Efficiency improvement, Mapping study, Quality assurance, Software testing, Test effort reduction"
382,paper_383,"Woodall IV W.J., Bevly D.",,"This paper describes the use of the Microsoft Kinect for building three dimensional maps for use in teleoperation. Though these maps are being used for teleoperation in this paper, these map building techniques could also be applied in localization for navigation or robot path planning. The Kinect is a relatively new depth sensor that has become popular in the field of robotics, often replacing significantly more expensive systems like tilting laser range finders and stereoscopic vision systems. Two main sources of error in the map making are investigated, random and systematic error from the depth sensor and uncertainty in the navigation solution of the vehicle. Systematic and random error in the Kinect has previously been described in the literature [1] and this paper looks at how these errors affect the users ability to do mapping and offers some practical solutions. The teleoperation system design around this map making process is presented, and it builds on existing work [2] using octrees as the storage for the maps. © 2012 IEEE.","<b>Authors:</b><br/>Woodall IV W.J., Bevly D. <br/><b>Key words:</b><br/>Kinect, Mapping, Octree, Robotics, Teleoperation"
383,paper_384,"Jayan V., Sunil R., Kurambath G.S., Kumar R.R.",,"The translation divergence is the problem pertained with the Transfer and Interlingua Machine Translation (MT) because it requires a large combination of complex lexical and structural mappings. Divergence makes straight forward transfer from source language to target language impractical. The main aim of the divergence study is to know the source and target language pattern and morphophonemic changes upon translation of a sentence. This will enable the MT system to handle most of the anomalies coming out of divergence problem. For this we need a systematic mapping between the Interlingua representation and the surface syntactic structure that accommodates all of the divergences. In this paper we try to list out different types of divergence occurring among English and Malayalam. © 2012 ACM.","<b>Authors:</b><br/>Jayan V., Sunil R., Kurambath G.S., Kumar R.R. <br/><b>Key words:</b><br/>causative, divergence, modality, polysemous"
384,paper_385,"Brandalize M.C.B., Antunes A.F.B.",,"The Geographic Service Directory (DSG), in conjunction with the Brazilian Institute of Geography and Statistics (IBGE), are responsible for the systematic mapping of the Brazilian territory (scales 1:1.000.000 to 1:25.000) since their creation, respectively, in 1890 and 1934. Considering their long trajectory in establishing the National Cartographic System, encompassing several topographic mapping projects through the last century, most of all based in photogrammetric coverages, this same territory is considered to be out of date in terms of cartographic products since the 80's decade. This out of date can be translated in numbers that can factually indicate the chaotic situation that planners and managers in general are experiencing nowadays in Brazil. In this manner, although the Brazilian territory is completely covered, since 1922, at 1:1.000.000 scale (46 charts), the most up-to-date products for this scale are from 1998, when they were updated based on remote sensing images. In the other hand, greater scales like 1:100.000, 1:50.000 and 1:25.000, which cover respectively 75%, 14% and 1% of the Brazilian territory, are from periods comprehended between 1908 and 1985, most of them with more than 30 years old and never updated. The lack of geographic information at such important scales submits regional and local development as much as environmental management and monitoring to plans and actions based on unreliable and untrue spatial information. Initiatives from the federal government in order to solve some of the environmental management and monitoring problems, especially in the Amazon Region, have culminated on projects based exclusively on remote sensing techniques, like RAD AM (Radar in Amazon) and SIVAM (Amazon Vigilance System). Another initiative relies, since 2006, on the cooperation agreement between IBGE and the Alaska Satellite Facility (ASF), aiming the distribution of remote sensing images to Federal Government agencies, research institutions and other non commercial users in Brazil. These images intend to help the acceleration of the territory mapping process and its updating, allowing more efficient governmental actions planning. In a country where cartographic development is running far away behind the social and economic ones, initiatives like these must be critically analyzed and discussed. That is the topic of this article.","<b>Authors:</b><br/>Brandalize M.C.B., Antunes A.F.B. <br/><b>Key words:</b><br/>"
385,paper_386,"Zhang C., Budgen D.",,"Context. Although research in software engineering largely seeks to improve the practices and products of software development, many practices are based upon codification of expert knowledge, often with little or no underpinning from objective empirical evidence. Software design patterns seek to codify expert knowledge to share experience about successful design structures. Objectives. To investigate how extensively the use of software design patterns has been subjected to empirical study and what evidence is available about how and when their use can provide an effective mechanism for knowledge transfer about design. Method. We conducted a systematic literature review in the form of a mapping study, searching the literature up to the end of 2009 to identify relevant primary studies about the use of the 23 patterns catalogued in the widely referenced book by the &#x201C,Gang of Four.&#x201D, These studies were then categorized according to the forms of study employed, the patterns that were studied, as well as the context within which the study took place. Results. Our searches identified 611 candidate papers. Applying our inclusion/exclusion criteria resulted in a final set of 10 papers that described 11 instances of &#x201C,formal&#x201D, experimental studies of object-oriented design patterns. We augmented our analysis by including seven &#x201C,experience&#x201D, reports that described application of patterns using less rigorous observational forms. We report and review the profiles of the empirical evidence for those patterns for which multiple studies exist. Conclusions. We could not identify firm support for any of the claims made for patterns in general, although there was some support for the usefulness of patterns in providing a framework for maintenance, and some qualitative indication that they do not help novices learn about design. For future studies we recommend that researchers use case studies that focus upon some key patterns, and seek to identify the impact that their use can have upon maintenance. © 2012 IEEE.","<b>Authors:</b><br/>Zhang C., Budgen D. <br/><b>Key words:</b><br/>Design patterns, empirical software engineering, systematic literature review"
386,paper_387,"Mohebzada J.G., Ruhe G., Eberlein A.",,"Recommendation systems provide users with up-to-date guidance on processes, artifacts or other project-relevant information. Recommendation systems for requirements engineering can be used to provide the right information, at the right time, to requirements engineers. In this paper, we use systematic mapping to provide an overview of recommendation systems for the requirements engineering process, their characteristics, and state of validation. The resulting maps are analyzed to provide conclusions and to identify the limitations of current studies, and future research areas. The results of the mapping are used to outline the motivation for our future work on a recommendation system that helps product managers decide on the assignment of requirements to subsequent releases while considering constraints such as time, effort, quality, and resources. © 2012 IEEE.","<b>Authors:</b><br/>Mohebzada J.G., Ruhe G., Eberlein A. <br/><b>Key words:</b><br/>recommendation systems, requirements engineering, systematic mapping study"
387,paper_388,"Psychou G., Fasthuber R., Catthoor F., Hulzink J., Huisken J.",,"Data-parallel processing is a widely applicable technique, which can be implemented on different processor styles, with varying capabilities. Here we address single or multi-core data-parallel instruction-set processors. Often, handling and reorganisation of the parallel data may be needed because of diverse needs during the execution of the application code. Signal word-length considerations are crucial to incorporate because they influence the outcome very strongly. This paper focuses on the broader solution space of selecting sub-word lengths (at design time) including especially hybrids, so that mapping on these data parallel single/multi-core processors is more energy-efficient. Our goal is to introduce systematic exploration techniques so that part of the designers effort is removed. The methodology is evaluated on a representative application driver for a number of data-path variants and the most promising trade-off points are indicated. The range of throughput-energy ratios among the different mapping implementations is spanning a factor of 2.2. © 2012 Gesellschaft fuer Informatk.","<b>Authors:</b><br/>Psychou G., Fasthuber R., Catthoor F., Hulzink J., Huisken J. <br/><b>Key words:</b><br/>"
388,paper_389,"Budgen D., Drummond S., Brereton P., Holland N.",,"Context: In teaching about software engineering we currently make little use of any empirical knowledge. Aim: To examine the outcomes available from the use of Evidence-Based Software Engineering (EBSE) practices, so as to identify where these can provide support for, and inform, teaching activities. Method: We have examined all known secondary studies published up to the end of 2009, together with those published in major journals to mid-2011, and identified where these provide practical results that are relevant to student needs. Results: Starting with 145 candidate systematic literature reviews (SLRs), we were able to identify and classify potentially useful teaching material from 43 of them. Conclusions: EBSE can potentially lend authority to our teaching, although the coverage of key topics is uneven. Additionally, mapping studies can provide support for research-led teaching. © 2012 IEEE.","<b>Authors:</b><br/>Budgen D., Drummond S., Brereton P., Holland N. <br/><b>Key words:</b><br/>education, empirical, evidence-based"
389,paper_390,"Lemos J., Alves C., Duboc L., Rodrigues G.N.",,"In the last years, the field of creativity for requirements engineering has received a growing interest from researchers and practitioners. This paper presents a mapping study to aggregate literature in this field. The motivation is to identify trends and research opportunities in the application of creativity approaches to support the requirements engineering. The mapping study analyzed 46 papers. The results show four main research groups working on creativity in RE. Requirements elicitation is the phase concentrating the majority of studies. The study confirms that creativity techniques enhance creative thinking in requirements activities. However, creative thinking strategies should be fully integrated in current requirements engineering processes, methods and tools. © 2012 ACM.","<b>Authors:</b><br/>Lemos J., Alves C., Duboc L., Rodrigues G.N. <br/><b>Key words:</b><br/>creativity, mapping study, requirements engineering"
390,paper_391,"Portillo-Rodríguez J., Vizcaíno A., Piattini M., Beecham S.",,"Context: This systematic mapping review is set in a Global Software Engineering (GSE) context, characterized by a highly distributed environment in which project team members work separately in different countries. This geographic separation creates specific challenges associated with global communication, coordination and control. Objective: The main goal of this study is to discover all the available communication and coordination tools that can support highly distributed teams, how these tools have been applied in GSE, and then to describe and classify the tools to allow both practitioners and researchers involved in GSE to make use of the available tool support in GSE. Method: We performed a systematic mapping review through a search for studies that answered our research question, ""Which software tools (commercial, free or research based) are available to support Global Software Engineering?"" Applying a range of related search terms to key electronic databases, selected journals, and conferences and workshops enabled us to extract relevant papers. We then used a data extraction template to classify, extract and record important information about the GSD tools from each paper. This information was synthesized and presented as a general map of types of GSD tools, the tool's main features and how each tool was validated in practice. Results: The main result is a list of 132 tools, which, according to the literature, have been, or are intended to be, used in global software projects. The classification of these tools includes lists of features for communication, coordination and control as well as how the tool has been validated in practice. We found that out the total of 132, the majority of tools were developed at research centers, and only a small percentage of tools (18.9%) are reported as having been tested outside the initial context in which they were developed. Conclusion: The most common features in the GSE tools included in this study are: team activity and social awareness, support for informal communication, Support for Distributed Knowledge Management and Interoperability with other tools. Finally, there is the need for an evaluation of these tools to verify their external validity, or usefulness in a wider global environment. © 2012 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Portillo-Rodríguez J., Vizcaíno A., Piattini M., Beecham S. <br/><b>Key words:</b><br/>Distributed Software Engineering, Global Software Development, Systematic Mapping Study, Tool"
391,paper_392,"Da Mota Silveira Neto P.A., Do Carmo MacHado I., McGregor J.D., De Almeida E.S., De Lemos Meira S.R.",,"[No abstract available]","<b>Authors:</b><br/>Da Mota Silveira Neto P.A., Do Carmo MacHado I., McGregor J.D., De Almeida E.S., De Lemos Meira S.R. <br/><b>Key words:</b><br/>"
392,paper_393,"Castro Llanos J.W., Acuña Castillo S.T.",,"The growing importance of open source software (OSS) has led researchers to study how OSS processes differ from traditional software engineering processes. The aim of this study is to determine the differences and similarities between development process activities (requirements, design, and implementation) enacted by the OSS community and established by IEEE Standard 1074:2006. We conducted a systematic mapping study to find out which activities are part of the OSS development process. We identified a total of 22 primary studies. Of these, 46% described activities related to the requirements process, just over 60% reported activities related to design and almost all accounted for activities related to implementation. The OSS community does not enact prescriptive software engineering models. OSS requirements are evolved using several different web artefacts, as well as through continual interactions in forums and via messaging. Requirements are asserted rather than elicited. A common feature of all OSS projects is that software system design and implementation is modular. The priority in the OSS community is implementation. Anyone, developers or users, can make contributions, including designs and code. © 2012 Springer-Verlag.","<b>Authors:</b><br/>Castro Llanos J.W., Acuña Castillo S.T. <br/><b>Key words:</b><br/>Open Source, Requirements Engineering, Software Development Process, Systematic Mapping Study"
393,paper_394,"Seng J.-L., Wong Z.",,"Motivated by the globalization trend and Internet speed competition, enterprise nowadays often divides into many departments or organizations or even merges with other companies that located in different regions to bring up the competency and reaction ability. As a result, there are a number of data warehouse systems in a geographically-distributed enterprise. To meet the distributed decision-making requirements, the data in different data warehouses is addressed to enable data exchange and integration. Therefore, an open, vendor-independent, and efficient data exchange standard to transfer data between data warehouses over the Internet is an important issue. However, current solutions for cross-warehouse data exchange employ only approaches either based on records or transferring plain-text files, which are neither adequate nor efficient. In this research, issues on multidimensional data exchange are studied and an Intelligent XML-based multidimensional data exchange model is developed. In addition, a generic-construct-based approach is proposed to enable many-to-many systematic mapping between distributed data warehouses, introducing a consistent and unique standard exchange format. Based on the transformation model we develop between multidimensional data model and XML data model, and enhanced by the multidimensional metadata management mechanism proposed in this research, a general-purpose intelligent XML-based multidimensional data exchange process over web is facilitated efficiently and improved in quality. Moreover, we develop an intelligent XML-based prototype system to exchange multidimensional data, which shows that the proposed multidimensional data exchange model is feasible, and the multidimensional data exchange process is more systematic and efficient using metadata. © 2011 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Seng J.-L., Wong Z. <br/><b>Key words:</b><br/>Data exchange, Data warehouse, Generic construct, Intelligent metadata management, Mapping and transformation, Multidimensional data cube, XML"
394,paper_395,"Lei S.-F., Lo C.-C., Kuo C.-C., Shieh M.-D.",,"H.264/AVC achieves a higher compression ratio than previous standards. However, this standard is also more complex because of the use of methods such as context-based adaptive binary arithmetic coding (CABAC). The high computational complexity of CABAC results in large power consumption. This study presents a systematic analysis for designing a low-power architecture which includes an embedded cache. The analysis provides the mapping scheme between the cache and the main memory where the contexts are stored. The observations for the proposed scheme are based on the statistical correlation between neighbouring blocks for H.264 coding. The proposed scheme allows the context access operations to hit frequently in the cache, significantly reducing the power consumption. The proposed architecture lowers power consumption by up to 50% compared to designs without embedded cache. An efficient bit-packing method of output bitstream that can be implemented by pipeline structure for high encoding data throughput is also proposed. The throughput of the proposed design is up to 200 Mbins per second for H.264 main profile. © 2012 The Institution of Engineering and Technology.","<b>Authors:</b><br/>Lei S.-F., Lo C.-C., Kuo C.-C., Shieh M.-D. <br/><b>Key words:</b><br/>"
395,paper_396,"Yang J.-B., Wang Y.-M., Xu D.-L., Chin K.-S., Chatton L.",,"Rapid and accurate identification of consumer demands and systematic assessment of product quality are essential to success for new product development, in particular for fast moving consumer goods such as food and drink products. This paper reports an investigation into a belief rule-based (BRB) methodology for quality assessment, target setting and consumer preference prediction in retro-fit design of food and drink products. The BRB methodology can be used to represent the relationships between consumer preferences and product attributes, which are complicated and nonlinear. A BRB system can initially be established using expert knowledge and then optimally trained and validated using data generated from consumer or expert panel assessments or from tests and experiments. The established BRBs can then be used to predict the consumer acceptance of new products or set product target values in retro-fit design. The proposed BRB methodology is applied to the design of a lemonade drink product using real data provided by a sensory product manufacturer in the UK. The results show that the BRB methodology can be used to predict consumer preferences with high accuracy and to set optimal target values for product quality improvement. © 2011 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Yang J.-B., Wang Y.-M., Xu D.-L., Chin K.-S., Chatton L. <br/><b>Key words:</b><br/>Belief rule-based system, Consumer preference modeling, Optimization, Preference mapping, Product design, Quality assessment, Target setting"
396,paper_397,"Parandeh-Afshar H., Benbihi H., Novo D., Ienne P.",,"Look-Up Tables (LUTs) are universally used in FPGAs as the elementary logic blocks. They can implement any logic function and thus covering a circuit is a relatively straightforward problem. Naturally, flexibility comes at a price, and increasing the number of LUT inputs to cover larger parts of a circuit has an exponential cost in the LUT complexity. Hence, rarely LUTs with more than 4-6 inputs have been used. In this paper we argue that other elementary logic blocks can provide a better compromise between hardware complexity, flexibility, delay, and input and output counts. Inspired by recent trends in synthesis and verification, we explore blocks based on And-Inverter Graphs (AIGs): they have a complexity which is only linear in the number of inputs, they sport the potential for multiple independent outputs, and the delay is only logarithmic in the number of inputs. Of course, these new blocks are extremely less flexible than LUTs, yet, we show (i) that effective mapping algorithms exist, (ii) that, due to their simplicity, poor utilization is less of an issue than with LUTs, and (iii) that a few LUTs can still be used in extreme unfortunate cases. We show first results indicating that this new logic block combined to some LUTs in hybrid FPGAs can reduce delay up to 22-32% and area by some 16% on average. Yet, we explored only a few design points and we think that these results could still be improved by a more systematic exploration. © 2012 ACM.","<b>Authors:</b><br/>Parandeh-Afshar H., Benbihi H., Novo D., Ienne P. <br/><b>Key words:</b><br/>and-inverter cone, and-inverter graph, FPGA logic block, logic synthesis"
397,paper_398,"Da Silva F.Q.B., Suassuna M., Lopes R.F., Gouveia T.B., França A.C.A., De Oliveira J.P.N., De Oliveira L.F.M., Santos A.L.M.",,"Our goal in this study is to review the research related to the replication of empirical studies in software engineering in terms of replications of empirical studies and conceptual or theoretical work about replications. In this article we present the preliminary findings of this review, concentrating on the studies reporting replications and the related original studies. We applied the systematic review method to perform a mapping study about the current state of the replication work of empirical studies performed in software engineering research. We analyzed 16,126 articles, from which we extracted 93 articles reporting 125 replications performed between 1994 and 2010, of 76 original studies. Over 60% of the replications were performed in the last six years and 71% percent of the studies were internal replications. The topics of software construction, testing, and maintenance concentrate nearly 50% of the replication work, while software design, configuration management and software tools and methods are the topics with least replications. The number of replications grew in the last few years, but the absolute number of replications is still very small, in particular considering the breadth of topics in software engineering. Incentive to perform external replications and better standards to report empirical studies and their replications are still needed. © 2012 IEEE.","<b>Authors:</b><br/>Da Silva F.Q.B., Suassuna M., Lopes R.F., Gouveia T.B., França A.C.A., De Oliveira J.P.N., De Oliveira L.F.M., Santos A.L.M. <br/><b>Key words:</b><br/>Empirical studies, Experiments, Literature review, Mapping study, Replications, Software engineering"
398,paper_399,"Felizardo K.R., Macdonell S.G., Mendes E., Maldonado J.C.",,"A systematic literature review (SLR) is a methodology used to find and aggregate all relevant existing evidence about a specific research question of interest. Important decisions need to be made at several points in the review process, relating to search of the literature, selection of relevant primary studies and use of methods of synthesis. Visualization can support tasks that involve large collections of data, such as the studies collected, evaluated and summarized in an SLR. The objective of this paper is to present the results of a systematic mapping study (SM) conducted to collect and evaluate evidence on the use of a specific visualization technique, visual data mining (VDM), to support the SLR process. We reviewed 20 papers and our results indicate a scarcity of research on the use of VDM to help with conducting SLRs in the software engineering domain. However, most of the studies (16 of the 20 studies included in our mapping) have been conducted in the field of medicine and they revealed that the activities of data extraction and data synthesis, related to conducting the review phase of an SLR process, have more VDM support than other activities. In contrast, according to our SM, previous studies using VDM techniques with SLRs have not employed such techniques during the SLR's planning and reporting phases.© 2012 ACADEMY PUBLISHER.","<b>Authors:</b><br/>Felizardo K.R., Macdonell S.G., Mendes E., Maldonado J.C. <br/><b>Key words:</b><br/>Systematic literature review, Systematic mapping, Visual data mining"
399,paper_400,"Wyrzykowski R., Kuczynski L., Wozniak M.",,"Erasure codes can improve the availability of distributed storage in comparison with replication systems. In this paper, we focus on investigating how to map systematically the Reed-Solomon and Cauchy Reed-Solomon erasure codes onto the Cell/B.E. and GPU multicore architecture. A method for the systematic mapping of computation kernels of encoding/decoding algorithms onto the Cell/B.E. architecture is proposed. This method takes into account properties of the architecture on all three levels of its parallel processing hierarchy. The performance results are shown to be very promising. The possibility of using GPUs is studied as well, based on the Cauchy version of Reed-Solomon codes. © 2012 Springer-Verlag.","<b>Authors:</b><br/>Wyrzykowski R., Kuczynski L., Wozniak M. <br/><b>Key words:</b><br/>Cauchy Reed-Solomon codes, Cell/B.E., Erasure codes, GPU, multicore architectures, Reed-Solomon codes"
400,paper_401,"Elberzhager F., Münch J., Nha V.T.N.",,"Context: A lot of different quality assurance techniques exist to ensure high quality products. However, most often they are applied in isolation. A systematic combination of different static and dynamic quality assurance techniques promises to exploit synergy effects, such as higher defect detection rates or reduced quality assurance costs. However, a systematic overview of such combinations and reported evidence about achieving synergy effects with such kinds of combinations is missing. Objective: The main goal of this article is the classification and thematic analysis of existing approaches that combine different static and dynamic quality assurance technique, including reported effects, characteristics, and constraints. The result is an overview of existing approaches and a suitable basis for identifying future research directions. Method: A systematic mapping study was performed by two researchers, focusing on four databases with an initial result set of 2498 articles, covering articles published between 1985 and 2010. Results: In total, 51 articles were selected and classified according to multiple criteria. The two main dimensions of a combination are integration (i.e., the output of one quality assurance technique is used for the second one) and compilation (i.e., different quality assurance techniques are applied to ensure a common goal, but in isolation). The combination of static and dynamic analyses is one of the most common approaches and usually conducted in an integrated manner. With respect to the combination of inspection and testing techniques, this is done more often in a compiled way than in an integrated way. Conclusion: The results show an increased interest in this topic in recent years, especially with respect to the integration of static and dynamic analyses. Inspection and testing techniques are currently mostly performed in an isolated manner. The integration of inspection and testing techniques is a promising research direction for the exploitation of additional synergy effects. © 2011 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Elberzhager F., Münch J., Nha V.T.N. <br/><b>Key words:</b><br/>Combination, Dynamic quality assurance, Inspection, Static quality assurance, Systematic mapping study, Testing"
401,paper_402,"Ueda E.T., Ruggiero W.V.",,"Access control is a key component of security in any computer system. In the last two decades, the research on Role Basead Access Control Models was intense. One of the most important components of a Role Based Model is the Role-Permission Relationship. In this paper, the technique of systematic mapping is used to identify, extract and analyze many approaches applied to establish the Role-Permission Relationship. The main goal of this mapping is pointing directions of significant research in the area of Role Based Access Control Models. © 2005 IEEE.","<b>Authors:</b><br/>Ueda E.T., Ruggiero W.V. <br/><b>Key words:</b><br/>RBAC, Role Based Access Control, Role-Permission Relationship"
402,paper_403,"Vavassori Benitti F.B., Sommariva L.",,"Study the human-computer interaction is essential for students of computer science related courses since it professionally will produce systems dedicated to different profiles of end users. Therefore, this article explores what should be taught in the context of IHC on computing courses, as well as identifies teaching strategies for the area. Therefore, initially is presented a documentary research involving curriculum guidelines and teaching plans, which aim to teach in the area. Afterwards, we present a systematic mapping involving resources and techniques for teaching. The results show a list of concepts, and some teaching strategies. © by the paper's authors.","<b>Authors:</b><br/>Vavassori Benitti F.B., Sommariva L. <br/><b>Key words:</b><br/>"
403,paper_404,"Akhtman Y., Martelletti L., Grandjean O., Lemmin U.",,"We have developed and deployed a Web-based GIS data management framework, which facilitates an effective and highly structured search, retrieval and visualisation of multi-modal scientific data, as well as its subsequent dissemination in multiple and standardised forms beneficial for both the research partners involved in the project and the general public. In the context of the long term objectives of the Élémo project, the developed methodology may be utilised for automated and systematic collection of the multifaceted scientific data and with the goal of assembling a comprehensive database encompassing all aspects of currently planned and future scientific investigations. © 2012 ISPRS.","<b>Authors:</b><br/>Akhtman Y., Martelletti L., Grandjean O., Lemmin U. <br/><b>Key words:</b><br/>Database, GIS, Mapping, Navigation, Surveying, Underwater, Visualisation, Web-GIS"
404,paper_405,"Zheng G., Liu X., Lu X.",,"In order to narrow the scope of line maintenance inspection, locate the fault line of grid quickly, we put forward the concept of optimal distribution of line into segments, with the figure and the tree structure methods to represent and process the distribution information of line. By analyzing the ungrounded single-phase power grid failure mechanism, a systematic study of two kind's methods to the fault section location, the fault section calculation location method and inference rules determine method, and indicate the faulty section of the edge clearly. In this paper, we adopt object-oriented design patterns, study and design an integrated approach to power system graph model based on. NET technology. The system use C#.NET as the development tool and ADO. NET technology to access the database, designed and built the meta-data structure based on object-oriented. Based on the graphical modeling of power system, the system generates network topology automatically after the input of power parameters in the graphical interface, through the topology reconstruction and map mode mapping. Finally, we achieve topological analysis, steady-state analysis, failure analysis and output analysis and other functions. © 2011 IEEE.","<b>Authors:</b><br/>Zheng G., Liu X., Lu X. <br/><b>Key words:</b><br/>Graphical modeling, intelligent distribution network, small current grounding fault section location"
405,paper_406,"Moghaddam S., Helmy A.",,"User online behavior and interests will play a central role in future mobile networks. We introduce a systematic method for large-scale multi-dimensional modeling and analysis of online activity and mobility for thousands of mobile users across 79 buildings over a variety of web domains. We propose a modeling approach based on kind of neural-networks, called self-organizing maps (SOM), for discovering, organizing and visualizing different mobile users'trends from billions of WLAN records. We find surprisingly that users'trends based on domains and locations can be accurately modeled using a self-organizing map with clearly distinct characteristics. We also find many non-trivial correlations between different types of web domains and locations. Copyright 2011 ACM.","<b>Authors:</b><br/>Moghaddam S., Helmy A. <br/><b>Key words:</b><br/>Data-driven, Self-organizing map, Trend, Wireless"
406,paper_407,"Qadir M.M., Usman M.",,"Software Engineering (SE) discipline has come a long way since the 1968 NATO conference when the term SE was first used. Lot of work has been done for developing and revising SE curriculum and body of knowledge (e.g. SE 2004, GSwE2009, SWEBOK efforts). Different universities are developing and revising SE program and curricula at graduate and undergraduate levels all over the world. Large number of SE curriculum related efforts are being published in conferences such as CSEET, FIE and REET etc. There is a need to see the state of the art of this abundant literature. In this paper, we report a systematic mapping study conducted to synthesize and aggregate the SE curriculum related reported efforts and to provide a broad overview of the area. Systematic mapping studies are performed to evaluate quantity and types of primary studies in an area of interest in an unbiased and systematic manner. © 2011 IEEE.","<b>Authors:</b><br/>Qadir M.M., Usman M. <br/><b>Key words:</b><br/>Software Engineering Curriculum, Systematic Mapping Study"
407,paper_408,"da Silva R.C., Vavassori Benitti F.B.",,"The requirements elicitation is reported in the literature as essential to the success of software development projects. The existence of requirements patterns aimed at not only improving the quality of requirements specifications, but also the reuse of successful solutions of previous experience in new projects. In this paper, we present the results of a systematic literature mapping on requirements patterns. The results show that there is a diverse range of patterns for the process of requirements engineering, but there are still few studies addressing the requirements patterns that deal specifically with the documentation of software requirements.","<b>Authors:</b><br/>da Silva R.C., Vavassori Benitti F.B. <br/><b>Key words:</b><br/>Requirement pattern, Systematic mapping study"
408,paper_409,"Da Silva R.C., Benitti F.B.V.",,"The requirements elicitation is reported in the literature as essential to the success of software development projects. The existence of requirements patterns aimed at not only improving the quality of requirements specifications, but also the reuse of successful solutions of previous experience in new projects. In this paper, we present the results of a systematic literature mapping on requirements patterns. The results show that there is a diverse range of patterns for the process of requirements engineering, but there are still few studies addressing the requirements patterns that deal specifically with the documentation of software requirements.","<b>Authors:</b><br/>Da Silva R.C., Benitti F.B.V. <br/><b>Key words:</b><br/>Requirement pattern, Systematic mapping study"
409,paper_410,"Cavalcanti T.R., Silva F.Q.B.D.",,"The goal of this article is to provide a comprehensive and systematic analysis of the scientific work published in the Brazilian Symposium on Software Engineering (SBES). We used a systematic literature review methodology to extract, catalog, analyze, and synthesize data from all articles published in each of the 24 editions of SBES, with respect to historical, conceptual, and methodological aspects. The results of our review showed that 509 articles have been published, which were authored and co-authored by 818 researchers from 151 organizations, demonstrating the relevance of the SBES to bring together a significant portion of the Brazilian Software Engineering research community. Consistent with other studies, our results show that research published at SBES is diversified on the topics of software engineering addressed, but narrow on research approach and methods used. Besides, there is a strong concentration on technical aspects and much less studies addressing human or social aspects. We discuss the implications of these results for research and practice of software engineering in Brazil. © 2011 IEEE.","<b>Authors:</b><br/>Cavalcanti T.R., Silva F.Q.B.D. <br/><b>Key words:</b><br/>engenharia de software, estudo secundário, revisão de literatura."
410,paper_411,"Kusumo D.S., Zhu L., Staples M., Zhang H.",,"Acquiring software from external suppliers and developing less software in-house can help software-developing organizations improve operational efficiency by reducing costs, time and reusing current technologies. Software projects increasingly use Off-The-Shelf (OTS) products. From the acquirer perspective, there is a need to understand in more detail OTS-based software acquisition processes, because they are different to and less wellunderstood than those for the acquisition of custom software. In this paper we have undertaken a systematic mapping study on OTS-based software acquisition. The study compares and contrasts OTS-based software acquisition and non-OTS-based software acquisition, and identifies factors influencing decision making in OTSbased software acquisition. We find that the main difference is that there is a relationship between determining the software requirements and OTS selection in OTS-based software acquisition. For commercial OTS software, the major factors are functionality and quality of the software, but for open-source OTS software, cost was the most important factor. © 2011 Dana S. Kusumo, Liming Zhu, Mark Staples and He Zhang.","<b>Authors:</b><br/>Kusumo D.S., Zhu L., Staples M., Zhang H. <br/><b>Key words:</b><br/>Decision making, OTS-based software acquisition, Process, Software acquisition"
411,paper_412,"Barreiros E., Almeida A., Saraiva J., Soares S.",,"Even though empirical research has grown in interest, techniques, methodologies and best practices are still in debate. In this context, testbeds are effective when one needs to evaluate and compare technologies. The concept is well disseminated in other areas such as Computer Networks, but remains poorly explored in Software Engineering (SE). This paper presents a systematic mapping study on the SE testbeds literature. From the initial set of 4239 studies, 13 primary studies were selected and categorized. Based on that, we found that Software Architecture is the most investigated topic, controlled experiment is the most used method to evaluate such testbeds, 20 benefits of using testbeds in SE have been identified and that testbeds comprise very heterogeneous structural elements. © 2011 IEEE.","<b>Authors:</b><br/>Barreiros E., Almeida A., Saraiva J., Soares S. <br/><b>Key words:</b><br/>Empirical software engineering, Technology evaluation, Technology transfer, Testbeds"
412,paper_413,"Alexe B., Cate B.T., Kolaitis P.G., Tan W.-C.",,"Schema mappings are high-level specifications that describe the relationship between two database schemas, they are considered to be the essential building blocks in data exchange and data integration, and have been the object of extensive research investigations. Since in real-life applications schema mappings can be quite complex, it is important to develop methods and tools for understanding, explaining, and refining schema mappings. A promising approach to this effect is to use ""good data examples that illustrate the schema mapping at hand. We develop a foundation for the systematic investigation of data examples and obtain a number of results on both the capabilities and the limitations of data examples in explaining and understanding schema mappings. We focus on schema mappings specified by source-to-target tuple generating dependencies (s-t tgds) and investigate the following problem: which classes of s-t tgds can be ""uniquely characterized by a finite set of data examples? Our investigation begins by considering finite sets of positive and negative examples, which are arguably the most natural choice of data examples. However, we show that they are not powerful enough to yield interesting unique characterizations. We then consider finite sets of universal examples, where a universal example is a pair consisting of a source instance and a universal solution for that source instance. We first show that unique characterizations via universal examples is, in a precise sense, equivalent to the existence of Armstrong bases (a relaxation of the classical notion of Armstrong databases). After this, we show that every schema mapping specified by LAV s-t tgds is uniquely characterized by a finite set of universal examples with respect to the class of LAV s-t tgds. Moreover, this positive result extends to the much broader classes of n-modular schema mappings, n a positive integer. Finally, we study the unique characterizability of GAV schema mappings. It turns out that some GAV schema mappings are uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds, while others are not. By unveiling a tight connection with homomorphism dualities, we establish an effective, sound, and complete criterion for determining whether or not a GAV schema mapping is uniquely characterizable by a finite set of universal examples with respect to the class of GAV s-t tgds. © 2011 ACM.","<b>Authors:</b><br/>Alexe B., Cate B.T., Kolaitis P.G., Tan W.-C. <br/><b>Key words:</b><br/>Data examples, Data exchange, Data integration, Schema mappings"
413,paper_414,"Morelli L.B., Nakagawa E.Y.",,"As video games evolve into richer and more sophisticated products, the software driving those games become more complex. One of the research areas offered by Software Engineering to cope with this complexity, while reducing risks and improving software quality, is Software Architecture. The purpose of this paper is to present an overview of possibly all work having investigated, established and used software architectures for the development of video games. For this, a Systematic Mapping was conducted. The achieved results show an increasing, however still mild, interest in the exploration of software architectures for the development of video games, and lays out lines of research that can be explored.","<b>Authors:</b><br/>Morelli L.B., Nakagawa E.Y. <br/><b>Key words:</b><br/>Computer game, Game development, Software architecture, Systematic mapping, Video game"
414,paper_415,"Durelli V.H.S., Araujo R.F., Silva M.A.G., Oliveira R.A.P., Maldonado J.C., Delamaro M.E.",,"Over the past 25 years the Brazilian Symposium on Software Engineering (SBES) has evolved to become the most important event on software engineering in Brazil. Throughout these years, SBES has gathered a large body of studies in software testing. Aimed at providing an insightful understanding of what has already been published in such event, we synthesized its rich 25-year history of research on software testing. Using information drawn from this overview we attempted to highlight which types of study have been the most applied for conveying software testing efforts. We also devised a co-authorship network to obtain a bird's-eye view of which research groups and scholars have been the most prolific ones. Moreover, by performing a citation analysis of the selected studies we set out to ascertain the importance of SBES in a wider scenario. Finally, borne out by the information extracted from the studies, we shed some light on the state-of-the-art of software testing in Brazil and provide an outlook on its foreseeable future. © 2011 IEEE.","<b>Authors:</b><br/>Durelli V.H.S., Araujo R.F., Silva M.A.G., Oliveira R.A.P., Maldonado J.C., Delamaro M.E. <br/><b>Key words:</b><br/>Software Testing, systematic mapping"
415,paper_416,"Do Carmo Machado I., Da Mota Silveira Neto P.A., De Almeida E.S., De Lemos Meira S.R.",,"Software Product Lines Testing has received special attention in recent years, due to its crucial role in quality and also due to the high cost this activity poses. In this effect, to make testing a feasible activity, some improvements are required. This paper presents a process for testing product lines, designed based on the gaps identified by a systematic mapping study, performed in order to understand the current scenario in this research field. An experimental study was performed in order to evaluate the proposed process in terms of understanding the role of a structured testing process in a SPL project.","<b>Authors:</b><br/>Do Carmo Machado I., Da Mota Silveira Neto P.A., De Almeida E.S., De Lemos Meira S.R. <br/><b>Key words:</b><br/>Software process, Software product lines, Software reuse, Software testing"
416,paper_417,"Marques G., Domingues M.A., Langlois T., Gouyon F.",,"The purpose of this paper is to address several aspects of music autotagging. We start by presenting autotagging experiments conducted with two different systems and show performances on a par with a method representative of the state-of-the-art. Beyond that, we illustrate via systematic experiments the importance of a number of issues relevant to autotagging, yet seldom reported in the literature. First, we show that the evaluation of autotagging techniques is fragile in the sense that small alterations to the set of tags to be learned, or in the set of music pieces may lead to dramatically different results. Hence we stress a set of methodological recommendations regarding data and evaluation metrics. Second, we conduct experiments on the generality of autotagging models, showing that a number of different methods at a similar performance level to the state-of-the-art fail to learn tag models able to generalize to datasets from different origins. Third we show that current performance level of a direct mapping between audio features and tags still appears insufficient to enable the possibility of exploiting natural tag correlations as a second stage to improve performance. © 2011 International Society for Music Information Retrieval.","<b>Authors:</b><br/>Marques G., Domingues M.A., Langlois T., Gouyon F. <br/><b>Key words:</b><br/>"
417,paper_418,"Liu Z., He C., Yang Y.",,"The stable lights data in the Version 4 Nighttime Lights Time Series Dataset gained by the Defense Meteorological Satellite Program's Operational Line-scan System can't be used to map urban areas directly because the data has no on-board calibration and many unstable lights are also included in the data. So the systematic correction was performed before the urban areas extraction according to the characteristic of stable lights data in China from 1992 to 2008 with the 1 km spatial resolution. It was found that, with systematic correction, the stable lights data could be strictly comparable from one year to the next, in which the remarkable differences of digital number values and the unstable lights were removed effectively. Besides, the urban areas extracted from the stable lights data could be used to describe the real process of urban expansion in China from 1992 to 2008. © 2011 IEEE.","<b>Authors:</b><br/>Liu Z., He C., Yang Y. <br/><b>Key words:</b><br/>China, DMSP/OLS, information extraction, Nighttime Lights, urban areas"
418,paper_419,"Maglyas A., Nikula U., Smolander K.",,"Software product management (SPM) offers tools and practices for achieving business goals of a company as well as for increasing the predictability and profitability of software product development. Despite the importance of this topic, the studies of SPM have this far been fragmented. The goal of the present study is to summarize the existing knowledge in software product management and identify the areas which need further research. The paper reports the conduct and the results of a systematic mapping study which identified 25 studies on SPM. Still, most of the papers had only hypotheses and theories that were not empirically confirmed or the confirmation was based on a small set of cases. The existing knowledge of software product management consists of small and unconnected pieces. In addition to this, our specific interest, software product management in the cloud environment has not been studied at all. However, since both researchers and practitioners find research in SPM important, this area needs more research in the future. © 2011 IEEE.","<b>Authors:</b><br/>Maglyas A., Nikula U., Smolander K. <br/><b>Key words:</b><br/>cloud environment, services, software product management, systematic literature review, systematic mapping study"
419,paper_420,"Defoin-Platel M., Hindle M.M., Lysenko A., Powers S.J., Habash D.Z., Rawlings C.J., Saqi M.",,"Background: In response to the rapid growth of available genome sequences, efforts have been made to develop automatic inference methods to functionally characterize them. Pipelines that infer functional annotation are now routinely used to produce new annotations at a genome scale and for a broad variety of species. These pipelines differ widely in their inference algorithms, confidence thresholds and data sources for reasoning. This heterogeneity makes a comparison of the relative merits of each approach extremely complex. The evaluation of the quality of the resultant annotations is also challenging given there is often no existing gold-standard against which to evaluate precision and recall.Results: In this paper, we present a pragmatic approach to the study of functional annotations. An ensemble of 12 metrics, describing various aspects of functional annotations, is defined and implemented in a unified framework, which facilitates their systematic analysis and inter-comparison. The use of this framework is demonstrated on three illustrative examples: analysing the outputs of state-of-the-art inference pipelines, comparing electronic versus manual annotation methods, and monitoring the evolution of publicly available functional annotations. The framework is part of the AIGO library (http://code.google.com/p/aigo) for the Analysis and the Inter-comparison of the products of Gene Ontology (GO) annotation pipelines. The AIGO library also provides functionalities to easily load, analyse, manipulate and compare functional annotations and also to plot and export the results of the analysis in various formats.Conclusions: This work is a step toward developing a unified framework for the systematic study of GO functional annotations. This framework has been designed so that new metrics on GO functional annotations can be added in a very straightforward way. © 2011 Defoin-Platel et al, licensee BioMed Central Ltd.","<b>Authors:</b><br/>Defoin-Platel M., Hindle M.M., Lysenko A., Powers S.J., Habash D.Z., Rawlings C.J., Saqi M. <br/><b>Key words:</b><br/>"
420,paper_421,"Susanti F., Sembiring J.",,"SOA governance is needed to make the company using SOA enables to get maximum profit. One of the failures of SOA sor far is no SOA governance implemented. Yet, there are still weaknesses in SOA governance, such as not providing yet a systematic process to manage service reuse, a guidance to determine what services should be created, a system to validate the suitability of designing and implementation result, and a complete guidance of change management system. Therefore, there are various alternative proposed SOA governance models. In this research, it is combined SOA governance with framework ITIL v3.0. Seeing the mapping of SOA governance concept and ITIL v3.0, it shows that SOA governance and ITIL v.30 have many connections in their elements. ITIL v3.0 enables to complete process to create the service reuse, to determine the role and responsibility in each process, to complete the rules in each process, and to complete the process in overall lifecycle service governance. This research resulted the mapping SOA governance and ITIL v3.0 connect in two main parts, - SOA governance lifecycle and SOA governance elements. SOA governance lifecycle consists of five steps: planning, designing, implementing, controlling, and evaluating. There are five main components in SOA governance, decision, rule and policy, procedure, role and responsibility and metric. These elements and phases are mapped with service strategy, service design, service transition, service operation and continual service improvent in ITIL v3.0. © 2011 IEEE.","<b>Authors:</b><br/>Susanti F., Sembiring J. <br/><b>Key words:</b><br/>ITIL v3.0, SOA governance lifecycle, SOA governance model"
421,paper_422,"Wang L., Chen J., Zhang H., Chen L.",,"Topographic data is one of the fundamental ancillary data for China's Global land cover mapping project at 30m resolution. SRTM C-band DEM(SRTM for short) and ASTER GDEM (ASTER for short) are two global DEM data sets available,but both have some limitations. It's nature to combine them together to derive a better DEM with the accuracy of SRTM and morphologic details of ASTER. A difference analysis of SRTM and ASTER in global scale was conducted by the authors using the least squares technique. The result shows that SRTM and ASTER have the best consistency in Australia. A better DEM can be derived by the fusion of these two data sets for about 94% area in Australia. Only 52% area of America has a tolerable difference of these two Data sets. Systematic errors between SRTM and ASTER were evaluated by a hypotheses test with H0: Systematic errors between them equals to 0. Under significance level ? = 0.05, 4% global land areas rejected hypothesis H0. Specifically 3.4% areas in the north boundary of SRTM(59N?60N strip around the world) rejected hypothesis H0. So no systematic adjustment is necessary when jointing the two DEMs together. Based on our analysis, a fusion method was proposed in considering of different topology conditions, land cover and consistency of the two DEMs. © 2011 IEEE.","<b>Authors:</b><br/>Wang L., Chen J., Zhang H., Chen L. <br/><b>Key words:</b><br/>ASTER GDEM, digital elevation model, Global Land Cover Mapping, least squares technique, SRTM"
422,paper_423,"Murugesupillai E., Mohabbati B., Gaevic D.",,"Service Oriented Architectures (SOA) and Software Product Lines (SPL) have individually proven to be software engineering concepts that create added value to the development of software systems. Recently, the research community has recognized and investigated potentials for combining these two concepts. However, there have been no mapping study and literature surveys that systematically review the present research results in combining the two. This paper presents results of a preliminary work on a systematic mapping study of research papers that report on combining SOA and SPL. The main goal of a systematic mapping study is to provide a breath overview, classification of approaches and the quantity and type of research as well as available research results, which is complimentary step toward further systematic literature review. This paper, based on selected papers published from 2002 to mid-2010, reports on various aspects of the analyzed literature, including the motivations for combining the two concepts, contributions to specific stages of software engineering lifecycles, types of synergies and characteristics that are accomplished through combinations of the two concepts, and the methods used for and the rigor of the evaluations of the research conducted on the studied topic. Copyright © 2011 ACM.","<b>Authors:</b><br/>Murugesupillai E., Mohabbati B., Gaevic D. <br/><b>Key words:</b><br/>Service- oriented product line, Service-oriented architecture, Software product line, Software variability, Variability management"
423,paper_424,"Tang Q., Zhang X.",,"The MODerate resolution Imaging Spectroradiometer (MODIS) data were widely used to estimate land surface evapotranspiration (ET). However, the MODIS ET estimates may not close the land surface water budget. A MODIS ET estimation approach was implemented in the Yellow River Basin of China from 2002 to 2004. The MODIS ET estimates were compared with the estimates from a water budget approach that inferred ET from the Gravity Recovery and Climate Experiment (GRACE) water storage variations, precipitation and streamflow observations. The results showed that the MODIS ET estimates had systematic bias in the monthly mean. The ET estimates from the water budget approach were used to adjust the MODIS ET estimates. The adjusted MODIS ET data have the MODIS spatiotemporal resolution and can fit better with the land surface water budget. It suggests that the MODIS ET estimates may be improved by the use of the GRACE water storage variations. © 2011 IEEE.","<b>Authors:</b><br/>Tang Q., Zhang X. <br/><b>Key words:</b><br/>Gravity Recovery and Climate Experiment (GRACE), land surface evapotranspiration (ET), MODerate resolution Imaging Spectroradiometer (MODIS)"
424,paper_425,"Da Silva F.Q.B., Santos A.L.M., Soares S., Frana A.C.C., Monteiro C.V.F., MacIel F.F.",,"Context: Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008. Objective: In this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published SLRs for education and practice. Method: We performed automatic and manual searches for SLRs published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies. Results: We found 67 new SLRs addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of SLRs in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting SLRs is also increasing and spreading. Conclusion: Our findings suggest that the software engineering research community is starting to adopt SLRs consistently as a research method. However, the majority of the SLRs did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice. © 2011 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Da Silva F.Q.B., Santos A.L.M., Soares S., Frana A.C.C., Monteiro C.V.F., MacIel F.F. <br/><b>Key words:</b><br/>Mapping studies, Software engineering, Systematic reviews, Tertiary studies"
425,paper_426,"Wientapper F., Wuest H., Kuijper A.",,"This paper focuses on the preparative process of retrieving accurate feature maps for a camera-based tracking system. With this system it is possible to create ready-to-use Augmented Reality applications with a very easy setup work-flow, which in practice only involves three steps: filming the object or environment from various viewpoints, defining a transformation between the reconstructed map and the target coordinate frame based on a small number of 3D-3D correspondences and, finally, initiating a feature learning and Bundle Adjustment step. Technically, the solution comprises several sub-algorithms. Given the image sequence provided by the user, a feature map is initially reconstructed and incrementally extended using a Simultaneous-Localization-and- Mapping (SLAM) approach. For the automatic initialization of the SLAM module, a method for detecting the amount of translation is proposed. Since the initially reconstructed map is defined in an arbitrary coordinate system, we present a method for optimally aligning the feature map to the target coordinated frame of the augmentation models based on 3D-3D correspondences defined by the user. As an initial estimate we solve for a rigid transformation with scaling, known as Absolute Orientation. For refinement of the alignment we present a modification of the well-known Bundle Adjustment, where we include these 3D-3D- correspondences as constraints. Compared to ordinary Bundle Adjustment we show that this leads to significantly more accurate reconstructions, since map deformations due to systematic errors such as small camera calibration errors or outliers are well compensated. This again results in a better alignment of the augmentations during run-time of the application, even in large-scale environments. © 2011 IEEE.","<b>Authors:</b><br/>Wientapper F., Wuest H., Kuijper A. <br/><b>Key words:</b><br/>(Monocular) camera tracking, Augmented Reality, Constrained bundle adjustment, Feature-map alignment, Reconstruction, SfM-initialization, Simultaneous localization and mapping (SLAM), Structure-from-motion (SfM)"
426,paper_427,"Barmi Z.A., Ebrahimi A.H., Feldt R.",,"Requirements should specify expectations on a software system and testing should ensure these expectations are met. Thus, to enable high product quality and efficient development it is crucial that requirements and testing activities and information are aligned. A lot of research has been done in the respective fields Requirements Engineering and Testing but there is a lack of summaries of the current state of the art on how to link the two. This study presents a systematic mapping of the alignment of specification and testing of functional or nonfunctional requirements in order to identify useful approaches and needs for future research. In particular we focus on results relevant for nonfunctional requirements but since only a few studies was found on alignment in total we also cover the ones on functional requirements. The map summarizes the 35 relevant papers found and discuss them within six major sub categories with model-based testing and traceability being the ones with most prior results. © 2011 IEEE.","<b>Authors:</b><br/>Barmi Z.A., Ebrahimi A.H., Feldt R. <br/><b>Key words:</b><br/>"
427,paper_428,"Moghaddam S., Helmy A.",,"Users behavior and interests will play a central role in future mobile networks. We introduce a systematic method for large-scale multi-dimensional analysis of online activity for thousands of mobile users across 79 buildings over 100 web domains. We propose a modeling approach based on self-organizing maps for discovering, organizing and visualizing different mobile users' trends from billions of WLAN and netflow records. We find surprisingly that users' trends based on domains or locations can be modeled using a self-organizing map with clearly distinct characteristics. This is the first study to acquire such detailed results for wireless users Internet access patterns. © 2011 IEEE.","<b>Authors:</b><br/>Moghaddam S., Helmy A. <br/><b>Key words:</b><br/>data-driven, self-organizing map, trend, wireless"
428,paper_429,"Fernandez A., Insfran E., Abrahão S.",,"Context: In recent years, many usability evaluation methods (UEMs) have been employed to evaluate Web applications. However, many of these applications still do not meet most customers' usability expectations and many companies have folded as a result of not considering Web usability issues. No studies currently exist with regard to either the use of usability evaluation methods for the Web or the benefits they bring. Objective: The objective of this paper is to summarize the current knowledge that is available as regards the usability evaluation methods (UEMs) that have been employed to evaluate Web applications over the last 14 years. Method: A systematic mapping study was performed to assess the UEMs that have been used by researchers to evaluate Web applications and their relation to the Web development process. Systematic mapping studies are useful for categorizing and summarizing the existing information concerning a research question in an unbiased manner. Results: The results show that around 39% of the papers reviewed reported the use of evaluation methods that had been specifically crafted for the Web. The results also show that the type of method most widely used was that of User Testing. The results identify several research gaps, such as the fact that around 90% of the studies applied evaluations during the implementation phase of the Web application development, which is the most costly phase in which to perform changes. A list of the UEMs that were found is also provided in order to guide novice usability practitioners. Conclusions: From an initial set of 2703 papers, a total of 206 research papers were selected for the mapping study. The results obtained allowed us to reach conclusions concerning the state-of-the-art of UEMs for evaluating Web applications. This allowed us to identify several research gaps, which subsequently provided us with a framework in which new research activities can be more appropriately positioned, and from which useful information for novice usability practitioners can be extracted. © 2011 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Fernandez A., Insfran E., Abrahão S. <br/><b>Key words:</b><br/>Systematic mapping, Usability evaluation methods, Web development"
429,paper_431,"Muñoz L., Mazon J.N., Trujillo J.",,"BACKGROUND: A data warehouse (DW) is an integrated collection of subject-oriented data in the support of decision making. Importantly, the integration of data sources is achieved through the use of ETL (Extract, Transform, and Load) processes. It is therefore extensively recognized that the appropriate design of the ETL processes are key factors in the success of DW projects. OBJECTIVE: We assess existing research proposals about ETL process modeling for data warehouse in order to identify their main characteristics, notation, and activities. We also study if these modeling approaches are supported by some kind of prototype or tool. METHOD: We have undertaken a systematic mapping study of the research literature about modeling ETL processes. A mapping study provides a systematic and objective procedure for identifying the nature and extent of the available research by means of research questions. RESULTS: The study is based on a comprehensive set of papers obtained after using a multi-stage selection criteria and are published in international workshops, conferences and journals between 2000 and 2009. CONCLUSIONS: This systematic mapping study states that there is a clear classification of ETL process modeling approaches, but that they are not enough covered by researchers. Therefore, more effort is required to bridge the research gap in modeling ETL processes. © 2011 IEEE.","<b>Authors:</b><br/>Muñoz L., Mazon J.N., Trujillo J. <br/><b>Key words:</b><br/>data warehouse, ETL process, modeling conceptual, systematic mapping studies"
430,paper_432,"Kitchenham B.A., Budgen D., Pearl Brereton O.",,"Context: We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research. Objective: This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic. Method: We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies. Results: Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers. Conclusion: Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Kitchenham B.A., Budgen D., Pearl Brereton O. <br/><b>Key words:</b><br/>Case study, Mapping studies, Software engineering, Systematic literature review"
431,paper_433,"Da Mota Silveira Neto P.A., Carmo MacHado I.D., McGregor J.D., De Almeida E.S., De Lemos Meira S.R.",,"Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Da Mota Silveira Neto P.A., Carmo MacHado I.D., McGregor J.D., De Almeida E.S., De Lemos Meira S.R. <br/><b>Key words:</b><br/>Mapping study, Software product lines, Software testing"
432,paper_434,"Lane S., Richardson I.",,"Context: Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them. Objective: The objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs. Method: In order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted. Results: During this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified. Conclusion: There are many process models reported for the development of SBAs varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Lane S., Richardson I. <br/><b>Key words:</b><br/>Service-based application, SOA, Software process, Systematic literature review"
433,paper_435,"Budgen D., Burn A.J., Brereton O.P., Kitchenham B.A., Pretorius R.",,"The Unified Modeling Language (UML) was created on the basis of expert opinion and has now become accepted as the 'standard' object-oriented modelling notation. Our objectives were to determine how widely the notations of the UML, and their usefulness, have been studied empirically, and to identify which aspects of it have been studied in most detail. We undertook a mapping study of the literature to identify relevant empirical studies and to classify them in terms of the aspects of the UML that they studied. We then conducted a systematic literature review, covering empirical studies published up to the end of 2008, based on the main categories identified. We identified 49 relevant publications, and report the aggregated results for those categories for which we had enough papers-metrics, comprehension, model quality, methods and tools and adoption. Despite indications that a number of problems exist with UML models, researchers tend to use the UML as a 'given' and seem reluctant to ask questions that might help to make it more effective. © 2010 John Wiley & Sons, Ltd.","<b>Authors:</b><br/>Budgen D., Burn A.J., Brereton O.P., Kitchenham B.A., Pretorius R. <br/><b>Key words:</b><br/>systematic literature review, UML"
434,paper_436,"Petersen K.",,"Context: Software productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results. Objective: In this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted. Method: As a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used. Results: In total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners. Conclusion: Risks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Petersen K. <br/><b>Key words:</b><br/>Efficiency, Measurement, Performance, Prediction, Software development, Software productivity"
435,paper_437,"Durelli V.H.S., Felizardo K.R., Delamaro M.E.",,"Background: There is a large body of literature on research in virtual machine for high-level languages, i.e., high-level language virtual machines (HLL VMs). Despite being a well-established research area, there are no studies focusing on characterizing the sorts of research that have been conducted and shedding light on most investigated subjects as well as subjects requiring further research. Objectives: To conduct a systematic mapping study of the literature describing research into HLL VM. Research method: We undertook a systematic mapping study of the literature based upon searching of major electronic databases. Results: 128 papers have been selected and classified by their contribution, employed HLL VM implementation, type and date of publication. Conclusions: The majority of the selected studies concentrate on improvements for boosting performance, introducing better garbage collection capabilities, and adapting HLL VMs or their core components to meet the requirements for embedded platforms. Furthermore, from examining the selected studies we have found that Java virtual machine (JVM) implementations are by far the most employed within academic settings. Among them, Jikes Research Virtual Machine is the most-widely used. Copyright © 2010 ACM.","<b>Authors:</b><br/>Durelli V.H.S., Felizardo K.R., Delamaro M.E. <br/><b>Key words:</b><br/>Evidence, High-level language virtual machine, Systematic mapping study"
436,paper_438,"Palacios M., García-Fanjul J., Tuya J.",,"Context: Service Oriented Architectures (SOA) have emerged as a new paradigm to develop interoperable and highly dynamic applications. Objective: This paper aims to identify the state of the art in the research on testing in Service Oriented Architectures with dynamic binding. Method: A mapping study has been performed employing both manual and automatic search in journals, conference/workshop proceedings and electronic databases. Results: A total of 33 studies have been reviewed in order to extract relevant information regarding a previously defined set of research questions. The detection of faults and the decision making based on the information gathered from the tests have been identified as the main objectives of these studies. To achieve these goals, monitoring and test case generation are the most proposed techniques testing both functional and non-functional properties. Furthermore, different stakeholders have been identified as participants in the tests, which are performed in specific points in time during the life cycle of the services. Finally, it has been observed that a relevant group of studies have not validated their approach yet. Conclusions: Although we have only found 33 studies that address the testing of SOA where the discovery and binding of the services are performed at runtime, this number can be considered significant due to the specific nature of the reviewed topic. The results of this study have contributed to provide a body of knowledge that allows identifying current gaps in improving the quality of the dynamic binding in SOA using testing approaches. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Palacios M., García-Fanjul J., Tuya J. <br/><b>Key words:</b><br/>Dynamic binding, Mapping study, Service Oriented Architectures, SOA, Software testing, Systematic literature review"
437,paper_439,"Park W.-J., Bae D.-H.",,"Context: Specification matching techniques are crucial for effective retrieval processes. Despite the prevalence for object-oriented methodologies, little attention has been given to Unified Modeling Language (UML) for matching. Objective: This paper presents a two-stage framework for matching two UML specifications and quantifying the results based on the systematic integration of their structural and behavioral similarities in order to identify the candidate component set for reuse. Method: The first stage in the framework is an evaluation of the similarities between UML class diagrams using the Structure-Mapping Engine (SME), a simulation of the analogical reasoning approach known as the structure-mapping theory. The second stage, performed on the components identified in the first stage, is based on a graph-similarity scoring algorithm in which UML class diagrams and sequence diagrams are transformed into an SME representation and a Message-Object-Order Graph (MOOG). The effectiveness of the proposed framework was evaluated using a case study. Results: The experimental results showed a reduction in potential mismatches and an overall high precision and recall. Conclusion: It is concluded that the two-stage framework is capable of performing more precise matching compared to those of other single-stage matching frameworks. Moreover, the two-stage framework could be utilized within a reuse process, bypassing the need for extra information for retrieval of the components described by UML. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Park W.-J., Bae D.-H. <br/><b>Key words:</b><br/>Graph similarity scoring, Specification matching, Structure mapping, UML"
438,paper_440,"Sigrist R., Schellenberg J., Rauter G., Broggi S., Riener R., Wolf P.",,"In general, concurrent augmented feedback has been shown to effectively enhance learning in complex motor tasks. However, to optimize technical systems that are intended to reinforce motor learning, a systematic evaluation of different augmented feedback designs is required. Until now, mainly visual augmented feedback has been applied to enhance learning of complex motor tasks. Since most complex motor tasks are mastered in response to information visually perceived, providing augmented concurrent feedback in a visual manner may overload the capacities of visual perception and cognitive processing. Thus, the aim of this work was to evaluate the practicability of auditory feedback designs supporting a three-dimensional rowing-type movement in comparison with visual feedback designs. We term a feedback design practical if the provided information can easily be perceived and interpreted, and immediately be used to support the movement. In a first experiment, it became evident that participants could interpret three-dimensional auditory feedback designs based on stereo balance, pitch, timbre, and/or volume. Eleven of 12 participants were able to follow the different target movements using auditory feedback designs as accurately as with a very abstract visual feedback design. Visual designs based on superposition of actual and target oar orientation led to the most accurate performance. Considering the first experimental results, the feedback designs were further developed and again evaluated. It became evident that a permanent visual display of the target trajectories could further enhance movement accuracy. Moreover, results indicated that the practicability of the auditory designs depends on the polarity of the mapping functions. In general, both visual and auditory concurrent feedback designs were practical to immediately support multidimensional movement. In a next step, the effectiveness to enhance motor learning will be systematically evaluated. © 2011 by the Massachusetts Institute of Technology.","<b>Authors:</b><br/>Sigrist R., Schellenberg J., Rauter G., Broggi S., Riener R., Wolf P. <br/><b>Key words:</b><br/>"
439,paper_441,"Brochhausen M., Spear A.D., Cocos C., Weiler G., Martín L., Anguita A., Stenzhorn H., Daskalaki E., Schera F., Schwarz U., Sfakianakis S., Kiefer S., Dörr M., Graf N., Tsiknakis M.",,"Objective: This paper introduces the objectives, methods and results of ontology development in the EU co-funded project Advancing Clinico-genomic Trials on Cancer - Open Grid Services for Improving Medical Knowledge Discovery (ACGT). While the available data in the life sciences has recently grown both in amount and quality, the full exploitation of it is being hindered by the use of different underlying technologies, coding systems, category schemes and reporting methods on the part of different research groups. The goal of the ACGT project is to contribute to the resolution of these problems by developing an ontology-driven, semantic grid services infrastructure that will enable efficient execution of discovery-driven scientific workflows in the context of multi-centric, post-genomic clinical trials. The focus of the present paper is the ACGT Master Ontology (MO). Methods: ACGT project researchers undertook a systematic review of existing domain and upper-level ontologies, as well as of existing ontology design software, implementation methods, and end-user interfaces. This included the careful study of best practices, design principles and evaluation methods for ontology design, maintenance, implementation, and versioning, as well as for use on the part of domain experts and clinicians. Results: To date, the results of the ACGT project include (i) the development of a master ontology (the ACGT-MO) based on clearly defined principles of ontology development and evaluation, (ii) the development of a technical infrastructure (the ACGT Platform) that implements the ACGT-MO utilizing independent tools, components and resources that have been developed based on open architectural standards, and which includes an application updating and evolving the ontology efficiently in response to end-user needs, and (iii) the development of an Ontology-based Trial Management Application (ObTiMA) that integrates the ACGT-MO into the design process of clinical trials in order to guarantee automatic semantic integration without the need to perform a separate mapping process. © 2010 Elsevier Inc.","<b>Authors:</b><br/>Brochhausen M., Spear A.D., Cocos C., Weiler G., Martín L., Anguita A., Stenzhorn H., Daskalaki E., Schera F., Schwarz U., Sfakianakis S., Kiefer S., Dörr M., Graf N., Tsiknakis M. <br/><b>Key words:</b><br/>Cancer research, Clinical trial administration, Ontological engineering, Ontology, Translational medicine"
440,paper_442,"Barbosa O., Alves C.",,"Software ecosystem is an approach that investigates the complex relationships among companies in the software industry. Companies work cooperatively and competitively in order to achieve their strategic objectives. They must engage in a new perspective considering both their own business and third party ones. Inspired from properties by natural and business ecosystems, a software ecosystem covers technical and business aspects of software development as well as partnership among companies. In this paper, we undertake a systematic mapping study to present a wide review of primary studies on software ecosystems. Systematic mapping is a methodology that gives, after a systematic research process, a visual summary map of its results. The search procedure identified 1026 studies, of which 44 were identified as relevant to answer our research questions. This study mapped what is currently known about software ecosystems perspective. We conclude that software ecosystems research is concentrated in 8 main areas in which the most relevant are open source software, ecosystem modeling, and business issues. The paper is intended to practitioners and academics investigating the field of software ecosystems. It contributes to summarize the body of knowledge in the field and direct efforts for future research in software ecosystems.","<b>Authors:</b><br/>Barbosa O., Alves C. <br/><b>Key words:</b><br/>Business ecosystem, Digital ecosystem, Software ecosystem, Systematic mapping study"
441,paper_443,"Engström E., Runeson P.",,"Context: Software product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped. Objective: This study aims at surveying existing research on SPL testing in order to identify useful approaches and needs for future research. Method: A systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type. Results: A majority of the papers are of proposal research types (64%). System testing is the largest group with respect to research focus (40%), followed by management (23%). Method contributions are in majority. Conclusions: More validation and evaluation research is needed to provide a better foundation for SPL testing. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Engström E., Runeson P. <br/><b>Key words:</b><br/>Software product line testing, Systematic literature review, Systematic mapping, Testing"
442,paper_444,"Abdul Qadoos Bilal Khan M., Shahbaz A., Shahzad F.",,"Open source software license is a contract under which any software release is called open source software. In 1980's, R. Stallman formed free software foundation which later introduced GNU GPL license. Any software which is release under this license is called open source software. Open source software is software which is free of cost and provides equal right to both contributors and users to redistribute, inspect and modification of its code. Therefore it has been attracted a large number of contributors from their community toward open source software movement. This also encouraged introducing different types of open source software licenses. Different open source software license introduces different types of benefits for both contributors and users. There are many factors, which can influence on a contributor's decision about the selection of open source software license. Therefore we gather motivation factors for which a contributor participates in open source software and also gather motivation factors on which a contributor takes decision about selecting open source software license. In this paper we conduct systematic mapping study. In this systematic mapping we accumulate publications on the motivation factors for which contributors participate in open source software and we also accumulate publications on motivation factors which influence on contributors in selection of open source software license. From the frequency of publications we found out the trends in this area of research. In this systematic mapping we included evaluation research, review paper mostly. © 2005 - 2011 JATIT & LLS. All rights reserved.","<b>Authors:</b><br/>Abdul Qadoos Bilal Khan M., Shahbaz A., Shahzad F. <br/><b>Key words:</b><br/>Open source software, Open source software license, Systematic mapping"
443,paper_445,"Li-Chee-ming J., Armenakis C., Lee R.",,"A low-cost portable light-weight mobile stereo-mapping system (MSMS) is under development in the GeoICT Lab, Geomatics Engineering program at York University. The MSMS is designed for remote operation on board unmanned aerial vehicles (UAV) for navigation and rapid collection of 3D spatial data. Pose estimation of the camera sensors is based on single frequency RTK-GPS, loosely coupled in a Kalman filter with MEMS-based IMU. The attitude and heading reference system (AHRS) calculates orientation from the gyro data, aided by accelerometer and magnetometer data to compensate for gyro drift. Two low-cost consumer digital cameras are calibrated and time-synchronized with the GPS/IMU to provide direct georeferenced stereo vision, while a video camera is used for navigation. Object coordinates are determined using rigorous photogrammetric solutions supported by direct georefencing algorithms for accurate pose estimation of the camera sensors. Before the MSMS is considered operational its sensor components and the integrated system itself has to undergo a rigorous calibration process to determine systematic errors and biases and to determine the relative geometry of the sensors. In this paper, the methods and results for system calibration, including camera, boresight and leverarm calibrations are presented. An overall accuracy assessment of the calibrated system is given using a 3D test field.","<b>Authors:</b><br/>Li-Chee-ming J., Armenakis C., Lee R. <br/><b>Key words:</b><br/>3D mapping, Direct georeferencing, GPS, IMU, Magnetometer, MEMS, Mobile stereo-mapping systems, Sensor and system calibration, Unmanned aerial vehicles"
444,paper_446,"Nakagawa E.Y., Feitosa D., Felizardo K.R.",,"Software architectures have played a significant role in determining the success of software systems. In this perspective, a lot of work have been conducted and considerable knowledge in the software architecture area has been accumulated. In another perspective, systematic mapping is a technique that provides an overview of a research area to assess the quantity of evidence existing on a topic of interest. Thus, the main objective of this paper is to introduce systematic mapping in order to explore, understand, organize and summarize software architecture knowledge, aiming at contributing to software architecture area. A simple example of use of this technique is presented and results point out to its viability also in the software architecture research area. Copyright 2010 ACM.","<b>Authors:</b><br/>Nakagawa E.Y., Feitosa D., Felizardo K.R. <br/><b>Key words:</b><br/>Reference architecture, Software architecture knowledge, Systematic mapping"
445,paper_447,"Purdy K., Kindt L., Densmore J., Benson C., Zhou N., Leonard J., Whiteside C., Nolan R., Shanks D.",,"Lean manufacturing is a systematic method of identifying and eliminating waste. Use of Lean manufacturing techniques at the IBM photomask manufacturing facility has increased efficiency and productivity of the photomask process. Tools, such as, value stream mapping, 5S and structured problem solving are widely used today. In this paper we describe a step-by-step Lean technique used to systematically decrease defects resulting in reduced material costs, inspection costs and cycle time. The method used consists of an 8-step approach commonly referred to as the 8D problem solving process. This process allowed us to identify both prominent issues as well as more subtle problems requiring in depth investigation. The methodology used is flexible and can be applied to numerous situations. Advantages to Lean methodology are also discussed. © 2010 SPIE.","<b>Authors:</b><br/>Purdy K., Kindt L., Densmore J., Benson C., Zhou N., Leonard J., Whiteside C., Nolan R., Shanks D. <br/><b>Key words:</b><br/>Defect reduction, Lean, Lean manufacturing"
446,paper_448,"Wimmer M., Kappel G., Kusel A., Retschitzegger W., Schoenboeck J., Schwinger W.",,"A crucial prerequisite for the success of Model Driven Engineering (MDE) is the seamless exchange of models between different modeling tools demanding for mappings between tool-specific metamodels. Thereby the resolution of heterogeneities between these tool-specific metamodels is a ubiquitous problem representing the key challenge. Nevertheless, there is no comprehensive classification of potential heterogeneities available in the domain of MDE. This hinders the specification of a comprehensive benchmark explicating requirements wrt. expressivity of mapping tools, which provide reusable components for resolving these heterogeneities. Therefore, we propose a feature-based classification of heterogeneities, which accordingly adapts and extends existing classifications. This feature-based classification builds the basis for a mapping benchmark, thereby providing a comprehensive set of requirements concerning expressivity of dedicated mapping tools. In this paper a first set of benchmark examples is presented by means of metamodels and conforming models acting as an evaluation suite for mapping tools.","<b>Authors:</b><br/>Wimmer M., Kappel G., Kusel A., Retschitzegger W., Schoenboeck J., Schwinger W. <br/><b>Key words:</b><br/>Classification of heterogeneities, Mapping benchmark"
447,paper_449,"Feitosa D., Felizardo K.R., De Oliveira L.B.R., Wolf D., Nakagawa E.Y.",,"Currently, embedded software have been required more and more by a diversity of new products. As a consequence, an increase in the software complexity can be observed, requiring more attention to the software quality. Initiatives of exploring software engineering knowledge to develop this type of software can be identified, resulting in the Embedded Software Engineering (ESE) research area. However, there is a lack of a complete panorama about researches conducted in the context of ESE. This paper intends to present a view about how software engineering has been currently used in the embedded software development. For this, we have used systematic mapping, a technique based in the Evidence-Based Software Engineering (EBSE). Achieved results point out that in spite of the increase in the interest of applying software engineering to develop embedded software, there are still important lines of research that must receive attention.","<b>Authors:</b><br/>Feitosa D., Felizardo K.R., De Oliveira L.B.R., Wolf D., Nakagawa E.Y. <br/><b>Key words:</b><br/>"
448,paper_450,"Johnson T.",,"Risk Management is a logical and systematic method of identifying, analyzing, treating and monitoring the risks involved in any activity or process. The key to successful risk management lies in the ability to tailor a formal risk management process that addresses the complementary needs of the business and its customers. A formal risk management process is a continuous process for systematically addressing risk throughout the product/project life-cycle. Risks can be introduced (or latently reside) at the very earliest stages of the project life-cycle. The ability to identify risks earlier translates into earlier risk removal, at less cost, which promotes higher project success probability. Data mining refers to discovery or ""mining"" of knowledge from large amounts of data. Data Mining has been described as a confluence of different disciplines primarily database systems, statistics, machine learning and information science. This paper aims to study the conceptual mapping of Risk Management to Data Mining. A new paradigm has been suggested for Risk Management using the main attributes and key aspects of Data Mining. © 2010 IEEE.","<b>Authors:</b><br/>Johnson T. <br/><b>Key words:</b><br/>Conceptual mapping, Data Mining, Risk Management"
449,paper_451,"Kim Y.S., Lee S.W., Maeng J.W., Cho C.K.",,"As consumers demand diverse values reflecting their individual needs and wants from various viewpoints, including economical, ecological and experiential concerns, more comprehensive and more flexible ways to provide values to consumers are desired. Product-Service Systems (PSS) have been proposed as a solution to realize such diverse value provision. In this paper, a systematic methodology for designing PSS based on activities and functions is proposed, which is much different from the case of product design. The proposed PSS design process includes the following six steps: requirement identification and value proposition, stakeholder activity design, PSS functional modeling, function-activity mapping and PSS concept generation, PSS concept detailing and PSS concept prototyping. In the proposed PSS design process, the activities of stakeholders are defined and analyzed via service blueprint. The functions of PSS fulfilling target values are then defined and represented with the specification service providers and service receivers, and they are further decomposed into sub-functions. Then the relationship between stakeholder activities and functions are established by considering associated stakeholders, and the PSS concepts are generated by mapping product and service elements. Sample case studies are conducted to validate the proposed PSS design process. Copyright © 2010 by ASME.","<b>Authors:</b><br/>Kim Y.S., Lee S.W., Maeng J.W., Cho C.K. <br/><b>Key words:</b><br/>Activity, Function, Product-Service Systems (PSS), PSS design process"
450,paper_452,"Basori A.H., Bade A., Sunar M.S., Daman D., Saari N.",,"Current and existing avatars are only able to express their emotional expressions only by using facial animation, gesture and voice. Yet, only few researchers successfully combined haptic tactile with human emotion expression. In this paper, we present an avatar in a creative way that can gain user attention while they play game and use computer. Our proposed technique is achieved through combination of visual sense and systematic mapping of sense of touch. The mapping procedure is developed by systemically map the emotion layer into color spectrum based on the standard color emotion theories. To prove our solution, we benchmark our proposed technique prototype with existing system like Alfred. From the conducted experiment, we found that 57% respondents feel strong expression of virtual head emotion expression when dedicated haptic device stimulates their sensory nerve. Using our design solution, we can see that psychological signal can be adapted with vibration characteristic.","<b>Authors:</b><br/>Basori A.H., Bade A., Sunar M.S., Daman D., Saari N. <br/><b>Key words:</b><br/>Avatar, Colour, Emotion, Facial expression"
451,paper_453,"Sano S., Sano M., Sato S., Miyoshi T., Kise K.",,"The Network-on-Chip (NoC) is a promising interconnection for many-core processors. On the NoC-based manycore processors, the network performance of multi-thread programs depends on the method of task mapping. In this paper, we propose a pattern-based task mapping method in order to improve the performance of many-core processors. Evaluation of the proposed method using a detailed software simulator reveals an average performance improvement of at least 4.4%, as compared with standard task mapping using NAS parallel benchmarks. © 2010 IEEE.","<b>Authors:</b><br/>Sano S., Sano M., Sato S., Miyoshi T., Kise K. <br/><b>Key words:</b><br/>"
452,paper_454,"Kitchenham B.A., Brereton P., Turner M., Niazi M.K., Linkman S., Pretorius R., Budgen D.",,"Systematic literature reviews (SLRs) are a major tool for supporting evidence-based software engineering. Adapting the procedures involved in such a review to meet the needs of software engineering and its literature remains an ongoing process. As part of this process of refinement, we undertook two case studies which aimed 1) to compare the use of targeted manual searches with broad automated searches and 2) to compare different methods of reaching a consensus on quality. For Case 1, we compared a tertiary study of systematic literature reviews published between January 1, 2004 and June 30, 2007 which used a manual search of selected journals and conferences and a replication of that study based on a broad automated search. We found that broad automated searches find more studies than manual restricted searches, but they may be of poor quality. Researchers undertaking SLRs may be justified in using targeted manual searches if they intend to omit low quality papers, or they are assessing research trends in research methodologies. For Case 2, we analyzed the process used to evaluate the quality of SLRs. We conclude that if quality evaluation of primary studies is a critical component of a specific SLR, assessments should be based on three independent evaluators incorporating at least two rounds of discussion. © 2010 Springer Science+Business Media, LLC.","<b>Authors:</b><br/>Kitchenham B.A., Brereton P., Turner M., Niazi M.K., Linkman S., Pretorius R., Budgen D. <br/><b>Key words:</b><br/>Automated search, Broad search, Case study, Manual search, Mapping studies, Quality evaluation process, Systematic literature review, Targeted search"
453,paper_455,"Moghaddam S., Helmy A.",,"Human behavior and interests will play a central role in future mobile networks. We introduce a systematic method for large-scale multi-dimensional analysis of online activity for thousands of mobile users over 100 web domains. We propose a modeling approach based on self-organizing maps for discovering, organizing and visualizing different mobile users' trends from billions of WLAN and netflow records. We find surprisingly that users' trends can be modeled using a self-organizing map with clearly distinct characteristics while similar domains are modeled by neighboring nodes. This is the first study to acquire such detailed results for mobile Internet usage. ©2010 IEEE.","<b>Authors:</b><br/>Moghaddam S., Helmy A. <br/><b>Key words:</b><br/>Data-driven, Self-organizing map, Trend, Wireless"
454,paper_456,"Hu H., Du X.",,"The Linked Data Clouds are the best expression and realization of the Semantic Web vision to date. The Data Clouds can significantly benefit both the AI and Semantic Web communities by enabling new classes of tasks and enhancing reasoning, data mining and knowledge discovery applications. There are various forms of spatiotemporal data in the Linked Data Clouds. To efficiently utilize these spatiotemporal data, schema heterogeneity problem must be resolved. A hierarchical spatiotemporal model is proposed to give a conceptual description of different spatiotemporal dataset of Linked Data Clouds. The model can support schema level mappings and convey relationships between concepts of different datasets at the schema level. The hierarchical spatiotemporal model contains three layers: an meta level for abstract level spacetime knowledge, an schema level for well-known models in spatial and temporal reasoning - Allen's Interval Algebra in temporal reasoning and the RCC model in spatial reasoning, and an instantiations level to provide systematic mapping and formal descriptions of the various ground spatiotemporal statements. © 2010 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Hu H., Du X. <br/><b>Key words:</b><br/>Linked Data, SPARQL, Spatiotemporal Data"
455,paper_457,"Koziel S., Ogurtsov S., Bakr M.H.",,"Design of UWB antennas is a challenging task due to the lack of systematic design procedures. On the other hand, high computational cost of EM simulation of antenna structures makes a direct EM-based optimization impractical. There has been some tendency to use global optimization methods such as genetic algorithms [1] or particle swarm optimizers [2] in antenna design. Although these methods alleviate some of the difficulties of the traditional, gradient-based techniques (e.g., allow handling multiple local optima), they are characterized by huge computational overhead. © 2010 IEEE.","<b>Authors:</b><br/>Koziel S., Ogurtsov S., Bakr M.H. <br/><b>Key words:</b><br/>"
456,paper_458,"Agrafiotis D.K., Xu H., Zhu F., Bandyopadhyay D., Liu P.",,"Since its inception in 1996, the stochastic proximity embedding (SPE) algorithm and its variants have been applied to a wide range of problems in computational chemistry and biology with notable success. At its core, SPE attempts to generate Euclidean coordinates for a set of points so that they satisfy a prescribed set of geometric constraints. The algorithm's appeal rests on three factors: 1) its conceptual and programmatic simplicity, 2) its superior speed and scaling properties, and 3) its broad applicability. Here, we review some of the key applications, outline known limitations and ways to circumvent them, and highlight additional problem domains where the use of this technique could lead to significant breakthroughs. © 2010 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim.","<b>Authors:</b><br/>Agrafiotis D.K., Xu H., Zhu F., Bandyopadhyay D., Liu P. <br/><b>Key words:</b><br/>Alignment, Boosting, Conformational analysis, Dimensionality reduction, Docking, Loop modeling, Manifold learning, Nonlinear mapping, Pharmacophore, Protein loop, Self-organizing superposition, Stochastic proximity embedding, Stochastic search, Structure depiction, Systematic search"
457,paper_459,"Da Silva F.Q.B., Santos A.L.M., Soares S.C.B., França A.C.C., Monteiro C.V.F.",,"After a seminal article introducing-evidence based software engineering in 2004, systematic reviews (SR) have been increasingly used as a method for conducting secondary studies in software engineering. Our goal is to critically appraise the use of SR in software engineering with respect to the research questions asked and the ways the questions were used in the reviews. We analyzed 53 literature reviews that had been collected in two published tertiary studies. We found that over 65% of the research questions asked in the reviews were exploratory and only 15% investigated causality questions. We concluded that there is a need for a consistent use of terminology to classify secondary studies and that reports of literature reviews should follow reporting guidelines to support assessment and comparison. © 2010 ACM.","<b>Authors:</b><br/>Da Silva F.Q.B., Santos A.L.M., Soares S.C.B., França A.C.C., Monteiro C.V.F. <br/><b>Key words:</b><br/>mapping studies, software engineering, systematic reviews"
458,paper_460,"Kitchenham B., Pretorius R., Budgen D., Brereton O.P., Turner M., Niazi M., Linkman S.",,"Context: In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007. Objective: The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search. Method: We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study. Results: Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines. Conclusion: SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality. © 2010 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Kitchenham B., Pretorius R., Budgen D., Brereton O.P., Turner M., Niazi M., Linkman S. <br/><b>Key words:</b><br/>Mapping study, Software engineering, Systematic literature review, Tertiary study"
459,paper_461,"Wimmer M., Kappel G., Kusel A., Retschitzegger W., Schoenboeck J., Schwinger W.",,"Model transformations play a key role in the vision of Model-Driven Engineering. Nevertheless, mechanisms like abstraction, variation and composition for specifying and applying reusable model transformations - like urgently needed for resolving recurring structural heterogeneities - are insufficiently supported so far. Therefore, we propose to specify model transformations by a set of pre-defined mapping operators (MOps), each resolving a certain kind of structural heterogeneity. Firstly, these MOps can be used in the context of arbitrary metamodels since they abstract from concrete metamodel types. Secondly, MOps can be tailored to resolve certain structural heterogeneities by means of black-box reuse. Thirdly, based on a systematic set of kernel MOps resolving basic heterogeneities, composite ones can be built in order to deal with more complex scenarios. Finally, an extensible library of MOps is proposed, allowing for automatically executable mapping specifications since every MOp exhibits a clearly defined operational semantics. © 2010 Springer-Verlag.","<b>Authors:</b><br/>Wimmer M., Kappel G., Kusel A., Retschitzegger W., Schoenboeck J., Schwinger W. <br/><b>Key words:</b><br/>Executable Mappings, Reuse, Structural Heterogeneities"
460,paper_462,"Kitchenham B., Brereton P., Budgen D.",,"We identify three challenges related to the provenance of the material we use in teaching software engineering. We suggest that these challenges can be addressed by using evidence-based software engineering (EBSE) and its primary tool of systematic literature reviews (SLRs). This paper aims to assess the educational and scientific value of undergraduate and postgraduate students undertaking a specific form of SLR called a mapping study. Using a case study methodology, we asked three postgraduate students and three undergraduates and their supervisor to complete a questionnaire concerning the educational value of mapping studies and any problems they experienced. Students found undertaking a mapping study to be a valuable experience providing both reusable research skills and a good overview of a research topic. Postgraduates found it useful as a starting point for their studies. Undergraduates reported problems undertaking the study in the required timescales. Searching and classifying the literature was difficult. © 2010 ACM.","<b>Authors:</b><br/>Kitchenham B., Brereton P., Budgen D. <br/><b>Key words:</b><br/>education, evidence-based software engineering, mapping studies, systematic literature review"
461,paper_463,"Alexe B., Kolaitis P.G., Tan W.-C.",,"Schema mappings are high-level specifications that describe the relationship between two database schemas, they are considered to be the essential building blocks in data exchange and data integration, and have been the object of extensive research investigations. Since in real-life applications schema mappings can be quite complex, it is important to develop methods and tools for understanding, explaining, and refining schema mappings. A promising approach to this effect is to use ""good"" data examples that illustrate the schema mapping at hand. We develop a foundation for the systematic investigation of data examples and obtain a number of results on both the capabilities and the limitations of data examples in explaining and understanding schema mappings. We focus on schema mappings specified by source-to-target tuple generating dependencies (s-t tgds) and investigate the following problem: which classes of s-t tgds can be ""uniquely characterized"" by a finite set of data examples? Our investigation begins by considering finite sets of positive and negative examples, which are arguably the most natural choice of data examples. However, we show that they are not powerful enough to yield interesting unique characterizations. We then consider finite sets of universal examples, where a universal example is a pair consisting of a source instance and a universal solution for that source instance. We unveil a tight connection between unique characterizations via universal examples and the existence of Armstrong bases (a relaxation of the classical notion of Armstrong databases). On the positive side, we show that every schema mapping specified by LAV s-t tgds is uniquely characterized by a finite set of universal examples with respect to the class of LAV s-t tgds. Moreover, this positive result extends to the much broader classes of n-modular schema mappings, n a positive integer. Finally, we show that, on the negative side, there are schema mappings specified by GAV s-t tgds that are not uniquely characterized by any finite set of universal examples and negative examples with respect to the class of GAV s-t tgds (hence also with respect to the class of all s-t tgds). © 2010 ACM.","<b>Authors:</b><br/>Alexe B., Kolaitis P.G., Tan W.-C. <br/><b>Key words:</b><br/>data examples, data exchange, data integration, schema mappings"
462,paper_464,"Pellenz J., Paulus D.",,"Often Particle Filters are used to solve the SLAM (Simultaneous Localization and Mapping) problem in robotics: The particles represent the possible poses of the robot, and their weight is determined by checking if the sensor readings are consistent with the so far acquired map. Mostly a single map is maintained during the exploration, and only with Rao-Blackwellized Particle Filters each particle carries its own map. In this contribution, we propose a Hyper Particle Filter (HPF) - a Particle Filter of Particle Filters - for solving the SLAM problem in unstructured environments. Each particle of the HPF contains a standard Particle Filter (with a map and a set particles, that model the belief of the robot pose in this particular map). To measure the weight of a particle in the HPF, we developed two map quality measures that can be calculated automatically and do not rely on a ground truth map: The first map quality measure determines the contrast of the occupancy map. If the map has a high contrast, it is likely that the pose of the robot was always determined correctly before the map was updated, which finally leads to an overall consistent map. The second map quality measure determines the distribution of the orientation of wall pixels calculated by the Sobel operator. Using the model of a rectangular overall structure, slight but systematic errors in the map can be detected. Using the two measures, broken maps can automatically be detected. The corresponding particle is then more likely to be replaced by a particle with a better map within the HPF. We implemented the approach on our robot ""Robbie 12"", which will be used in the RoboCup Rescue league in 2009. We tested the HPF using the log files from last years RoboCup Rescue autonomy final, and with new data of a larger building. The quality of the generated maps outperformed our last years (league's best) maps. With the data acquired in the larger structure, Robbie was able to close loops in the map. Due to a highly efficient implementation, the algorithm still runs online during the autonomous exploration. © 2010 Springer.","<b>Authors:</b><br/>Pellenz J., Paulus D. <br/><b>Key words:</b><br/>"
463,paper_465,"Ochoa I., Crespo P., Ser J.D., Hernaez M.",,"This letter proposes a novel one-layer coding/ shaping scheme with single-level codes and sigma-mapping for the bandwidth-limited regime. Specifically, we consider nonuniform memoryless sources sent over AWGN channels. At the transmitter, binary data are encoded by a Turbo code composed of two identical RSC (Recursive Systematic Convolutional) encoders. The encoded bits are randomly interleaved and modulated before entering the sigma-mapper. The modulation employed in this system follows the unequal energy allocation scheme first introduced in [1]. The receiver consists of an iterative demapping/decoding algorithm, which incorporates the a priori probabilities of the source symbols. To the authors knowledge, work in this area has only been done for the power-limited regime. In particular, the authors in [2] proposed a scheme based on a Turbo code with RSC encoders and unequal energy allocation. Therefore, it is reasonable to compare the performance with respect to the Shannon limit of our proposed bandwidthlimited regime scheme with this former power-limited regime scheme. Simulation results show that our performance is as good or slightly better than that of the system in [2]. © 2006 IEEE.","<b>Authors:</b><br/>Ochoa I., Crespo P., Ser J.D., Hernaez M. <br/><b>Key words:</b><br/>Bandwidth-limited regime, Non-uniform memoryless sources, Sigma-mapping, Turbo codes"
464,paper_466,"Rebholz-Schuhmann D., Jimeno-Yepes A., Arregui M., Kirsch H.",,"Motivation: The identification of events such as protein-protein interactions (PPIs) from the scientific literature is a complex task. One of the reasons is that there is no formal syntax to denote such relations in the scientific literature. Nonetheless, it is important to understand such relational event representations to improve information extraction solutions (e.g., for gene regulatory events). In this study, we analyze publicly available protein interaction corpora (AIMed, BioInfer, BioCreAtIve II) to determine the scope of verbs used to denote protein interactions and to measure their predictive capacity for the identification of PPI events. Our analysis is based on syntactical language patterns. This restriction has the advantage that the verb mention is used as the independent variable in the experiments enabling comparability of results in the usage of the verbs. The initial selection of verbs has been generated from a systematic analysis of the scientific literature and existing corpora for PPIs. We distinguish modifying interactions (MIs) such as posttranslational modifications (PTMs) from non-modifying interactions (NMIs) and assumed that MIs have a higher predictive capacity due to stronger scientific evidence proving the interaction. We found that MIs are less frequent in the corpus but can be extracted at the same precision levels as PPIs. A significant portion of correct PPI reportings in the BioCreAtIve II corpus use the verb ""associate"", which semantically does not prove a relation. The performance of every monitored verb is listed and allows the selection of specific verbs to improve the performance of PPI extraction solutions. Programmatic access to the text processing modules is available online (www.ebi.ac.uk/webservices/whatizit/info.jsf) and the full analysis of Medline abstracts will be made through the Web pages of the Rebholz group. © 2009 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Rebholz-Schuhmann D., Jimeno-Yepes A., Arregui M., Kirsch H. <br/><b>Key words:</b><br/>Information extraction, Protein-protein interaction, Semantic relations, Text mining"
465,paper_467,"Svahnberg M., Gorschek T., Feldt R., Torkar R., Saleem S.B., Shafique M.U.",,"Context: Strategic release planning (sometimes referred to as road-mapping) is an important phase of the requirements engineering process performed at product level. It is concerned with selection and assignment of requirements in sequences of releases such that important technical and resource constraints are fulfilled. Objectives: In this study we investigate which strategic release planning models have been proposed, their degree of empirical validation, their factors for requirements selection, and whether they are intended for a bespoke or market-driven requirements engineering context. Methods: In this systematic review a number of article sources are used, including Compendex, Inspec, IEEE Xplore, ACM Digital Library, and Springer Link. Studies are selected after reading titles and abstracts to decide whether the articles are peer reviewed, and relevant to the subject. Results: Twenty four strategic release planning models are found and mapped in relation to each other, and a taxonomy of requirements selection factors is constructed. Conclusions: We conclude that many models are related to each other and use similar techniques to address the release planning problem. We also conclude that several requirement selection factors are covered in the different models, but that many methods fail to address factors such as stakeholder value or internal value. Moreover, we conclude that there is a need for further empirical validation of the models in full scale industry trials. © 2009 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Svahnberg M., Gorschek T., Feldt R., Torkar R., Saleem S.B., Shafique M.U. <br/><b>Key words:</b><br/>Requirements selection factors, Road-mapping, Strategic release planning models, Systematic review"
466,paper_468,"Eva H., Carboni S., Achard F., Stach N., Durieux L., Faure J.-F., Mollicone D.",,"A global systematic sampling scheme has been developed by the UN FAO and the EC TREES project to estimate rates of deforestation at global or continental levels at intervals of 5 to 10 years. This global scheme can be intensified to produce results at the national level. In this paper, using surrogate observations, we compare the deforestation estimates derived from these two levels of sampling intensities (one, the global, for the Brazilian Amazon the other, national, for French Guiana) to estimates derived from the official inventories. We also report the precisions that are achieved due to sampling errors and, in the case of French Guiana, compare such precision with the official inventory precision. We extract nine sample data sets from the official wall-to-wall deforestation map derived from satellite interpretations produced for the Brazilian Amazon for the year 2002 to 2003. This global sampling scheme estimate gives 2.81 million ha of deforestation (mean from nine simulated replicates) with a standard error of 0.10 million ha. This compares with the full population estimate from the wall-to-wall interpretations of 2.73 million ha deforested, which is within one standard error of our sampling test estimate. The relative difference between the mean estimate from sampling approach and the full population estimate is 3.1%, and the standard error represents 4.0% of the full population estimate. This global sampling is then intensified to a territorial level with a case study over French Guiana to estimate deforestation between the years 1990 and 2006. For the historical reference period, 1990, Landsat-5 Thematic Mapper data were used. A coverage of SPOT-HRV imagery at 20 m × 20 m resolution acquired at the Cayenne receiving station in French Guiana was used for year 2006. Our estimates from the intensified global sampling scheme over French Guiana are compared with those produced by the national authority to report on deforestation rates under the Kyoto protocol rules for its overseas department. The latter estimates come from a sample of nearly 17,000 plots analyzed from same spatial imagery acquired between year 1990 and year 2006. This sampling scheme is derived from the traditional forest inventory methods carried out by IFN (Inventaire Forestier National). Our intensified global sampling scheme leads to an estimate of 96,650 ha deforested between 1990 and 2006, which is within the 95% confidence interval of the IFN sampling scheme, which gives an estimate of 91,722 ha, representing a relative difference from the IFN of 5.4%. These results demonstrate that the intensification of the global sampling scheme can provide forest area change estimates close to those achieved by official forest inventories (<6%), with precisions of between 4% and 7%, although we only estimate errors from sampling, not from the use of surrogate data. Such methods could be used by developing countries to demonstrate that they are fulfilling requirements for reducing emissions from deforestation in the framework of an REDD (Reducing Emissions from Deforestation in Developing Countries) mechanism under discussion within the United Nations Framework Convention on Climate Change (UNFCCC). Monitoring systems at national levels in tropical countries can also benefit from pan-tropical and regional observations, to ensure consistency between different national monitoring systems. © 2009 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","<b>Authors:</b><br/>Eva H., Carboni S., Achard F., Stach N., Durieux L., Faure J.-F., Mollicone D. <br/><b>Key words:</b><br/>Change detection, Forestry, Landsat, Sampling, SPOT"
467,paper_469,"Wessel P.",,"I present a new set of tools for detection of intersections among tracks in 2-D Cartesian or geographic coordinates. These tools allow for evaluation of crossover errors at intersections, analysis of such crossover errors to determine appropriate linear models of systematic corrections for each track, and application of these corrections and further adjustments to data that completely eliminates crossover discrepancies from final 2-D data compilations. Unlike my older x_system tools, the new x2sys tools implement modern algorithms for detecting track intersections and are capable of reading a wide range of data file formats, including data files following the netCDF COARDS convention. The x2sys package contains several programs that address the various tasks needed to undertake a comprehensive crossover analysis and is distributed as a supplement to the Generic Mapping Tools, making them available for all computer platforms and architectures. © 2009 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Wessel P. <br/><b>Key words:</b><br/>Crossover errors, Data quality improvement, Leveling, Track intersection"
468,paper_470,"Geelen B., Ferentinos V., Catthoor F., Lafruit G., Verkest D., Lauwereins R., Stouraitis T.",,"Future dynamic applications will require new mapping strategies to deliver power-efficient performance. Fully static design-time mappings will not be able to optimally address the unpredictably varying application characteristics and system resource requirements. Instead, the platforms will not only need to be programmable in terms of instruction set processors, but also at least partial reconfigurability will be required, while the applications themselves will need to exploit this increased freedom at runtime to adapt to the dynamism. In this context, it is important for applications to optimally exploit the memory hierarchy under varying memory availability. This article presents an analysis of spatial locality trade-offs in wavelet-based applications, to be used in dynamic execution environments: Depending on the encountered runtime conditions, the execution switches to different memory optimized instantiations or localizations, optimally exploiting temporal and spatial locality under these conditions. This is enabled by systematic mapping guidelines, indicating how the miss-rate behavior of a localization is influenced by a specific execution condition, under which conditions a certain localization is optimal and which miss-rate gains may be obtained by switching to that localization. © 2010 ACM.","<b>Authors:</b><br/>Geelen B., Ferentinos V., Catthoor F., Lafruit G., Verkest D., Lauwereins R., Stouraitis T. <br/><b>Key words:</b><br/>Dynamism, Loop transformations, Wavelet transform"
469,paper_471,"Underwood J.P., Hill A., Peynot T., Scheding S.J.",,"Reliable robotic perception and planning are critical to performing autonomous actions in uncertain, unstructured environments. In field robotic systems, automation is achieved by interpreting exteroceptive sensor information to infer something about the world. This is then mapped to provide a consistent spatial context, so that actions can be planned around the predicted future interaction of the robot and the world. The whole system is as reliable as the weakest link in this chain. In this paper, the term mapping is used broadly to describe the transformation of range-based exteroceptive sensor data (such as LIDAR or stereo vision) to a fixed navigation frame, so that it can be used to form an internal representation of the environment. The coordinate transformation from the sensor frame to the navigation frame is analyzed to produce a spatial error model that captures the dominant geometric and temporal sources of mapping error. This allows the mapping accuracy to be calculated at run time. A generic extrinsic calibration method for exteroceptive range-based sensors is then presented to determine the sensor location and orientation. This allows systematic errors in individual sensors to be minimized, and when multiple sensors are used, it minimizes the systematic contradiction between them to enable reliable multisensor data fusion. The mathematical derivations at the core of this model are not particularly novel or complicated, but the rigorous analysis and application to field robotics seems to be largely absent from the literature to date. The techniques in this paper are simple to implement, and they offer a significant improvement to the accuracy, precision, and integrity of mapped information. Consequently, they should be employed whenever maps are formed fromrange-based exteroceptive sensor data. © 2009 Wiley Periodicals, Inc.","<b>Authors:</b><br/>Underwood J.P., Hill A., Peynot T., Scheding S.J. <br/><b>Key words:</b><br/>"
470,paper_472,"Chen C., Zhuang Y., Xiao J.",,"Inferring 3D human poses from marker-free images is an important but challenging task. A large body of algorithms has been proposed to that end, among which the discriminative methods using silhouettes as visual inputs are an important category. For these methods, silhouette representation and matching is very important. An effective silhouette representation method computes discriminative and compact silhouette descriptors which are used for learning the silhouette-pose mapping, and a good silhouette matching algorithm enables effective comparison and search in the example database. However, there has not been an extensive study on the abundance of shape analysis techniques in the context of pose discrimination. In this paper, we give a systematic study on the performances of shape representation and matching algorithms for pose discrimination, and we explore the influences of different realistic factors encountered in practical systems, such as yaw angle, camera tilt, silhouette noise, and the selection of training examples. We conduct various quantitative evaluations using synthetic and real silhouettes based on HumanEva dataset. Our work provides new insights into pose inferring algorithms and the designing and building of practical systems. © 2009 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Chen C., Zhuang Y., Xiao J. <br/><b>Key words:</b><br/>Comparative study, Motion recovery, Performance evaluation, Pose inferring, Shape analysis"
471,paper_473,"Ko S., Lee H.",,"Background: Protein function prediction has been one of the most important issues in functional genomics. With the current availability of various genomic data sets, many researchers have attempted to develop integration models that combine all available genomic data for protein function prediction. These efforts have resulted in the improvement of prediction quality and the extension of prediction coverage. However, it has also been observed that integrating more data sources does not always increase the prediction quality. Therefore, selecting data sources that highly contribute to the protein function prediction has become an important issue.Results: We present systematic feature selection methods that assess the contribution of genome-wide data sets to predict protein functions and then investigate the relationship between genomic data sources and protein functions. In this study, we use ten different genomic data sources in Mus musculus, including: protein-domains, protein-protein interactions, gene expressions, phenotype ontology, phylogenetic profiles and disease data sources to predict protein functions that are labelled with Gene Ontology (GO) terms. We then apply two approaches to feature selection: exhaustive search feature selection using a kernel based logistic regression (KLR), and a kernel based L1-norm regularized logistic regression (KL1LR). In the first approach, we exhaustively measure the contribution of each data set for each function based on its prediction quality. In the second approach, we use the estimated coefficients of features as measures of contribution of data sources. Our results show that the proposed methods improve the prediction quality compared to the full integration of all data sources and other filter-based feature selection methods. We also show that contributing data sources can differ depending on the protein function. Furthermore, we observe that highly contributing data sets can be similar among a group of protein functions that have the same parent in the GO hierarchy.Conclusions: In contrast to previous integration methods, our approaches not only increase the prediction quality but also gather information about highly contributing data sources for each protein function. This information can help researchers collect relevant data sources for annotating protein functions. © 2009 Ko and Lee, licensee BioMed Central Ltd.","<b>Authors:</b><br/>Ko S., Lee H. <br/><b>Key words:</b><br/>"
472,paper_474,"Condori-Fernandez N., Daneva M., Sikkel K., Wieringa R., Dieste O., Pastor O.",,"This paper describes an empirical mapping study, which was designed to identify what aspects of Software Requirement Specifications (SRS) are empirically evaluated, in which context, and by using which research method. On the basis of 46 identified and categorized primary studies, we found that understandability is the most commonly evaluated aspect of SRS, experiments are the most commonly used research method, and the academic environment is where most empirical evaluation takes place. © 2009 IEEE.","<b>Authors:</b><br/>Condori-Fernandez N., Daneva M., Sikkel K., Wieringa R., Dieste O., Pastor O. <br/><b>Key words:</b><br/>"
473,paper_475,"Li Y., Wu L., Yang Y., Xia R., Wang Y., Jin X.",,"Systematic mapping and monitoring of wetland landscape are of fundamental importance for wetland development and management. To accurately classify wetland in Yancheng coastal wetland, ground investigation was conducted in 2006. Integrated with ground investigation, the wetland was classified into 8 categories such as Spartina alterniflora Loisel, Farm land, Phragmites Australis, Artemisia halodendron Turcz, Bare beach, Salt field, Fish & shrimp pond, and Sea water. A total of three decision trees were successfully produced. The first represented broad divisions of vegetation (in fact, at this stage, it just can be called vegetated cover like) and non-vegetation, and the second two represented more detailed vegetation classes and non-vegetation classes. To construct the decision trees, NDVI and principal component analysis were used as the evaluation factors. The thresholds were built combining with ground investigation and spectral property. Firstly, almost all kinds of vegetable were divided out of non-vegetation by NDVI. Secondly, the different species of vegetation were distinguished and some vegetated cover like was eliminated out of vegetation. Phragmites Australis belt, Artemisia halodendron Turcz belt, Spartina alterniflora Loisel belt and bare beach belt were distributed regularly from land to sea. © 2009 Copyright SPIE - The International Society for Optical Engineering.","<b>Authors:</b><br/>Li Y., Wu L., Yang Y., Xia R., Wang Y., Jin X. <br/><b>Key words:</b><br/>ALOS image, Classification, Coastal wetland, Decision tree, Principal component analysis, Vegetation"
474,paper_476,"Rau J.-Y., Chen L.-C., Hsieh C.-C., Wen J.-Y., Huang D.-M.",,"Mobile mapping system (MMS) is widely used for fast and large quantify of geospatial data acquisition. The integration of GPS/INS with digital camera or laser scanning for direct geo-referencing is a pre-requisite for MMS. Comparing with conventional in-direct aerial triangulation method, direct geo-referencing technique is more efficient and economic in the derivation of camera's exterior orientation parameters. In this research, we apply the Applanix POS AV 510, which is designed for airborne vehicle MMS, in a land vehicle MMS for the acquisition of facade texture information. The system includes two DSLR digital cameras with 30 degrees tilted-up angle and was installed as looking forward and backward, respectively. The image acquisition was controlled and saved by the notebooks and their trigger events were stored in the POS's memory card concurrently. The error budgets for the direct geo-referencing were evaluated including the surveying of calibration field, lens distortion correction, and boresight/lever system calibration. Preliminary experimental results show that the calibration field is the most critical error source in direct georeferencing. However, due to the calibration field has wide open sky for GPS signal reception and a static data collection scheme is proposed during calibration, the designed system can achieve less than 6 cm of 3D positioning error when single camera is used and a 3cm of accuracy could be achieved when dual camera convergent imaging is adopted. The results are promising for facade texture generation and topographic mapping. Copyright © (2009) by the Asian Association on Remote Sensing.","<b>Authors:</b><br/>Rau J.-Y., Chen L.-C., Hsieh C.-C., Wen J.-Y., Huang D.-M. <br/><b>Key words:</b><br/>Direct sensor orientation, Mobile mapping system, Systematic calibration"
475,paper_477,"Okoli C., Schabram K.",,"Context: Wikipedia has become one of the ten-most visited sites on the Web, and the world's leading source of Web reference information. Its rapid success has attracted over 1,000 scholarly studies that treat Wikipedia as a major topic or data source. Objectives: This article presents a protocol for conducting a systematic mapping (a broad-based literature review) of research on Wikipedia. It identifies what research has been conducted, what research questions have been asked, which have been answered, and what theories and methodologies have been employed to study Wikipedia. Methods: This protocol follows the rigorous methodology of evidence-based software engineering to conduct a systematic mapping study. Results and conclusions: This protocol reports a study in progress. Copyright 2009 ACM.","<b>Authors:</b><br/>Okoli C., Schabram K. <br/><b>Key words:</b><br/>Literature review, Open content, Open source, Wikipedia"
476,paper_479,"Islam S., Suri N., Balogh A., Csertán G., Pataricza A.",,"Moving from the traditional federated design paradigm, integration of mixed-criticality software components onto common computing platforms is increasingly being adopted by automotive, avionics and the control industry. This method faces new challenges such as the integration of varied functionalities (dependability, responsiveness, power consumption, etc.) under platform resource constraints and the prevention of error propagation. Based on model driven architecture and platform based design's principles, we present a systematic mapping process for such integration adhering a transformation based design methodology. Our aim is to convert/transform initial platform independent application specifications into post integration platform specific models. In this paper, a heuristic based resource allocation approach is depicted for the consolidated mapping of safety critical and non-safety critical applications onto a common computing platform meeting particularly dependability/fault- tolerance and real-time requirements. We develop a supporting tool suite for the proposed framework, where VIATRA (VIsual Automated model TRAnsformations) is used as a transformation tool at different design steps. We validate the process and provide experimental results to show the effectiveness, performance and robustness of the approach. © Springer Science+Business Media, LLC 2009.","<b>Authors:</b><br/>Islam S., Suri N., Balogh A., Csertán G., Pataricza A. <br/><b>Key words:</b><br/>Constraints, Fault-tolerance, Mapping, Real-time, Transformation"
477,paper_480,"Boni G., Candela L., Castelli F., Dellepiane S., Palandri M., Persi D., Pierdicca N., Rudari R., Serpico S., Siccardi F., Versace C.",,"This paper illustrates some applications of COSMO-SkyMed (CSK) observations for rapid mapping of flooded areas and damages in small to medium size catchments. The results presented here have been obtained within the framework of the project ""OPERA - Civil protection from floods"" funded by the Italian Space Agency and run by a team of scientific research centres and private companies. The project aims to the systematic evaluation of the added value of the use of Earth Observation techniques into operational flood prediction chains. Due to the specific geomorphology of Italy, the focus is mainly on flash floods on small sized river catchments. Monitoring and modelling processes at proper space-time scales in this environment raise several issues to be solved, compared to applications in larger river basins. Here we address some related to the suitable use of CSK imagery. ©2009 IEEE.","<b>Authors:</b><br/>Boni G., Candela L., Castelli F., Dellepiane S., Palandri M., Persi D., Pierdicca N., Rudari R., Serpico S., Siccardi F., Versace C. <br/><b>Key words:</b><br/>Satellite applications, Synthetic aperture radar, Water"
478,paper_481,"Cai J.-Y., Yegneswaran V., Alfeld C., Barford P.",,"A honeynet is a portion of routed but otherwise unused address space that is instrumented for network traffic monitoring. It is an invaluable tool for understanding unwanted Internet traffic and malicious attacks. We formalize the problem of defending honeynets from systematic mapping (a serious threat to their viability) as a simple two-person game. The objective of the Attacker is to identify a honeynet with a minimum number of probes. The objective of the Defender is to maintain a honeynet for as long as possible before moving it to a new location within a larger address space. Using this game theoretic framework, we describe and prove optimal or near-optimal strategies for both Attacker and Defender. This is the first mathematically rigorous study of this increasingly important problem on honeynet defense. Our theoretical ideas provide the first formalism of the honeynet monitoring problem, illustrate the viability of network address shuffling, and inform the design of next generation honeynet defense. © 2009 Springer Berlin Heidelberg.","<b>Authors:</b><br/>Cai J.-Y., Yegneswaran V., Alfeld C., Barford P. <br/><b>Key words:</b><br/>"
479,paper_482,"Carbone M., Cui W., Lu L., Lee W., Peinado M., Jiang X.",,"Dynamic kernel data have become an attractive target for kernel-mode malware. However, previous solutions for checking kernel integrity either limit themselves to code and static data or can only inspect a fraction of dynamic data, resulting in limited protection. Our study shows that previous solutions may reach only 28% of the dynamic kernel data and thus may fail to identify function pointers manipulated by many kernel-mode malware. To enable systematic kernel integrity checking, in this paper we present KOP, a system that can map dynamic kernel data with nearly complete coverage and nearly perfect accuracy. Unlike previous approaches, which ignore generic pointers, unions and dynamic arrays when locating dynamic kernel objects, KOP (1) applies inter-procedural points-to analysis to compute all possible types for generic pointers (e.g., void*), (2) uses a pattern matching algorithm to resolve type ambiguities (e.g., unions), and (3) recognizes dynamic arrays by leveraging knowledge of kernel memory pool boundaries. We implemented a prototype of KOP and evaluated it on a Windows Vista SP1 system loaded with 63 kernel drivers. KOP was able to accurately map 99% of all the dynamic kernel data. To demonstrate KOP's power, we developed two tools based on it to systematically identify malicious function pointers and uncover hidden kernel objects. Our tools correctly identified all malicious function pointers and all hidden objects from nine real-world kernel-mode malware samples as well as one created by ourselves, with no false alarms. Copyright 2009 ACM.","<b>Authors:</b><br/>Carbone M., Cui W., Lu L., Lee W., Peinado M., Jiang X. <br/><b>Key words:</b><br/>Introspection, Kernel integrity, Malware, Memory analysis, Pointer analysis"
480,paper_483,"Bray J.D., Gaab K.M., Lambert B.M., Lomheim T.S.",,"An improved and optimized spectral spot-scanning system for visible focal plane array (FPA) sub-micron pixel photoresponse testing is presented. This updated configuration includes: (1) additional diagnostic analysis tools which more completely characterize the operation of the system, (2) a confocal microscope fitted into the optical system to aid in more precise determination of spot focusing on the imager, (3) a post-acquisition transformation to imager pixel response data to reduce overall data acquisition time. Wavelength-dependent pixel response data is presented to demonstrate the repeatability of this setup as well as to quantify the impact of random and systematic experimental errors. © 2009 SPIE.","<b>Authors:</b><br/>Bray J.D., Gaab K.M., Lambert B.M., Lomheim T.S. <br/><b>Key words:</b><br/>Cmos imager, Confocal microscopy, Modulation transfer function, Non-uniform sampling effects, Precision spectral spot scanning, Responsivity mapping, Small pixels"
481,paper_484,"Garlandini S., Fabrikant S.I.",,"We propose an empirical, perception-based evaluation approach for assessing the effectiveness and efficiency of longstanding cartographic design principles applied to 2D map displays. The approach includes bottom-up visual saliency models that are compared with eye-movement data collected in human-subject experiments on map stimuli embedded in the so-called flicker paradigm. The proposed methods are applied to the assessment of four commonly used visual variables for designing 2D maps: size, color value, color hue, and orientation. The empirical results suggest that the visual variable size is the most efficient (fastest) and most effective (accurate) visual variable to detect change under flicker conditions. The visual variable orientation proved to be the least efficient and effective of the tested visual variables. These empirical results shed new light on the implied ranking of the visual variables that have been proposed over 40 years ago. With the presented approach we hope to provide cartographers, GIScientists and visualization designers a systematic assessment method to develop effective and efficient geovisualization displays. © 2009 Springer Berlin Heidelberg.","<b>Authors:</b><br/>Garlandini S., Fabrikant S.I. <br/><b>Key words:</b><br/>Change blindness, Empirical studies, Eye movements, Geographic visualization, Visual variables"
482,paper_485,"Simonnet M., Jacobson D., Vieilledent S., Tisseau J.",,"Navigating consists of coordinating egocentric and allocentric spatial frames of reference. Virtual environments have afforded researchers in the spatial community with tools to investigate the learning of space. The issue of the transfer between virtual and real situations is not trivial. A central question is the role of frames of reference in mediating spatial knowledge transfer to external surroundings, as is the effect of different sensory modalities accessed in simulated and real worlds. This challenges the capacity of blind people to use virtual reality to explore a scene without graphics. The present experiment involves a haptic and auditory maritime virtual environment. In triangulation tasks, we measure systematic errors and preliminary results show an ability to learn configurational knowledge and to navigate through it without vision. Subjects appeared to take advantage of getting lost in an egocentric ""haptic"" view in the virtual environment to improve performances in the real environment. © 2009 Springer Berlin Heidelberg.","<b>Authors:</b><br/>Simonnet M., Jacobson D., Vieilledent S., Tisseau J. <br/><b>Key words:</b><br/>Blind, Haptic, Navigation, Sailing, Spatial frames of reference, Virtual reality"
483,paper_486,"Maeda S., Ohno K., Morokuma K.",,"The global reaction route mapping (GRRM) method enabled an automated and a systematic search for routes of chemical reactions on a potential energy surface based on the anharmonic downward distortion following (ADDF) approach [Chem. Phys. Lett. 2004, 384, 277]. On the other hand, the microiteration technique [Mol. Phys. 2006, 104, 701] has been developed for full optimizations of transition state (TS) structures for reactions/transformations in large flexible molecular systems and successfully used in ONIOM(QM:MM) calculations. In the present paper, combining the GRRM method with the microiteration technique, we developed a microiteration-ADDF (?-ADDF) method for automated and systematic TS exploration of large flexible molecular systems. We showed that the method works well with two test systems, (H2CO)(H2O)100 and Si6(C12H17)6, in the ONIOM(QM:MM) framework. It is noted that the present ?-ADDF method can be used for pure quantum mechanics (QM) or molecular mechanics (MM) systems (without ONIOM) and has been tested successfully in C6H10O pure QM calculations. © 2009 American Chemical Society.","<b>Authors:</b><br/>Maeda S., Ohno K., Morokuma K. <br/><b>Key words:</b><br/>"
484,paper_487,"Resta M.",,"We describe a method to develop trading rules based on the responses of self-organizing maps (SOMs), trained under various distance metrics. The effectiveness of the procedure is examined on 5 min data of S&P MIB financial index, and its performances compared with those of classical buy and hold trading technique. The noticeable innovations of the paper include the methodology itself, which brings SOMs into a decision making tool to operate into the market, the focus on intraday tradings bars, the systematic study of how changes in the distance metrics employed to train the maps may affect the overall performances, and, finally, the discussion of system performances both in the absence and in the presence of commission fees. At the current stage the results, evaluated with both financial and statistical indicators, bring us to the following conclusions: (a) self-organizing maps can be helpful to localize profitable intraday patterns, achieving more stable performances than common trading rules, (b) working with proper metrics may enhance the overall performances, (c) trading strategies based on unsupervised neural networks make exploitable with profits almost continuous trades, until commission fees maintain below suitable thresholds. © 2009 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Resta M. <br/><b>Key words:</b><br/>Commission fees, Intraday tradings, Self-organizing maps, Similarity measures"
485,paper_488,"Sun N., Mei X., Zhang Y.",,"Faithfully obtaining design specifications from customer requirements is essential for successful designs. The natural lingual, inexact, incomplete, and vague attributes of customer requirements make it very difficult to map customer requirements to design specifications. In general design process, the design specifications are determined by designers based on their experience and intuition, and often a certain target value is set for a specification. However, it is on one hand very difficult, on the other hand unreasonable, so a suitable limit range rather than a certain value is preferred at the beginning of design, especially at the concept design process. In this paper, a simplified systematic approach of transforming customer requirements to design specifications is proposed. First, a two-stepped clustering approach for grouping customer requirements and design specifications based on the house of quality matrix is presented, by which the mapping is limited to within each group. To further simplify the inference mapping rules of customer requirements and design specifications, the minimal condition inference mapping rules for each design specification are extracted based on rough set theory. In the end, a suitable value range is determined for a specification by applying the fuzzy rule matrix. © 2009 by ASME.","<b>Authors:</b><br/>Sun N., Mei X., Zhang Y. <br/><b>Key words:</b><br/>Customer requirement, Design specification, Fuzzy rule matrix, Lower approximation"
486,paper_489,"Steffen J., Klanke S., Vijayakumar S., Ritter H.",,"We explore generic mechanisms to introduce structural hints into the method of Unsupervised Kernel Regression (UKR) in order to learn representations of data sequences in a semi-supervised way. These new extensions are targeted at representing a dextrous manipulation task. We thus evaluate the effectiveness of the proposed mechanisms on appropriate toy data that mimic the characteristics of the aimed manipulation task and thereby provide means for a systematic evaluation. © 2009 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Steffen J., Klanke S., Vijayakumar S., Ritter H. <br/><b>Key words:</b><br/>"
487,paper_490,"Kohonen T., Nieminen I.T., Honkela T.",,"The self-organizing map (SOM) is related to the classical vector quantization (VQ). Like in the VQ, the SOM represents a distribution of input data vectors using a finite set of models. In both methods, the quantization error (QE) of an input vector can be expressed, e.g., as the Euclidean norm of the difference of the input vector and the best-matching model. Since the models are usually optimized in the VQ so that the sum of the squared QEs is minimized for the given set of training vectors, a common notion is that it will be impossible to find models that produce a smaller rms QE. Therefore it has come as a surprise that in some cases the rms QE of a SOM can be smaller than that of a VQ with the same number of models and the same input data. This effect may manifest itself if the number of training vectors per model is on the order of small integers and the testing is made with an independent set of test vectors. An explanation seems to ensue from statistics. Each model vector in the VQ is determined as the average of those training vectors that are mapped into the same Voronoi domain as the model vector. On the contrary, each model vector of the SOM is determined as a weighted average of all of those training vectors that are mapped into the ""topological"" neighborhood around the corresponding model. The number of training vectors mapped into the neighborhood of a SOM model is generally much larger than that mapped into a Voronoi domain around a model in the VQ. Since the SOM model vectors are then determined with a significantly higher statistical accuracy, the Voronoi domains of the SOM are significantly more regular, and the resulting rms QE may then be smaller than in the VQ. However, the effective dimensionality of the vectors must also be sufficiently high. © 2009 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Kohonen T., Nieminen I.T., Honkela T. <br/><b>Key words:</b><br/>Quantization error, Self-organizing map, Vector quantization"
488,paper_491,"Corral G., Armengol E., Fornells A., Golobardes E.",,"Network security tests should be periodically conducted to detect vulnerabilities before they are exploited. However, analysis of testing results is resource intensive with many data and requires expertise because it is an unsupervised domain. This paper presents how to automate and improve this analysis through the identification and explanation of device groups with similar vulnerabilities. Clustering is used for discovering hidden patterns and abnormal behaviors. Self-organizing maps are preferred due to their soft computing capabilities. Explanations based on anti-unification give comprehensive descriptions of clustering results to analysts. This approach is integrated in Consensus, a computer-aided system to detect network vulnerabilities. © 2009 Elsevier B.V.","<b>Authors:</b><br/>Corral G., Armengol E., Fornells A., Golobardes E. <br/><b>Key words:</b><br/>Artificial intelligence applications, Explanations, Network security, Self-organizing maps, Unsupervised learning clustering"
489,paper_492,"Chen Y.-H., Truong T.-K., Huang C.-H., Chien C.-H.",,"A new decoding algorithm for the binary systematic (47, 24, 11) quadratic residue (QR) code, a code that allows error-correction of up to five errors, is presented in this paper. The key idea behind this decoding technique is based on the existence of a one-to-one mapping between the syndromes ""S1"" and correctable error patterns. By looking up a pre-calculated table, this algorithm determines the locations of errors directly, thus requires no multiplication operations over a finite field. Moreover, the algorithm dramatically reduces the memory required by approximately 89%. A full search confirms that when five or less errors occur, this algorithm decodes these errors perfectly. Since the implementation is written in the C-language, it is readily adaptable for use in Digital Signal Processing (DSP) applications. © 2009 Elsevier Inc. All rights reserved.","<b>Authors:</b><br/>Chen Y.-H., Truong T.-K., Huang C.-H., Chien C.-H. <br/><b>Key words:</b><br/>Digital Signal Processing, Error pattern, Finite field, Quadratic residue, Syndrome"
490,paper_493,"Habib A.F., Kersting A.P., Bang K.-I., Zhai R., Al-d Urgham M.",,"Lidar (laser scanning) technology has been proven as a prominent technique for the acquisition of high-density and accurate topographic information. Because of systematic errors in the lidar measurements (drifts in the position and orientation information and biases in the mirror angles and ranges) and/or in the parameters relating the system components (mounting parameters), adjacent lidar strips may exhibit discrepancies. Although position and orientation drifts can have a more significant impact, these errors and their impact do not come as a surprise if the quality of the GPS/INS integration process is carefully examined. Therefore, the mounting errors are singled out in this work. The ideal solution for improving the compatibility of neighbouring strips in the presence of errors in the mounting parameters is the implementation of a rigorous calibration procedure. However, such a calibration requires the original observations, which may not be usually available. In this paper, a strip adjustment procedure to improve the compatibility between parallel lidar strips with moderate flight dynamics (for example, acquired by a fixed-wing aircraft) over an area with moderately varying elevation is proposed. The proposed method is similar to the photogrammetric block adjustment of independent models. Instead of point features, planar patches and linear features, which are represented by sets of non-conjugate points, are used for the strip adjustment. The feasibility and the performance of the proposed procedure together with its impact on subsequent activities are illustrated using experimental results from real data. © Journal Compilation © 2009 Remote Sensing and Photogrammetry Society and Blackwell Publishing Ltd.","<b>Authors:</b><br/>Habib A.F., Kersting A.P., Bang K.-I., Zhai R., Al-d Urgham M. <br/><b>Key words:</b><br/>Lidar, Linear features, Mounting errors, Planar patches, Strip adjustment"
491,paper_494,"Bodenreider O., Peters L.B.",,"Objectives: RxNorm is a standardized nomenclature for clinical drug entities developed by the National Library of Medicine. In this paper, we audit relations in RxNorm for consistency and completeness through the systematic analysis of the graph of its concepts and relationships. Methods: The representation of multi-ingredient drugs is normalized in order to make it compatible with that of single-ingredient drugs. All meaningful paths between two nodes in the type graph are computed and instantiated. Alternate paths are automatically compared and manually inspected in case of inconsistency. Results: The 115 meaningful paths identified in the type graph can be grouped into 28 groups with respect to start and end nodes. Of the 19 groups of alternate paths (i.e., with two or more paths) between the start and end nodes, 9 (47%) exhibit inconsistencies. Overall, 28 (24%) of the 115 paths are inconsistent with other alternate paths. A total of 348 inconsistencies were identified in the April 2008 version of RxNorm and reported to the RxNorm team, of which 215 (62%) had been corrected in the January 2009 version of RxNorm. Conclusion: The inconsistencies identified involve missing nodes (93), missing links (17), extraneous links (237) and one case of mix-up between two ingredients. Our auditing method proved effective in identifying a limited number of errors that had defeated the quality assurance mechanisms currently in place in the RxNorm production system. Some recommendations for the development of RxNorm are provided.","<b>Authors:</b><br/>Bodenreider O., Peters L.B. <br/><b>Key words:</b><br/>Auditing methods, Biomedical terminologies, Graphs, Quality assurance, RxNorm"
492,paper_495,"Luo F., Li B., Wan X.-F., Scheuermann R.H.",,"Background: Characterizing the structural properties of protein interaction networks will help illuminate the organizational and functional relationships among elements in biological systems. Results: In this paper, we present a systematic exploration of the core/ periphery structures in protein interaction networks (PINs). First, the concepts of cores and peripheries in PINs are defined. Then, computational methods are proposed to identify two types of cores, k-plex cores and star cores, from PINs. Application of these methods to a yeast protein interaction network has identified 110 k-plex cores and 109 star cores. We find that the k-plex cores consist of either ""party"" proteins, ""date"" proteins, or both. We also reveal that there are two classes of 1-peripheral proteins: ""party"" peripheries, which are more likely to be part of protein complex, and ""connector"" peripheries, which are more likely connected to different proteins or protein complexes. Our results also show that, besides connectivity, other variations in structural properties are related to the variation in biological properties. Furthermore, the negative correlation between evolutionary rate and connectivity are shown toysis. Moreover, the core/periphery structures help to reveal the existence of multiple levels of protein expression dynamics. Conclusion: Our results show that both the structure and connectivity can be used to characterize topological properties in protein interaction networks, illuminating the functional organization of cellular systems. © 2009 Luo et al, licensee BioMed Central Ltd.","<b>Authors:</b><br/>Luo F., Li B., Wan X.-F., Scheuermann R.H. <br/><b>Key words:</b><br/>"
493,paper_496,"Duan C., Cordero Calle V.H., Khatri S.P.",,"Interconnect delay has become a limiting factor for circuit performance in deep sub-micrometer designs. As the crosstalk in an on-chip bus is highly dependent on the data patterns transmitted on the bus, different crosstalk avoidance coding schemes have been proposed to boost the bus speed and/or reduce the overall energy consumption. Despite the availability of the codes, no systematic mapping of datawords to codewords has been proposed for CODEC design. This is mainly due to the nonlinear nature of the crosstalk avoidance codes (CAC). The lack of practical CODEC construction schemes has hampered the use of such codes in practical designs. This work presents guidelines for the CODEC design of the forbidden pattern free crosstalk avoidance code (FPF-CAC). We analyze the properties of the FPF-CAC and show that mathematically, a mapping scheme exists based on the representation of numbers in the Fibonacci numeral system. Our first proposed CODEC design offers a near-optimal area overhead performance. An improved version of the CODEC is then presented, which achieves theoretical optimal performance. We also investigate the implementation details of the CODECs, including design complexity and the speed. Optimization schemes are provided to reduce the size of the CODEC and improve its speed. © 2009 IEEE.","<b>Authors:</b><br/>Duan C., Cordero Calle V.H., Khatri S.P. <br/><b>Key words:</b><br/>CODEC, Crosstalk, Fibonacci number, On-chip bus"
494,paper_497,"Chen D., Wei H.",,"Four binary thematic maps with combinations of two spatial autocorrelation levels and two different class proportions are simulated to study their effect on the precision of accuracy measures from different sampling designs. A series of eleven sample sizes (from a minimum of 25 to a maximum of 1296) are simulated using three popular sampling designs, including simple random sampling (SRS), systematic sampling (SYS), and stratified random sampling (StrRS) on the four simulated maps. The conventional error matrix and related accuracy measures are calculated for each simulation, and the precision of different estimates of accuracy measures is compared among the three sampling designs. The selection of a particular sampling design and sample size depends on the spatial autocorrelation level, the class proportion difference, and the accuracy indices that a given application requires. In general, the class proportion difference has a greater impact on the performance of different sampling methods than the spatial autocorrelation level does on a map. For estimating the accuracy of individual classes, stratified sampling achieves better precision than SRS and SYS with smaller sample sizes, especially for estimating the small class. For estimating the overall accuracy, different sampling designs achieve very similar levels of precision with fewer samples. To achieve a better estimate of the kappa coefficient, stratified random sampling is recommended for use on a map with a high class proportion difference, while random sampling is preferred for a map with low spatial autocorrelation and a low class proportion difference. © 2008 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).","<b>Authors:</b><br/>Chen D., Wei H. <br/><b>Key words:</b><br/>Accuracy assessment, Class proportion, Classification error, Sampling, Spatial autocorrelation"
495,paper_498,"Ding Y., Kandemir M., Irwin M.J., Raghavan P.",,"Process variations, which lead to timing and power variations across identically-designed components, have been identified as one of the key future design challenges by the semiconductor industry. Using worst case latency/power assumptions is one option to address process variations. This option, while simplifying the problem, is becoming less and less attractive as its performance and power costs keep increasing. As a result, exploring options that allow the software to have knowledge about the actual latency/power consumption values is critical for future systems. Targeting systematic process variations, this paper makes two contributions. First, we discuss how we can assign threads to the cores of a chip multiprocessor (CMP) with process variations in mind and show the energy-delay product (EDP) benefits such a process variation-aware thread mapping can bring. Second, we study the benefits of varying the frequencies on a subset of the cores to increase EDP savings. We propose and evaluate integer linear programming based thread mapping schemes in both studies. While these schemes operate with profile data, they can be made to work with partial profiling as well with the help of curve fitting. We tested our schemes using both sequential and multi-threaded benchmarks from different suites and the results collected indicate that we can achieve EDP savings as much as 73.4%, with an average saving of 37.1% over a process variation agnostic scheme. © 2009 Springer Berlin Heidelberg.","<b>Authors:</b><br/>Ding Y., Kandemir M., Irwin M.J., Raghavan P. <br/><b>Key words:</b><br/>"
496,paper_499,"Wang H.J., Zhao J.L., Zhang L.-J.",,"Analyzing business policies for discovering and validating business process models is a critical task in modern organizations, which is currently done in an ad hoc manner due to a lack of systematic methodologies. In this paper, we propose a novel methodology called Policy-Driven Process Mapping (PDPM) for extracting process models from business policy documents. Our research objective is to make process discovery from policy documents more systematic with fewer structural and semantic errors. To the best of our knowledge, PDPM is the first formal approach to discovering process models from business policies. © 2009 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Wang H.J., Zhao J.L., Zhang L.-J. <br/><b>Key words:</b><br/>Business policy, Business process management, Process design, Process mapping"
497,paper_500,"Mo J.P.T., Sigit A., Myers K.",,"Purpose - The purpose of this paper is to describe the development of a product model that is created from the existing products of a small made-to-order furniture company. Design/methodology/approach - The methodology is based on process mapping of the manufacturing of existing and catalogued products and transforming the process maps to operation flow diagrams. Analysis of the diagrams revealed the basis of a product model by sorting large number of parts into two object classes: carcasses and accessories. The production times of these parts were measured by time study methods and populated to a database of standard times. The product model was validated by aggregating the production times for products manufactured over a period and compared against the total available resources. Findings - After validating and adjustments for abnormal circumstances, the product model formed the basis for the development of a production scheduling and control system that assisted the company to achieve 30 per cent productivity improvement. Research limitations/implications - Made-to-order businesses are generally small companies with a large number of product designs that are influenced by customers. Individual customisation of every production order makes it almost impossible to employ traditional standard times data for scheduling and capacity planning. Originality/value - Since every order and product is different, production planning in a made-to-order manufacturing environment is often managed in an ad hoc fashion with little computerised scheduling support. The methodology described in this paper is a practical approach to develop a systematic process that can enable small companies to migrate to a computer aided manufacturing environment without substantial disruption to their normal business. © Emerald Group Publishing Limited.","<b>Authors:</b><br/>Mo J.P.T., Sigit A., Myers K. <br/><b>Key words:</b><br/>Bespoke production, Modelling, Production planning and control"
498,paper_501,"Zhang R.-J., Jiang Y.-S., Liu G.-Y., Wang Z., Li X., Chen Z.-Q., Li J., Huang C.",,"In this study, we developed a systematic method to find risk alleles and relative gene for Rheumatoid Arthritis (RA). The method consists of three steps: 1) genome-wide case-control association studying based on haplotypes, 2) genome-wide association mapping based on directly mining haplotypes produced from case-control data via a density-based clustering algorithm, 3) candidate genes within 1Mb of the interesting haplotype blocks prioritizing underlying biological processes or diseases, based on their similarity to known genes involved in these phenomena. By analyzing the dataset of 5393 informative single-nucleotide polymorphisms (SNPs) markers containing 822 uncorrected individuals which obtained from the North American Rheumatoid Arthritis Consortium (NARAC), we found 25 haplotypes in 18 haplotype blocks and 33 genes will be increase the risk of RA. 9 of the genes have been identified by previous studies, while novel genes may be risk genes for RA. The genes PTPRC (p =1.15E-04) and F12 (1.36E-02) have the highest risk of RA. In summary, the results of our analysis will provide fundamental new insights into the pathogenesis of RA, and the systematic analysis method combining the genome-wide association study based on haplotype and the prioritizing study of candidate genes based on their similarity to known genes will help to comprehend the genetic architecture underlying other complex human diseases. © 2008 IEEE.","<b>Authors:</b><br/>Zhang R.-J., Jiang Y.-S., Liu G.-Y., Wang Z., Li X., Chen Z.-Q., Li J., Huang C. <br/><b>Key words:</b><br/>"
499,paper_502,"Schaffer R., Merker R., Hannig F., Teich J.",,"In this paper a systematic mapping method for a specific algorithm class is given which exploits all levels of parallelism of the target architecture. This target architecture is a processor array where each processing element can have several functional units. This functional units allow subword parallelism, that means multiple equal operations with low data word width can be executed in parallel in the data path of the functional units. The mapping method is illustrated on the edge detection algorithm, and achieves up to 99% of the theoretical speed-up. © 2008 IEEE.","<b>Authors:</b><br/>Schaffer R., Merker R., Hannig F., Teich J. <br/><b>Key words:</b><br/>"
500,paper_503,"Lin L., Qin Z., Li J.",,"North China Plain was the most important cropping region in China with severe challenges of water shortage. Cropping in the plain required large amount of irrigation water to support harvest. However, water resource was very limited due to high evaporation and unbalanced precipitation. Both surface and underground water resources in the region had been over-extracted. Since agriculture consists of the largest component of water uses, mapping irrigation area for estimation of agricultural water demand was urgently required to improve the administration of water resource for effective utilization in the region. We presented our systematic investigation of mapping the irrigation area in the plain using MODIS remote sensing data. Winter wheat had been identified as the main cropping systems requiring intensive irrigation during the growing season from March to early June. The normalized difference of vegetation index (NDVI) had been used to identify winter wheat and forest, which could then be used as the input for irrigation mapping. Then Vegetation supply water index (VSWI) had been used for identifying irrigated area in winter wheat field, which combined the information of temperature and growing condition of vegetation together. According to our study in North China plain, irrigation area could be properly mapped for estimation of agricultural water demand using the MODIS data. Total irrigation area of the region was about 5.9 million ha in 2006. The results indicated that the spatial variation of irrigation area was very obvious in the region. More intensive irrigation could be observed in southern Hebei and northeast Henan of the region. Irrigation percentage in these areas might reach up to 70% for winter wheat in 2006. Therefore, our study demonstrated that MODIS data could be useful for irrigation mapping in regional scale. © 2008 SPIE.","<b>Authors:</b><br/>Lin L., Qin Z., Li J. <br/><b>Key words:</b><br/>Irrigation mapping, MODIS data, North China Plain, Water shortage, Winter wheat"
501,paper_504,"Kaufman T., Sudan M.",,"We argue that the symmetries of a property being tested play a central role in property testing. We support this assertion in the context of algebraic functions, by examining properties of functions mapping a vector space double strok K signn over a field double strok K sign to a subfield double strok F sign. We consider (double strok F sign-)linear properties that are invariant under linear transformations of the domain and prove that an O(1)-local ""characterization"" is a necessary and sufficient condition for 0(l)-local testability, when |double strok K sign| = O(1). (A local characterization of a property is a definition of a property in terms of local constraints satisfied by functions exhibiting a property.) For the subclass of properties that are invariant under affine transformations of the domain, we prove that the existence of a single O(1)-local constraint implies O(1)-local testability. These results generalize and extend the class of algebraic properties, most notably linearity and low-degree-ness, that were previously known to be testable. In particular, the extensions include properties satisfied by functions of degree linear in n that turn out to be O(1)-locally testable. Our results are proved by introducing a new notion that we term ""formal characterizations"". Roughly this corresponds to characterizations that are given by a single local constraint and its permutations under linear transformations of the domain. Our main testing result shows that local formal characterizations essentially imply local testability. We then investigate properties that are linear-invariant and attempt to understand their local formal characterizability. Our results here give coarse upper and lower bounds on the locality of constraints and characterizations for linear-invariant properties in terms of some structural parameters of the property we introduce. The lower bounds rule out any characterization, while the upper bounds give formal characterizations. Combining the two gives a test for all linear-invariant properties with local characterizations. We believe that invariance of properties is a very interesting notion to study in the context of property testing in general and merits a systematic study. In particular, the class of linear-invariant and afflne-invariant properties exhibits a rich variety among algebraic properties and offer better intuition about algebraic properties than the more limited class of low-degree functions. Copyright 2008 ACM.","<b>Authors:</b><br/>Kaufman T., Sudan M. <br/><b>Key words:</b><br/>Error-correcting codes, Locally testable codes, Sublinear time algorithms"
502,paper_505,"Alamús R., Kornus W.",,"Since the advent of the first large format digital aerial cameras, high expectations have been placed on their performance. The dream of obtaining aerial images virtually free of geometric errors and with greater radiometric quality is getting close. Nevertheless, systematic image residuals, unexpected height errors in aerial triangulation and the need for additional self-calibration parameters have been reported since 2005. In this paper a preliminary analysis of the theoretical accuracies in aerial triangulation using the Zeiss/Intergraph (Z/I) Digital Mapping Camera (DMC) and an analogue camera is conducted, motivated by those recent reports. This analysis considers a mathematical model where the image has conical geometry and is free of systematic errors. The influence on the propagated block accuracy of the base-to-height ratio, image pointing precision (both manual and automatic), GPS observations for projection centres and of pass/tie point density is studied. Moreover, the expected accuracy in the aerial triangulation of analogue images using current procedures (having regard to the a priori accuracy for image pointing, ground control measurement and GPS and pass/tie point density) is computed. The goal of this theoretical study is to find the requirements for aerial triangulation with DMC data which would yield the same or an even higher level of accuracy than that obtained with analogue data under the same conditions. The paper continues with a check on the conclusions of this theoretical analysis, using real data-sets and aerial triangulation set-up, which fit with the theoretical analysis. The results prove that the expected theoretical accuracy in aerial triangulation is only obtained if an appropriate self-calibration parameter set is considered in the bundle block adjustment and/or if good GPS observations are available. These requirements result from the unfavourable propagation from unmodelled systematic error in the DMC image blocks. Some authors have detected systematic residuals in the order of one-tenth of a pixel rms in DMC image space. For this reason, investigations are being carried out on systematic error characterisation, distribution in image space and stability over time and flying height, and systematic error modelling, using self-calibration parameter sets and applying correction grids. Finally, conclusions are drawn from the investigations. © 2008 The Authors. Journal Compilation © 2008 Remote Sensing and Photogrammetry Society and Blackwell Publishing Ltd.","<b>Authors:</b><br/>Alamús R., Kornus W. <br/><b>Key words:</b><br/>Accuracy, Digital aerial camera, Geometric calibration"
503,paper_506,"Afzal W., Torkar R., Feldt R.",,"Automated software test generation has been applied across the spectrum of test case design methods, this includes white-box (structural), black-box (functional), grey-box (combination of structural and functional) and non-functional testing. In this paper, we undertake a systematic mapping study to present a broad review of primary studies on the application of search-based optimization techniques to non-functional testing. The motivation is to identify the evidence available on the topic and to identify gaps in the application of search-based optimization techniques to different types of non-functional testing. The study is based on a comprehensive set of 35 papers obtained after using a multi-stage selection criteria and are published in workshops, conferences and journals in the time span 1996-2007. We conclude that the search-based software testing community needs to do more and broader studies on non-functional search-based software testing (NFSBST) and the results from our systematic map can help direct such efforts.","<b>Authors:</b><br/>Afzal W., Torkar R., Feldt R. <br/><b>Key words:</b><br/>"
504,paper_507,"Zhang L., Zhang Q.-J.",,"This paper presents an application of the space mapping concept in the modeling of semiconductor devices. A recently proposed device modeling technique, called neuro-space mapping (Neuro-SM), is described to meet the constant need of new device models due to rapid progress in the semiconductor technology. Neuro-SM is a systematic method allowing us to exceed the present capabilities of the existing device models. It uses a neural network to map the voltage and current signals between an existing device model (coarse model) and the actual device behavior (fine model), such that the mapped model becomes an accurate representation of the new device. An efficient training method based on analytical sensitivity analysis for such mapping neural network is also addressed. The trained Neuro-SM model can retain the speed of the existing device model while improving the model accuracy. The benefit of the Neuro-SM method is demonstrated by examples of SiGe HBT and GaAs MESFET modeling and use of the models in harmonic balance simulation. © Springer Science+Business Media, LLC 2007.","<b>Authors:</b><br/>Zhang L., Zhang Q.-J. <br/><b>Key words:</b><br/>Electronic circuit simulation, Neural networks, Semiconductor device modeling, Space mapping"
505,paper_508,"Fremerey C., Müller M., Kurth F., Clausen M.",,"Significant digitization efforts have resulted in large multimodal music collections comprising visual (scanned sheet music) as well as acoustic material (audio recordings). In this paper, we present a novel procedure for mapping scanned pages of sheet music to a given collection of audio recordings by identifying musically corresponding audio clips. To this end, both the scanned images as well as the audio recordings are first transformed into a common feature representation using optical music recognition (OMR) and methods from digital signal processing, respectively. Based on this common representation, a direct comparison of the two different types of data is facilitated. This allows for a search of scan-based queries in the audio collection. We report on systematic experiments conducted on the corpus of Beethoven's piano sonatas showing that our mapping procedure works with high precision across the two types of music data in the case that there are no severe OMR errors. The proposed mapping procedure is relevant in a real-world application scenario at the Bavarian State Library for automatically identifying and annotating scanned sheet music by means of already available annotated audio material.","<b>Authors:</b><br/>Fremerey C., Müller M., Kurth F., Clausen M. <br/><b>Key words:</b><br/>"
506,paper_509,"Nasruminallah, El-Hajjar M., Othman N.S., Quang A.P., Hanzo L.",,"In this paper we evaluate the performance of Data-Partitioned H.264 video transmission using Unequal Error Protection (UEP) Recursive Systematic Convolutional Codes (RSC). Sphere Packing modulation aided Differential Space Time Spreading is used to improve the overall BER performance and to enhance the objective quality of the transmitted video sequence expressed in terms of Peak Signal-to-Noise Ratio (PSNR). The effect of different error protection schemes on the attainable system performance is demonstrated, when using iterative Soft-Bit Source Decoding (SBSD) and channel decoding, while keeping the overall bit-rate budget constant. Additional system performance improvements are achieved by intentionally increasing the redundancy of the source coded bit-stream using Over-Complete Mapping (OCM). This is achieved by appropriately partitioning the total available bit rate budget between the source and channel codecs. EXIT Charts were used for analysing the attainable system performance. Explicitly, our experimental results show that the proposed UEP scheme outperforms its Equal Error Protection (EEP) counterpart by about 1 dB E b/N0 at the PSNR degradation point of 1 dB. Additionally, an Eb/N0 gain of 12 dB is attained using iterative soft-bit source and channel decoding with the aid of rate-3/4 OCM. ©2008 IEEE.","<b>Authors:</b><br/>Nasruminallah, El-Hajjar M., Othman N.S., Quang A.P., Hanzo L. <br/><b>Key words:</b><br/>"
507,paper_510,"Godtmann S., Lüders H., Ascheid G., Vary P.",,"In this paper, we investigate Turbo-coded transmission over a temporally correlated flat Rayleigh fading channel. Conventionally, channel estimation is performed prior to decoding and is solely based on periodically inserted pilot symbols. We here consider the case that the pilot spacing is not small enough to satisfy the Nyquist criterion for the bandlimited channel process. It is demonstrated that iterative code-aided channel estimation can then significantly improve the bit error rate (BER). Furthermore, we show that the convergence speed of the iterative channel estimation can be significantly accelerated without additional computational power by just carefully choosing the mapping strategy of systematic bits to data symbols. ©2008 IEEE.","<b>Authors:</b><br/>Godtmann S., Lüders H., Ascheid G., Vary P. <br/><b>Key words:</b><br/>"
508,paper_511,"Juramy C., Barrelet E., Schahmaneche K., Bailly P., Bertoli W., Evrard C., Ghislain P., Guimard A., Huppert J.-F., Imbault D., Laporte D., Lebbolo H., Repain P., Sefri R., Vallereau A., Vincent D., An",,"We present the first results of the SuperNova Direct Illumination Calibration Experiment (SNDICE), installed in January 2008 at the Canada France Hawaii Telescope. SNDICE is designed for the absolute calibration of the instrumental response of a telescope in general, and for the control of systematic errors in the SuperNova Legacy Survey (SNLS) on Megacam in particular. Since photometric calibration will a critical ingredient for the cosmological results of future experiments involving instruments with large focal planes (like SNAP, LSST and DUNE), SNDICE functions also as a real-size demonstrator for such a system of instrumental calibration.SNDICE includes a calibrated source of 24 LEDs, chosen for their stability, spectral coverage, and their power, sufficient for a flux of at least 100 electron/s/pixel on the camera. It includes also Cooled Large Area Photodiode modules (CLAPs), which give a redundant measurement of the flux near the camera focal plane. Before installing SNDICE on CFHT, we completed a full calibration of both subsystems, including a spectral relative calibration and a 3D mapping of the beam emitted by each LED. At CFHT, SNDICE can be operated both to obtain a complete one-shot absolute calibration of telescope transmission in all wavelengths for all filters with several incident angles, and to monitor variations on different time scales.","<b>Authors:</b><br/>Juramy C., Barrelet E., Schahmaneche K., Bailly P., Bertoli W., Evrard C., Ghislain P., Guimard A., Huppert J.-F., Imbault D., Laporte D., Lebbolo H., Repain P., Sefri R., Vallereau A., Vincent D., Antilogus P., Astier P., Guy J., Pain R., Regnault N., Attapatu R., Benedict T., Barrick G., Cuillandre J.-C., Gajadhar S., Ho K., Salmon D. <br/><b>Key words:</b><br/>Calibration, LED, Photometry, Telescope"
509,paper_512,"Gorman G.J., Piggott M.D., Wells M.R., Pain C.C., Allison P.A.",,"A systematic approach to unstructured mesh generation for ocean modelling is presented. The method optimises unstructured meshes to approximate bathymetry to a user specified accuracy which may be defined as a function of longitude, latitude and bathymetry. GMT (Generic Mapping Tools) is used to perform the initial griding of the bathymetric data. Subsequently, the Terreno meshing package combines automated shoreline approximation, mesh gradation and optimisation methods to generate high-quality bathymetric meshes. The operation of Terreno is based upon clearly defined error measures and this facilitates the automation of unstructured mesh generation while minimising user intervention and the subjectivity that this can introduce. © 2008 Elsevier Ltd. All rights reserved.","<b>Authors:</b><br/>Gorman G.J., Piggott M.D., Wells M.R., Pain C.C., Allison P.A. <br/><b>Key words:</b><br/>Bathymetry, Ocean modelling, Optimisation, Shoreline, Simplification, Unstructured mesh generation"
510,paper_513,"Park H.",,"[No abstract available]","<b>Authors:</b><br/>Park H. <br/><b>Key words:</b><br/>"
511,paper_514,"Choi C., Kim C., Jeon J., Park Y.",,"Given that the study on the creation and operation of e-service is an important research field, the challenges of developing systematic and scientific methodology for the analysis of e-service operation and creation still carry on. This research attempts to suggest a systematic approach to identify the evolution of e-service based on the BM patent database as a quantitative database. Despite the considerable contribution that BM patents can bring to the service world as a valuable database, only a few studies have been conducted on them. This research proposes an algorithm for constructing a BM patent evolution map by selecting major arcs in the patent citation network. Furthermore, the algorithm is applied to the e-finance field to construct and evolution map. The results of this research can be used to understand the process of evolution of e-service, extracting fruitful new service ideas from existing e-service processes or methods, and identifying the legal rights and limitations of created e-service concepts and operation methods. © 2008 IEEE.","<b>Authors:</b><br/>Choi C., Kim C., Jeon J., Park Y. <br/><b>Key words:</b><br/>Business method patent, E-service, Patent citation analysis, Patent evolution map"
512,paper_515,"Pretorius R., Budgen D.",,"Context: Although the Unified Modeling Language (UML) is now widely used, there is little empirical knowledge about the effectiveness of its different elements. Objectives: To conduct a systematic review of the literature describing empirical studies of this paradigm. Method: We undertook a Mapping Study of the literature, based upon electronic searching of major digital libraries. Results: 33 papers have been identified and classified by topic, form of study involved, and type of publication. Conclusions: The largest group of empirical studies of the UML concentrate on comprehension, and many key aspects have hardly been investigated at all. Copyright 2008 ACM.","<b>Authors:</b><br/>Pretorius R., Budgen D. <br/><b>Key words:</b><br/>Evidence-Based software engineering, Mapping study"
513,paper_516,"Isaac A., van der Meij L., Schlobach S., Wang S.",,"Instance-based ontology mapping is a promising family of solutions to a class of ontology alignment problems. It crucially depends on measuring the similarity between sets of annotated instances. In this paper we study how the choice of co-occurrence measures affects the performance of instance-based mapping. To this end, we have implemented a number of different statistical co-occurrence measures. We have prepared an extensive test case using vocabularies of thousands of terms, millions of instances, and hundreds of thousands of joint items. We have obtained a human Gold Standard judgement for part of the mappingspace. We then study how the different co-occurrence measures and a number of algorithmic variations perform on our benchmark dataset as compared against the Gold Standard. Our systematic study shows excellent results of instance-based matching in general, where the more simple measures often outperform more sophisticated statistical measures. This paper is an abbreviated version of a paper accepted at the 6th International Semantic Web Conference, ISWC 2007 [3].","<b>Authors:</b><br/>Isaac A., van der Meij L., Schlobach S., Wang S. <br/><b>Key words:</b><br/>"
514,paper_517,"Hernández M.A., Papotti P., Tan W.-C",,"Data exchange is the process of converting an instance of one schema into an instance of a different schema according to a given specification. Recent data exchange systems have largely dealt with the case where the schemas are given a priori and transformations can only migrate data from the first schema to an instance of the second schema. In particular, the ability to perform data-metadata translations, transformation in which data is converted into metadata or metadata is converted into data, is largely ignored. This paper provides a systematic study of the data exchange problem with data-metadata translation capabilities. We describe the problem, our solution, implementation and experiments. Our solution is a principled and systematic extension of the existing data exchange framework, all the way from the constructs required in the visual interface to specify data-metadata correspondences, which naturally extend the traditional value correspondences, to constructs required for the mapping language to specify data-metadata translations, and algorithms required for generating mappings and queries that perform the exchange. © 2008 VLDB Endowment.","<b>Authors:</b><br/>Hernández M.A., Papotti P., Tan W.-C <br/><b>Key words:</b><br/>"
515,paper_518,"Chen Q., Kotani K., Lee F.-F., Ohmi T.",,"In this paper, we present a VQ-based fast face recognition algorithm using an optimized codebook. Previously, Chen et al. [16] proposed a novel codebook design method based on the systematic classification and organization of code patterns abstracted from facial images for reliable face recognition. In this paper, an improved codebook design method is proposed. Combined by a systematically organized codebook based on the classification of code patterns and another codebook created by Kohonen's Self-Organizing Maps (SOM), an optimized codebook consisted of 2x2 codevectors for facial images is generated. We demonstrate the performance of our algorithm using publicly available AT&T database containing variations in lighting, posing, and expressions. Compared with the algorithms employing original codebook or SOM codebook separately, experimental results show face recognition using the optimized codebook is more efficient. The highest average recognition rate of 98.2% is obtained for 40 persons' 400 images of AT&T database. A table look-up (TLU) method is also proposed for the speed up of the recognition processing in this paper. By applying this method in the quantization step, the total recognition processing time achieves only 28 msec, enabling real-time face recognition. ©2008 IEEE.","<b>Authors:</b><br/>Chen Q., Kotani K., Lee F.-F., Ohmi T. <br/><b>Key words:</b><br/>Codebook, Face recognition, Pattern classification, SOM, Vector quantization"
516,paper_519,"Wang L., Wong K.C.L., Zhang H., Shi P.",,"Body surface potential (BSP) has been used as the single data source in inverse electrocardiography (IECG). The lack of volumetric spatial resolutions in BSP, however, hinders the noninvasive mapping of volumetric cardiac transmembrane potentials (TMPs). Tomographic image sequence, which contains temporally sparse but spatially dense cardiac kinematic measures, becomes ideal dynamic complements to BSPs through cardiac electromechanical (EM) coupling. In this paper, we present a model-constrained Bayesian framework to integrate tomographic image sequence and BSP maps (BSPM) for volumetric cardiac TMP mapping. A priori physiological knowledge is incorporated via stochastic modeling of the cardiac electrophysiological system with unknown systematic errors. With this system as a platform for data integration, adaptive data assimilation is used to estimate patient specific TMPs from BSPMs under the guidance of tomographic images. In this way, complementary images are integrated in accord with their respective merits and limitations. Phantom and real data experiments exhibit notable improvements and practicability of the presented framework. © 2008 IEEE.","<b>Authors:</b><br/>Wang L., Wong K.C.L., Zhang H., Shi P. <br/><b>Key words:</b><br/>Cardiac electrical imaging, Inverse electrocardiography, Motion detection, MRI"
517,paper_520,"Hongbin Y., Minghua S., Xinhua X.",,"Voice of customer (VOC) is a key driving force for product development, as well as a fundamental base of marketing decision-making. The traditional approaches for VOC analysis using face-to-face customer interviews often result in high cost and long lead-time. Thus it is no longer suitable for dynamic response to changing market conditions. With fast growth and application of internet technology, extensive VOC with dynamic information can be collected and exchanged through the web media. In this paper, a new method of exploring and analyzing customer requirements on Web data source is presented. The scope and advantages of this approach are discussed. A systematic mapping framework, from the original web data to structured marketing setting, is constructed. Supporting knowledge base and rule-driven mining methods are also developed. Finally, a case study using car users data is presented to demonstrate the effectiveness of the proposed approach. © 2008 IEEE.","<b>Authors:</b><br/>Hongbin Y., Minghua S., Xinhua X. <br/><b>Key words:</b><br/>"
518,paper_521,"Yin Y.-S., Du G.-M., Song Y.-K.",,"The hybrid reconflgurable computing system that couples the traditional processor with the reconflgurable hardware is becoming a mainstream today. A new hybrid reconflgurable computing system MPRS (Multi-Pipeline Reconflgurable System) is presented, which has multiple linear arrays for pipelined applications. MPRS' architecture and the mapping method are discussed. Firstly, a complete system including the MPRS simulator at behavior level and its programming environment is built. Secondly, a systematic approach to MPRS arrays based on the DGRV (Dependency Graph with Reconflgurable Variable) is discussed, and the objective functions and design constraint are analyzed. Finally, this paper tests several typical applications. © 2008 IEEE.","<b>Authors:</b><br/>Yin Y.-S., Du G.-M., Song Y.-K. <br/><b>Key words:</b><br/>Multi-pipeline, Reconfigurable, Systematic approach"
519,paper_522,"Shen R., Vemuri N.S., Fan W., Fox E.A.",,"In this paper, we formalize the digital library (DL) integration problem and propose an overall approach based on the 5S (streams, structures, spaces, scenarios, and societies) framework. We then apply that framework to integrate domain-specific (archeological) DLs, illustrating our solutions for key problems in DL integration. An integrated Archeological DL, ETANA-DL, is used as a case study to justify and evaluate our DL integration approach. More specifically, we develop a minimal metamodel for archeological DLs within the 5S theory. We implement the 5SSuite tool set to cover the process of union DL generation, including requirements gathering, conceptual modeling, rapid prototyping, and code generation. 5SSuite consists of 5SGraph, 5SGen, and SchemaMapper, each of which plays an important role in DL integration. We also propose an approach to integrated DLs based on the 5S formalism, which provides a systematic method to design and implement DL exploring services. © 2008 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Shen R., Vemuri N.S., Fan W., Fox E.A. <br/><b>Key words:</b><br/>5S, Archaeology, Digital library integration, Exploring service, Information integration, Information visualization, Integration, Metadata, Multi-dimensional browsing, Schema mapping, Semantic interoperability"
520,paper_523,"Hardie K.A., Witbooi P.J.",,"We define the notion of homotopy pushout in the category of binary reflexive relational structures and explore its basic properties. We construct finite models in this category, of spaces and maps in Top with a view to developing systematic methods in this regard. © 2008 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Hardie K.A., Witbooi P.J. <br/><b>Key words:</b><br/>Binary reflexive relational structure, Double mapping cylinder, Homotopy pushout, Weak homotopy equivalence"
521,paper_524,"Bjørnson F.O., Dingsøyr T.",,"Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies. The majority of empirical studies relate to technocratic and behavioural aspects of knowledge management, while there are few studies relating to economic, spatial and cartographic approaches. A finding reported across multiple papers was the need to not focus exclusively on explicit knowledge, but also consider tacit knowledge. We also describe implications for research and for practice. © 2008 Elsevier B.V. All rights reserved.","<b>Authors:</b><br/>Bjørnson F.O., Dingsøyr T. <br/><b>Key words:</b><br/>Knowledge management, Learning software organization, Software engineering, Software process improvement, Systematic review"
522,paper_525,"Chen J., Chen J., Zhang X.",,"The analysis of airborne equipment invalidation data that contains system failures is becoming increasingly important in the aircraft maintenance. However, carrying out an effective predictive maintenance plan, information about current airborne equipment reliability conditions must be understood to the decision-maker. In this paper, a systematic methodology to construct a prediction model for aircraft reliability based on airborne equipment invalidation data has been proposed. We take advantage of the large power of the Self-Organizing Map SOM) technique developed by Teuvo Kohonen. SOM, that is an unsupervised neural network mapping a set of n-dimensional vectors to a two-dimensional topographic map, is used to combine their scatter data into a sequence model based on the time-to-failure data extracted from the repair registers. Its effectiveness is illustrated by the results of Mean Time Between Failures (MTBF) study and analysis. The method can help proactively diagnose airborne equipment faults with a sufficient lead time before actual system failures. It can allow preventive maintenance to be scheduled. Thereby it can reduce the downtime costs significantly. © 2008 IEEE.","<b>Authors:</b><br/>Chen J., Chen J., Zhang X. <br/><b>Key words:</b><br/>Genetic algorithms, MTBF, Prediction model, Reliability modeling, Self-organizing map"
523,paper_526,"Jin W., Zhang C.N., Li H.",,"Systolic array is a well known VLSI architecture to achieve extensive parallel and pipelining computing. Many systolic designs have been reported. All are algorithm based, that is one design is only for solving one specific problem. In this paper, the special purpose systolic architecture has been extended into a reconfigurable one and a systematic design approach to mapping two or more algorithms into a single reconfigurable systolic array is presented. First multiple algorithms are mapped into a reconfigurable systolic array that is able to compute one algorithm at a time with proper control settings. Second the reconfigurable systolic array is extended by using time or space redundancy so that it can compute multiple algorithms simultaneously. In addition, the optimal mapping, which minimizes the total hardware cost and computation time, is explored and the necessary condition of the transformation for computing multiple problem instances is also proposed. According to this condition, the search space of finding the optimal mapping can be significantly reduced. © 2008 IEEE.","<b>Authors:</b><br/>Jin W., Zhang C.N., Li H. <br/><b>Key words:</b><br/>Optimal design, Reconfigurable architecture, Space-time mapping, Systematic approach, Systolic array"
524,paper_527,"Ryu M., Kim Y., Park H.",,"A network defines its own Quality-Of-Service (QoS) class and has QoS support mechanisms. So, effectively to support end-to-end QoS in heterogeneous networks, a certain unified control is needed, however, it causes scalability problem as management complexity and implementation difficulty. There is a strong need to provide simple interoperability with QoS support so we present a QoS Class Mapping (QCM) framework: building blocks should be defined such as parameter mapping and class mapping. And we improve the framework, called as QCM-ASM, to support not only flawless class mapping but also fine-granular QoS in any circumstance. At last, another framework with adaptive QoS Class Selection (AQCS) mechanism, named as AQCM-ASM framework, is proposed. AQCS mechanism can prevent resource starvation of lower priority class and provide an effective resource distribution. As an experimental result, we demonstrate a performance of the proposed frameworks. The performance results show characteristics of each framework. © 2008 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Ryu M., Kim Y., Park H. <br/><b>Key words:</b><br/>Heterogeneous networks, Interoperability, QoS"
525,paper_528,"Manh Tuan V., Amano H.",,"The multi-process execution in dynamically reconfig-urable processors is a technique to enhance throughput by trying to exploit more inherent parallelism of applications. Basically, a total process for an application is divided into small processes, assigned into limited areas of a reconfigurable array, and concurrently executed in a pipelined manner. In order to improve the efficiency of the multi-process execution, a systematic method for mapping processes onto a reconfigurable array consisting of multiple hardware execution units is essential. This paper proposes and investigates a systematic method for mapping an application modeled as a Kahn Process Network onto a dynamically reconfigurable processing array. In order to execute streaming applications in a pipelined manner, the size of Tiles, which is a unit area of dynamically reconfigurable array, and the grouping of processes are adjusted. Using real applications such as DCT, JPEG encoder and Turbo encoder, the impact of different versions mapped onto the NEC Dynamically Reconfigurable Processor on performance is evaluated. Evaluation results show that our proposed mapping algorithm achieves the best performance in terms of the throughput and the execution time. Copyright © 2008 The Institute of Electronics, Information and Communication Engineers.","<b>Authors:</b><br/>Manh Tuan V., Amano H. <br/><b>Key words:</b><br/>Dynamically reconfigurable processor, Multi-process execution, Single-process execution"
526,paper_529,"Park J.M., Nam J.H., Hu Q.P., Suh H.W.",,"Ontology provides a base framework for knowledge representation, and the methodology of ontology construction is one of the most important research topics in the ontology community. There have been many methodologies proposed, and some of them have been along with constructing engineering ontology. However, the previous methodologies are mostly top-down approaches which do not maximize the benefits of bottom-up approaches. There are few bottom-up approaches, but they do not utilize the full resources of knowledge such as engineering documents. This study proposes the systematic methodology to develop the ontology in a bottom-up style from engineering documents, called DocOnto (Document-based Ontology). The methodology is mainly composed of three phases such as defining ontology for terms in engineering documents, integrating the ontology with semantic networks for both a single document and a focused document group, and pruning the ontology for practically usage. In this approach, first-order logic (FOL) and semantic networks (SN) are used for formal and visual representation of ontology, and semantic mapping with similarity evaluation is used in integrating ontology. This approach can be computerized for structured engineering documents. Through the methodology description, a comprehensive example is utilized to show the methodology in every detailed step.","<b>Authors:</b><br/>Park J.M., Nam J.H., Hu Q.P., Suh H.W. <br/><b>Key words:</b><br/>Engineering documents, First order logic, Ontology construction, Semantic networks"
527,paper_530,"Song K., Lee S.",,"In a rapidly growing information-oriented society, people with disabilities and older people are faced with serious inconveniences in accessing IT products due to complicated use of technologies and poorly designed interfaces. To solve the problems, it is required for designers and engineers to find out and understand user needs, and then to figure out the functionalities and design characteristics to meet the needs. Figuring out the Engineering Characteristics (EC) of products from user needs for people with disabilities and older people who have problems using main stream products due to limited accessibility would take great efforts and time. We merged two important concepts of product design for people with disabilities and older people, accessibility and universal design, using an engineering design framework of QFD (Quality Function Deployment) to provide engineers and designers with a systematic methodology for universal product design. We adapted a technical report from the ISO/IEC JTC1 Special Working Group on Accessibility (SWG-A), the Information Technology - Accessibility Considerations for People with Disabilities - Part 1: User Needs Summary (ISO/IEC PDTR 99999-1), and mapped the detailed user accessibility needs to the guidelines of 7 universal design principles which are widely accepted. We explained how a process model was built for mapping relationship between the guidelines and user accessibility needs, and extracting the critical engineering characteristics for IT product design based on these two models. A model House of Quality(HOQ) was built for such procedures. © 2008 Springer-Verlag.","<b>Authors:</b><br/>Song K., Lee S. <br/><b>Key words:</b><br/>Design guidelines, ISO/IEC PDTR 99999-1, Quality Function Deployment (QFD), Universal design process, User needs"
528,paper_531,"Guan Y., Wang X., Wang Q.",,"The relationship of similarity may be the most universal relationship that exists between every two objects in either the material world or the mental world. Although similarity modeling has been the focus of cognitive science for decades, many theoretical and realistic issues are still under controversy. In this paper, a new theoretical framework that conforms to the nature of similarity and incorporates the current similarity models into a universal model is presented. The new model, i.e., the systematic similarity model, which is inspired by the contrast model of similarity and structure mapping theory in cognitive psychology, is the universal similarity measurement that has many potential applications in text, image, or video retrieval. The text relevance ranking experiments undertaken in this research tentatively show the validity of the new model. © 2008 IEEE.","<b>Authors:</b><br/>Guan Y., Wang X., Wang Q. <br/><b>Key words:</b><br/>Matching objects pair, Structure mapping theory, Systematic similarity, Systematic similarity measurement criterion, Systematic similarity model (SSM)"
529,paper_532,"Sun N., Zhang Y., Mei X.",,"Faithfully obtaining design specifications from customer requirements is essential for successful designs. The natural lingual, inexact, incomplete and vague attributes of customer requirements make it very difficult to map customer requirements to design specifications. In general design process, the design specifications are determined by designers based on their experience and intuition, and often a certain target value is set for a specification. However, it is on one hand very difficult, on the other hand unreasonable, so a suitable limit range rather than a certain value is preferred at the beginning of design, especially at the concept design process. In this paper, a simplified systematic approach of transforming customer requirements to design specifications is proposed. First, a two-stepped clustering approach for grouping customer requirements and design specifications based on HOQ matrix is presented, by which the mapping is limited to within each group. To further simplify the inference mapping rules of customer requirements and design specifications, the minimal condition inference mapping rules for each design specification are extracted based on rough set theory. In the end, a suitable value range is determined for a specification by applying the fuzzy rule matrix. Copyright © 2007 by ASME.","<b>Authors:</b><br/>Sun N., Zhang Y., Mei X. <br/><b>Key words:</b><br/>Customer requirement, Design specification, Fuzzy rule matrix, Lower approximation"
530,paper_533,"Kitamura Y., Takafuji S., Mizoguchi R.",,"Functionality is one of the key aspects of artifact models for design. A function of a device, however, can be captured in different ways in different domains or by different modelauthors. Much research on functions has been conducted in the areas of engineering design, functional representation and philosophy, although there are several definitions and notions of functions. We view conceptualization of function is multiplicative in nature: different functions can be captured simultaneously from an objective behavior of an artifact under different teleological contexts of users/designers, or from different viewpoints (perspectives) of a model-author. Such differences become problematic for sharing functional knowledge among engineers. In this article, we attempt to clarify the differences of such perspectives for capturing functions on the basis of the ontological engineering. On the basis of a generalized model of the standard input-output model in the well-known systematic design methodology, we show descriptive categorization of some upper-level types (classes) of functions with references to some definitions of functions in the literature. Such upper-level ontological categories of functions are intended to be used as a reference ontology for functional knowledge interoperability. One of the two usages here is to convert functional models between different functional taxonomies. A functional term in a taxonomy is (ideally) categorized into a generic type defined in the reference ontology. It is widely recognized in the literature that such an upper-level ontology helps automatic 'mapping discovery' which is to find similarities between two ontologies and determine which concepts represent similar notion. The reference ontology of function might have such an effect. Another usage of the reference ontology is to integrate fault knowledge into functional knowledge and automatic transformation of FMEA sheets. The designer can describe an integrated model of both functional knowledge and fault knowledge. Based on ontology mappings, automatic transformations of FMEA sheets can be realized. In this article, we discuss the detail of the definitions of the upper-level categories of functions ontologically. Then, we give an overview of usages and effects of the upper-level categories as a reference ontology for functional knowledge interoperability. Copyright © 2007 by ASME.","<b>Authors:</b><br/>Kitamura Y., Takafuji S., Mizoguchi R. <br/><b>Key words:</b><br/>Design knowledge management, Functional design, Functional representation, Ontology"
531,paper_534,"Zhang J., Yang Y., Zhang L.",,"For improving the digital map's quality, collocation method is a helpful tool to fit the systematic errors and weaken the effects of the stochastic errors. The main difference of the collocation applied in our situation is that the remained systematic errors resulted by the inaccurate covariance function will be fitted again and reduced from the estimated signals. In this way, the modified collocation can effectively resist the influences of systematic errors. An actual calculation and corresponding analysis show that the proposed method is effective in improving the quality of digital maps.","<b>Authors:</b><br/>Zhang J., Yang Y., Zhang L. <br/><b>Key words:</b><br/>Covariance function, Deformation, Re-fit, Signal, Systematic errors"
532,paper_535,"Mosleh M., Kheyrandish M., Setayeshi S., Golshahi M.",,"This paper presents a systematic procedure to mapping uniform recurrent equations on to linear systolic arrays. In this method, a bijective transformation must be computed first. Then, by applying this transformation over the N-Dim index space of a uniform recurrent equation, a new index space will be obtained that includes a 1-Dim location and a (N-l)-Dim time. By using the location and time dimensions, the location of performing each computation of uniform recurrent equations on the processor array and the time to performing each computation can be determined, respectively. Each computation time is specified by a (N-l)-Dim vector. Therefore, for reaching the scalar times correspond to logical times, we can use full trees first. Then we compute the times correspond to time vectors. The proposed method has been compared with General Parameter method and Dependency method from ?Total time × Processor element? viewpoint.","<b>Authors:</b><br/>Mosleh M., Kheyrandish M., Setayeshi S., Golshahi M. <br/><b>Key words:</b><br/>Dependency method, Designation matrix, GPM, Linear Image, Parallel architecture, Systolic array"
533,paper_536,"Vu M.T., Amano H.",,"The multi-process execution in dynamically reconfigurable processors is a technique to enhance throughput by trying to exploit more inherent parallelism of applications. In order to improve the efficiency of the multi-process execution, the paper proposes a systematic method for mapping an application modeled as a Kahn Process Network onto a dynamically reconfigurable processing array. Using real applications, the impact on the performance from different versions mapped onto the Dynamically Reconfigurable Processor (DRP) is evaluated. Evaluation results show that our proposed mapping algorithm achieves the best performance in terms of the throughput and the execution time. © 2007 IEEE.","<b>Authors:</b><br/>Vu M.T., Amano H. <br/><b>Key words:</b><br/>"
534,paper_537,"Isaac A., Van Der Meij L., Schlobach S., Wang S.",,"Instance-based ontology mapping is a promising family of solutions to a class of ontology alignment problems. It crucially depends on measuring the similarity between sets of annotated instances. In this paper we study how the choice of co-occurrence measures affects the performance of instance-based mapping. To this end, we have implemented a number of different statistical co-occurrence measures. We have prepared an extensive test case using vocabularies of thousands of terms, millions of instances, and hundreds of thousands of co-annotated items. We have obtained a human Gold Standard judgement for part of the mapping-space. We then study how the different co-occurrence measures and a number of algorithmic variations perform on our benchmark dataset as compared against the Gold Standard. Our systematic study shows excellent results of instance-based matching in general, where the more simple measures often outperform more sophisticated statistical co-occurrence measures. © 2008 Springer-Verlag Berlin Heidelberg.","<b>Authors:</b><br/>Isaac A., Van Der Meij L., Schlobach S., Wang S. <br/><b>Key words:</b><br/>"
535,paper_538,"Xia X.",,"It is very important to diagnose systematic errors for improving working performance of manufacture systems. At present, many methods of diagnosis for systematic errors require the certain probability distribution and a great deal of data, primarily based on statistics. Therefore a method using grey relational analysis is proposed to resolve the problem. This method can diagnose systematic errors only with small sample, without special requirements for probability distribution. The concepts, grey confidence level, grey difference, weighting coefficient and weighting function mapping, are defined to test reliability of the diagnosis results. This method is validated by computer simulation and engineering experiments. And the grey confidence level is proved to be 95%.","<b>Authors:</b><br/>Xia X. <br/><b>Key words:</b><br/>Diagnosis, Grey relational analysis, Manufacture system, Systematic errors"
536,paper_539,"Jacobsen K.",,"Digital aerial frame cameras like Intergraph DMC and Vexcel UltraCamD are becoming standard for mapping application. The virtual or synthetic images of these cameras are based on individual sub-cameras. The calibration of the sub-cameras is respected for the matching of the sub-images to a large virtual image. So by theory, the virtual images should have perfect perspective geometry without any systematic image error. In reality this is not the case and for reaching the possible accuracy potential, block adjustment with self calibration by additional parameters is required. So also under operational conditions quite better accuracy can be reached like with analog photos. Standard deviation of unit weight (sigma0) up to 0.15 pixels is possible by automatic aero triangulation. Usual commercial digital photogrammetric workstations are not able to respect systematic image errors determined by block adjustment. This causes model deformations, acceptable for the horizontal coordinate components, but in the height, deformations exceeding the accuracy potential may occur. A program for a posteriori correction of digital elevation models has been generated, so the full accuracy potential can be reached. The photo flights have been made not with the latest versions of generating DMC- and UltraCamD images. In the meantime Intergraph and Vexcel modified the cameras and the data handling. The described geometric problems exist also for analog cameras, but they are usually ignored.","<b>Authors:</b><br/>Jacobsen K. <br/><b>Key words:</b><br/>Digital aerial cameras, Geometry, Model deformation, Self calibration"
537,paper_540,"Sun J., Sun Y., Ding G., Liu Q., Wang C., He Y., Shi T., Li Y., Zhao Z.",,"Background: Although many genomic features have been used in the prediction of protein-protein interactions (PPIs), frequently only one is used in a computational method. After realizing the limited power in the prediction using only one genomic feature, investigators are now moving toward integration. So far, there have been few integration studies for PPI prediction, one failed to yield appreciable improvement of prediction and the others did not conduct performance comparison. It remains unclear whether an integration of multiple genomic features can improve the PPI prediction and, if it can, how to integrate these features. Results: In this study, we first performed a systematic evaluation on the PPI prediction in Escherichia coli (E. coli) by four genomic context based methods: the phylogenetic profile method, the gene cluster method, the gene fusion method, and the gene neighbor method. The number of predicted PPIs and the average degree in the predicted PPI networks varied greatly among the four methods. Further, no method outperformed the others when we tested using three well-defined positive datasets from the KEGG, EcoCyc, and DIP databases. Based on these comparisons, we developed a novel integrated method, named InPrePPI. InPrePPI first normalizes the AC value (an integrated value of the accuracy and coverage) of each method using three positive datasets, then calculates a weight for each method, and finally uses the weight to calculate an integrated score for each protein pair predicted by the four genomic context based methods. We demonstrate that InPrePPI outperforms each of the four individual methods and, in general, the other two existing integrated methods: the joint observation method and the integrated prediction method in STRING. These four methods and InPrePPI are implemented in a user-friendly web interface. Conclusion: This study evaluated the PPI prediction by four genomic context based methods, and presents an integrated evaluation method that shows better performance in E. coli. © 2007 Sun et al, licensee BioMed Central Ltd.","<b>Authors:</b><br/>Sun J., Sun Y., Ding G., Liu Q., Wang C., He Y., Shi T., Li Y., Zhao Z. <br/><b>Key words:</b><br/>"
538,paper_541,"Yegneswaran V., Alfeld C., Barford P., Cai J.-Y.",,"Over the past several years, honeynets have proven invaluable for understanding the characteristics of unwanted Internet traffic from misconfigurations and malicious attacks. In this paper, we address the problem of defending honeynets against systematic mapping by malicious parties, so we can ensure that honeynets remain viable in the long term. Our approach is based on two ideas: (i) counting the number of probes received in the honeynet, and (ii) shuffling the location of live systems with those that comprise the honeynet in a larger address space after the probe count has exceeded a threshold. We describe four different strategies for randomizing the location of the honeynet. Each strategy is defined in terms of the degree of defense that it provides and its associated computational and state requirements. We implement a prototype middlebox that we call Kaleidoscope to gain practical insight on the feasibility of these strategies. Through a series of tests we show that the system is capable of effectively defending honeynets in large networks with limited impact on normal traffic, and that it continues to respond well in the face of large resource attacks. © 2007 IEEE.","<b>Authors:</b><br/>Yegneswaran V., Alfeld C., Barford P., Cai J.-Y. <br/><b>Key words:</b><br/>"
539,paper_542,"[No author name available]",,"The proceedings contain 114 papers. The topics discussed include: the maximum a posteriori decoding using variational Bayes methods for digital magnetic recording channels, design of nonbinary quasi-cyclic LDPC cycle codes, a unified decoding algorithm for linear codes based on partitioned parity-check matrices, exit functions for randomly punctured systematic codes, spectral shaping technique for permutation distance-preserving mapping codes, split-turbo codes for nonuniform sequences, power allocation for discrete-input non-ergodic block-fading channels, probabilistic capacity and optimal coding for asynchronous channel, the effect of finite memory on throughput of wireless packet networks, iterative coded pulse-position-modulation for deep-space optical communications, lattice coding for the vector fading paper problem, and average throughput with linear network coding over the binary field.","<b>Authors:</b><br/>[No author name available] <br/><b>Key words:</b><br/>"
540,paper_543,"Calin-Jageman R.J., Xie C., Pan Y., Vandenberg A., Katz P.S.",,"Neuroscience research increasingly involves the exploration of computational models of neurons and neural networks. To ensure systematic model exploration, it is often desirable to conduct a parameter-space analysis in which the behavior of the model is catalogued over a very large range of parameter permutations. Here we report the development and testing of a toolkit called NEURONgrid for conducting this type of analysis in a grid environment using NEURON (Hines & Carnevale, 1997, 2001), a popular and powerful simulation platform for the neurosciences. NEURONgrid provides helper classes within NEURON for manipulating parameters, a package of NEURON for running in a grid environment, and a management client that enables neuroscientists to submit a parameter-space analysis, monitor progress, and download results. NEURONgrid provides a user-friendly means for conducting intensive model exploration within the neurosciences. It is available for download at http://neurongrid.homeip. net. © Springer-Verlag Berlin Heidelberg 2007.","<b>Authors:</b><br/>Calin-Jageman R.J., Xie C., Pan Y., Vandenberg A., Katz P.S. <br/><b>Key words:</b><br/>"
541,paper_544,"Hu J., He X., Baggerly K.A., Coombes K.R., Hennessy B.T.J., Mills G.B.",,"Motivation: Proteins play a crucial role in biological activity, so much can be learned from measuring protein expression and post-translational modification quantitatively. The reverse-phase protein lysate arrays allow us to quantify the relative expression levels of a protein in many different cellular samples simultaneously. Existing approaches to quantify protein arrays use parametric response curves fit to dilution series data. The results can be biased when the parametric function does not fit the data. Results: We propose a non-parametric approach which adapts to any monotone response curve. The non-parametric approach is shown to be promising via both simulation and real data studies, it reduces the bias due to model misspecification and protects against outliers in the data. The non-parametric approach enables more reliable quantification of protein lysate arrays. © The Author 2007. Published by Oxford University Press. All rights reserved.","<b>Authors:</b><br/>Hu J., He X., Baggerly K.A., Coombes K.R., Hennessy B.T.J., Mills G.B. <br/><b>Key words:</b><br/>"
542,paper_545,"Wang M.-Q., Ma Y.-S.",,"This paper presents a fundamental research on customer requirements modelling and propagation throughout the whole product development lifecycle. A theoretical mapping model, from customer requirements to quality characteristics, is established. It covers three processes, i.e., the qualification and classification of customer requirements, the generation and transformation of product quality characteristics, and product quality characteristics optimisation. The Analytic Network Process (ANP) approach was adopted to establish the weights of different customer requirements and product quality characteristics. Intra- and inter-relations for customer requirements and quality characteristics are modelled. Matching and conflict-resolving algorithms are proposed. A case study is also given. © 2007 Inderscience Enterprises Ltd.","<b>Authors:</b><br/>Wang M.-Q., Ma Y.-S. <br/><b>Key words:</b><br/>Analytic Network Process, ANP, customer requirements, mapping and analysis, product design, product quality characteristics, Zero-One Goal Programming, ZOGP"
543,paper_546,"Sun J., Colon J., Karimi K.J.",,"This paper presents a systematic method for modeling small-signal input impedance of line-frequency ac-dc converters. The objective is to develop proper models that can be used for stability analysis of ac power systems with significant dc loads powered by such converters. The proposed modeling method uses harmonic linearization and Fourier analysis techniques to describe the current and voltage mapping process through the converter switching circuit. The voltage and current mapping relations are then combined to give an impedance mapping model which converts the impedance of any circuit or system connected to the dc output of converter into a corresponding small-signal input impedance of the converter at the ac side. Similar relations can be used to map the ac source impedance into the dc side to give the equivalent dc source impedance for stability analysis of the dc subsystem. This paper focuses on the basic principle of the impedance mapping method and uses a single-phase diode rectifier circuit to demonstrate the modeling process. The resulting ac input impedance model is validated by detailed circuit simulation as well as experimental measurements. ©2006 IEEE.","<b>Authors:</b><br/>Sun J., Colon J., Karimi K.J. <br/><b>Key words:</b><br/>"
544,paper_547,"Geelen B., Ferentinos A., Catthoor F., Vandecappelle A., Lafruit G., Stouraitis T., Lauwereins R., Verkest D.",,"A comparison is performed between a simple hardware-controlled cache mapping and a systematic, software-controlled scratchpad mapping of a two-dimensional, lifting-based Wavelet Transform, in the context of dynamic applications. In this context it is important for the application to optimally exploit the memory hierarchy under the dynamically varying memory availability. Contrary to expectations, the comparison shows the cache-based mapping to exhibit a lower missrate. The reasons for these different results are examined and improvements for the systematic scratchpad mapping are proposed which would give it a 30% lower missrate performance for localized wavelet-based applications. © 2006 IEEE.","<b>Authors:</b><br/>Geelen B., Ferentinos A., Catthoor F., Vandecappelle A., Lafruit G., Stouraitis T., Lauwereins R., Verkest D. <br/><b>Key words:</b><br/>"
545,paper_548,"Shiming L., Xing Z., Wenbo W.",,"Multiple Input Multiple Output-Orthogonal Frequency Division Multiplexing (MIMO-OFDM) systems can provide significant space diversity and combat multi-path fading, thus attracting increasing attention for the design of future broadband wireless communication systems. One enhancement technique commonly used in such systems is Adaptive Modulation and Coding (AMC) for link adaptation. With AMC, the modulation and coding scheme (MCS) is adaptively selected based on channel quality indication (CQI). In this paper, we give a comprehensive analysis of mapping between signal to interference plus noise ratio (SINR) fed beck by the mobile station and MCS to be used to transmit dada, while taking into account hybrid ARQ (HARQ) operation. Then we apply such mapping criteria to MEVIO-OFDM systems and develop a systematic algorithm for MCS selection to maximize the system throughput. The performance of the proposed approach is evaluated by extensive system level simulation. Results indicate that compared to traditional MCS selection schemes significant system throughput gain can be achieved by this method. ©2006 IEEE.","<b>Authors:</b><br/>Shiming L., Xing Z., Wenbo W. <br/><b>Key words:</b><br/>Adaptive modulation and coding, Hybrid ARQ, MCS slection, MIMO-OFDM, System throughput"
546,paper_549,"Muck M., Rouquette-Léveil S., De Courville M.",,"This contribution derives an algorithm for optimizing the mapping of irregular and systematic Low Density Parity Check (LDPC) code word bits onto Orthogonal Frequency Division Multiplexing (OFDM) carriers in the context of a frequency selective fading channel in both, a Single-Transmit Single-Receive (SISO channel) and Multiple-Transmit Multiple-Receive (MIMO channel) antennas scenario. The absolute values of the frequency domain channel coefficients are assumed to be known and the LDPC code to be given (contrary to existing proposals where the LDPC code is optimized with respect to a given propagation channel). The proposed solution can alternatively be interpreted as an adaptive interleaver (which is inherently available in software defined radio type of system implementations) optimized for a given channel impulse response. In a typical WLAN (IEEE802.11n) scenario, the proposed mapping technique improves the system performance by up to approx. 0.7dB compared to a linear (direct) mapping.","<b>Authors:</b><br/>Muck M., Rouquette-Léveil S., De Courville M. <br/><b>Key words:</b><br/>"
547,paper_550,"Hu Y., Tao V., Xu Z., Wang F., Lenson P.",,"In order to expedite 3D mapping from single images, we have developed a suite of tools and procedures for different use cases in the commercial software - SilverEye - to perform 2D/3D measurements on single images with or without the use of digital terrain models (DTMs). The 3D measurement from single images makes uses of the projection or shadow information of an object. The systematic biases resulting from the absence of DTMs can be compensated automatically when DTMs become available later. We will demonstrate these novel and fast mapping capabilities by extensive experiments using satellite and aerial images. The effects and characteristics of various error sources due to terrain relief, manual pointing, imaging geometry and availability of ground control points are analyzed to assess the attainable accuracies of the mapping results. This paper will also present the comparisons between these single-image based measurement results and those obtained using stereo images in SilverEye and other photogrammetric workstations to validate the feasibility of the new tools. © 2005 by the American Society for Photogrammetry and Remote Sensing.","<b>Authors:</b><br/>Hu Y., Tao V., Xu Z., Wang F., Lenson P. <br/><b>Key words:</b><br/>"
548,paper_551,"Wijesoma W.S., Perera L.D.L., Adams M.D., Challa S.",,"Unmodeled systematic and nonsystematic errors in robot kinematics and measurement processes often cause adverse effects in several autonomous navigation tasks. In particular, accumulated sensor biases can render simultaneous localization and mapping (SLAM) algorithms of autonomous vehicles to perform very poorly especially in large unexplored terrains including cycles, as a result of the estimator divergence and inconsistency. One way to deal with this problem is the accurate modeling and precise calibration of sensors. However this may add up to longer setup and calibration times. Even after accurate calibration and modeling, sensor calibration may often subject to drifts, rendering the efforts ineffective. Therefore, the correct and effective way to deal with this problem is explicit estimation of these parameters with other states. In this work we address the estimation theoretic sensor bias correction problem in SLAM using a simple unified framework and establish theoretically, the behavior and properties of the solution with special consideration to diminishing uncertainty, rates of convergence and observability. © 2005 IEEE.","<b>Authors:</b><br/>Wijesoma W.S., Perera L.D.L., Adams M.D., Challa S. <br/><b>Key words:</b><br/>Convergence, Localization, Mapping"
549,paper_552,"Fu Y., Dong Z., He X.",,"Architecture description languages (ADLs) are developed to precisely and formally describe software conceptual architecture that is distinguished from the system's implementation. Due to the gap between an architecture model and its implementation, the benefits of ADLs cannot be fully realized without a systematic mapping from an architectural description to an implementation. However, the implementation of an architecture model is not only error-prone, but also hard to verify. Some ADLs support code generation from software architecture. However, most of them cannot enforce communication integrity in the implementation. In this paper, we present a methodology to translate software architecture designs to Arch,Java automatically and the communication integrity is guaranteed by ArchJava.","<b>Authors:</b><br/>Fu Y., Dong Z., He X. <br/><b>Key words:</b><br/>"
550,paper_553,"Fenza D., Mion L., Canazza S., Rodà A.",,"Musical interpretations are often the result of a wide range of requirements on expressiveness rendering and technical skills. Aspects indicated by the term expressive intention and which refer to the communication of moods and feelings, are being considered more and more important in performer-computer interaction during music performance. Recent studies demonstrate the possibility of conveying different sensitive content like expressive intentions and emotions by opportunely modifying systematic deviations introduced by the musician. In this paper, we present a control strategy based on a multi-layer representation with three different stages of mapping, to explore the analogies between sound and movement spaces. The mapping between the performer (dancer and/or musician) movements and the expressive audio rendering engine resulting by two 3D ""expressive"" spaces, one obtained by the Laban and Lawrence's effort's theory, the other by means of a multidimensional analysis of perceptual tests carried out on various professionally performed pieces ranging from western classical to popular music. As an example, an application based on this model is presented: the system is developed using the eMotion SMART motion capture system and the Eyesweb software.","<b>Authors:</b><br/>Fenza D., Mion L., Canazza S., Rodà A. <br/><b>Key words:</b><br/>"
551,paper_554,"Choy S.Y., Lee W.B., Cheung C.F.",,"Knowledge audit lays a concrete foundation for any knowledge management programs. The central topic of this paper is to integrate various knowledge audit related techniques into pre-audit preparation, in-audit process and post-audit analysis in a systematic manner. Culture assessment, in the form of surveys and radar charts, along with orientation program make up the pre-audit preparation. Structured interviews are carried out to capture process-critical knowledge. Knowledge inventory, knowledge maps and knowledge flow analysis compose of post-audit analysis. Knowledge inventory is then built for stocktaking knowledge assets and thus revealing the key knowledge assets by measuring them against four performance criteria. Knowledge mapping together with social network analysis are to show the knowledge exchange path and make the key knowledge suppliers and customers visible. They are then being further applied into knowledge flow analysis, which serves to reveal the strength and weakness of the current knowledge flow. A case study of applying the designed instruments in the Engineering Division of the Hong Kong Dragon Airlines Limited and the related analysis are also present in this paper. © J.UCS.","<b>Authors:</b><br/>Choy S.Y., Lee W.B., Cheung C.F. <br/><b>Key words:</b><br/>Knowledge audit, Knowledge flow analysis, Knowledge inventory, Knowledge map, Social network analysis"
552,paper_555,"Grattoni P., Spertino M.",,"This paper describes an approach to the acquisition and representation of information on 3D painted surfaces (usually frescoes) based on the tassellation and mosaicing of the whole surface. The acquisition is carried out using an active vision system specifically designed for these purposes. In this paper, particular emphasis is placed on the mosaicing procedure of the acquired images, which, while conceptually simple, allows one to obtain very good results thanks to the effective exploitation of the features of the acquisition system. A careful qualitative and quantitative analysis of the performances of the system, obtained through tests on real scenes in the laboratory, is also presented. © Springer-Verlag 2003.","<b>Authors:</b><br/>Grattoni P., Spertino M. <br/><b>Key words:</b><br/>Active vision, Image mosaicing, Image registration, Works of art"
553,paper_556,"Robertson C., Fisher R.B.",,"Describes a cheap and easy method of capturing colour images in order to map textures accurately onto range data. By using an empirical mapping algorithm it avoids systematic errors arising from the use of incomplete parametric camera models. © 2002 IEEE.","<b>Authors:</b><br/>Robertson C., Fisher R.B. <br/><b>Key words:</b><br/>"
554,paper_557,"Xu G., Embabi S.H.K.",,"The design of a common mode feedback for multistage amplifiers is a difficult process. Based on constructing the common mode feedback path to be topologically similar to the differential mode path, a systematic mapping approach for deriving a fully differential amplifier, from its single-ended counterpart, is presented. The motivation, usage, and efficiency of the proposed approach is demonstrated by two examples. © 2000 IEEE.","<b>Authors:</b><br/>Xu G., Embabi S.H.K. <br/><b>Key words:</b><br/>Analog integrated circuits, Differential amplifiers"
555,paper_558,"Huh I., Yuan J., Koren Y.",,"The enhancement of machine tool accuracy has a direct effect on part quality. The state-of-the-art technology for this is the compensation method based on the measurement of the machine tool error parameters. This technology, however, has two major drawbacks: (i) it requires long calibration time (ii) for many applications this complicated compensation process is over specified because the parts have limited features and dimensions, and hence the compensation of whole working space is not needed. Furthermore, if the compensation of machining systems for high-volume production is considered, then the fast error measurement and subsequent rapid compensation becomes critical, and the current technology obviously cannot meet these requirements. An alternative method that we propose in this paper is to measure the part rather than the machine, and compensate the machine tools according to these measurements. This proposed methodology requires the modeling of the relationship between the machine tool errors and the part errors, which is called here 'machine-part error mapping model'. The modeling enables to identify the individual machine tool error parameters that contribute on the part error. In order to use this model to compensate for the machine tool error, test part should be carefully developed. In this paper, several rules in the design of test part are presented first, and as an example, the test part for the flatness error compensation of horizontal machining center was designed. From the measurement result of the machined test part, each error source parameters that are responsible for the chosen machining process were identified, and compensated by using the real time compensation system.","<b>Authors:</b><br/>Huh I., Yuan J., Koren Y. <br/><b>Key words:</b><br/>"
556,paper_559,"Du Boulay B., Teather B., Du Boulay G., Jeffrey N., Teather D., Sharples M., Cuthbert L.",,"We have developed a system that aims to help trainees learn a systematic method of describing MR brain images by means of a structured image description language (IDL). The training system makes use of an archive of cases previously described by an expert neuroradiologist. The system utilises a visualisation method - an Overview Plot - which allows the trainee to access individual cases in the database as well as view the overall distribution of cases within a disease and the relative distribution of different diseases. This paper describes the evolution of the image description training system towards a decision support training system, based on the diagnostic notion of a small world. The decision support training system will employ components from the image description training system, so as to provide a uniform interface for training and support. © Springer-Verlag Berlin Heidelberg 1999.","<b>Authors:</b><br/>Du Boulay B., Teather B., Du Boulay G., Jeffrey N., Teather D., Sharples M., Cuthbert L. <br/><b>Key words:</b><br/>"
557,paper_560,"Martini A., Wolter U.",,"Concerning different notions of mappings between institutions, we believe that the current state of the art is somehow unsatisfactory. On the one hand because of the variety of different concepts proposed in the literature. On the other hand because of the apparent lack of a suitable basis to formally discuss about what they mean and how they relate to each other. In this paper we aim at a systematic study of some of the most important notions of these mappings by proposing a methodology based on the concept of power institutions. Firstly, power institutions allow the investigation of the entire logical structure of an institution along these mappings, i.e., the satisfaction relation together with the satisfaction condition. Secondly, they allow this investigation in a systematic way, i.e., the transformation of the institutional logical structure can be described by means of simpler, more elementary transformations or units which are themselves also power institutions. These units are constructions which denote, e.g., typing reduction along functors between signatures, borrowing of models, common model theory, semantical restriction, and logical semantical restriction. The mappings can then be related to each other by showing that they all comprise a particular number of these more fundamental, elementary transformations. © Springer-Verlag Berlin Heidelberg 1998.","<b>Authors:</b><br/>Martini A., Wolter U. <br/><b>Key words:</b><br/>"
558,paper_561,"Likhoded N.A., Tiountchik A.A.",,"A formalized method for a space-time mapping of algorithms onto three-dimensional fixed-size array processors without long interconnections is proposed. Initial algorithms are represented by dependence graphs which are constructed on the basis of recurrence equations. Some special forms of partitioning and spatial mapping of the graph models are considered to provide within the framework of locally parallel globally sequential strategy of partitioning both fixed size of the arrays and the locality of interconnections between processor elements. Conflict-free scheduling of the model is developed. Minimizing of the time requirements is considered. The parameters of scheduling providing the minimization are found. © Springer-Verlag Berlin Heidelberg 1997.","<b>Authors:</b><br/>Likhoded N.A., Tiountchik A.A. <br/><b>Key words:</b><br/>"
559,paper_562,"Wilhelm Wolfgang, Noll Tobias G.",,"A systematic mapping approach leading to efficient VLSI-architectures for FIR-filters with a wide range of system parameters is presented. This approach is subdivided into two steps. In the first step the folding technique is applied at bit-level. The free parameters of this technique are then fixed in the second step according to guidelines which are derived from design-strategies for efficient VLSI-architectures. For many applications this approach leads to a reduced hardware complexity in comparison with state-of-the-art techniques. In addition, regularity and scalability of the resulting architectures keep the design effort small. In order to demonstrate the efficiency and the flexibility of this approach a new class of efficient time-shared FIR-filters for adaptive equalizing and a new class of efficient matched filters for rapid code acquisition in spread spectrum receivers are presented.","<b>Authors:</b><br/>Wilhelm Wolfgang, Noll Tobias G. <br/><b>Key words:</b><br/>"
560,paper_563,"Arnold Douglas",,"The book Machine Translation: A View From the Lexicon, by Bonnie Jean Dorr is reviewed. The book sets out to describe and justify an approach to machine translation (MT) which is based on two leading ideas which are interlinguality and principle based. Some brief remarks about the approach to syntax and parsing are presented. Dorr's approach to interlinguality and her classification of translation divergences, which are the central ideas from an MT perspective, is described.","<b>Authors:</b><br/>Arnold Douglas <br/><b>Key words:</b><br/>"
561,paper_564,"Lorenzelli F., Yao K.",,"In this paper we consider a systematic mapping procedure for systolic arrays. Integral matrix theory provides the basic concepts used here to define projection and scheduling vectors. Unimodular matrices are defined which describe projection, timing, and bases for the processor space and a correct timing function. The use of these matrices couples the definition of correct projection and scheduling functions and provides relatively simple tools for the design. The same mathematical description furnishes a rigorous definition of the partitioning block structure, as well as the cluster set. Both partitioning schemes of locally parallel, globally sequential (LPGS) and locally sequential, globally parallel (LSGP), as well as a number of intermediate partitioning schemes, can be generated by using this technique. Folding (intended as spatial relocation of portions of processing elements) as well as a number of design constraints can also be included, and are briefly considered here. The possible application of a systolic design for low power requirements is also discussed.","<b>Authors:</b><br/>Lorenzelli F., Yao K. <br/><b>Key words:</b><br/>"
562,paper_565,"Alnuweiri H.M.",,"This short note presents a new class of AT<inf>2</inf>optimal networks for computing the multidimensional Discrete Fourier Transform. Although optimal networks have been proposed previously, the networks proposed in this short note are based on a new methodology for mapping large A-shuffle networks, K ? 2, onto smaller area networks that maintain the optimality of the DFT network. Such networks are used to perform the index-rotation operations needed by the multidimensional computation. The resulting networks have simple regular layouts, and can be easily partitioned among several chips in order to reduce the number of inputoutput pins per chip. © 1994 IEEE","<b>Authors:</b><br/>Alnuweiri H.M. <br/><b>Key words:</b><br/>area-time tradeoffs, discrete, folded index-rotation networks, Fourier transform, multidimensional transforms, Parallel processing, shuffle permutations, VLSI computations"
563,paper_566,"Zhang C.N., Li H.F., Jayakumar R.",,"A systematic approach for designing systolic arrays with concurrent error detection (CED) capability using time and/or space redundancy is proposed. This approach is based on a new theory which relates CED and the generalized space-time mapping. Under a restriction that there is one generated (modified) variable in the systolic arrays, a simplified CED scheme is presented. That not only significantly reduces the hardware and time overheads but also has capability of error correction. As well, the resulting systolic array can be used to compute two problem instances simultaneously to achieve double throughput without extra cost. © 1993.","<b>Authors:</b><br/>Zhang C.N., Li H.F., Jayakumar R. <br/><b>Key words:</b><br/>concurrent error detection, Fault-tolerance, space-time mapping, systolic array"
564,paper_567,"Kavianpour A., Bagherzadeh N.",,"A new and systematic approach for mapping application tasks to hypercubes is proposed. The proposed method is based on a partitioning algorithm where the final mapping is rendered as a task-node tuple assignment for an n-cube system. For this method, we assume a single-tasking environment where each task is assigned to a unique processor. Dilation-bound and expansion-ratio parameters are used to evaluate the efficacy of this mapping algorithm. We introduce an algorithm that minimizes the expansion-ratio parameter. In addition, an algorithm that reduces the dilation bound is proposed. Because of the structured formulation of the algorithms, they can be applied to a given task structure. As an illustration of the effectiveness of this new method, we apply these proposed algorithms for mapping complete binary and d  ary tree task structures to hypercubes. © 1993 IEEE","<b>Authors:</b><br/>Kavianpour A., Bagherzadeh N. <br/><b>Key words:</b><br/>"
565,paper_568,"Ieumwananonthachai A., Aizawa A.N., Schwartz S.R., Wah B.W., Yan J.C.",,"In this paper, we present the design of a system for automatically learning and evaluating new heuristic methods that can be used to map a set of communicating processes on a network of computers. Our learning system is based on testing a population of competing heuristic methods within a fixed time constraint. We develop and analyze various resource scheduling strategies based on a statistical model that trades between the number of new heuristic methods considered and the amount of testing performed on each. We implement a prototype learning system (TEACHER 4.1) for learning new heuristic methods used in post-game analysis, a system that iteratively generates and refines mappings of a set of communicating processes on a network of computers. Our performance results show that a significant improvement can be obtained by a systematic exploration of the space of possible heuristic methods. © 1992.","<b>Authors:</b><br/>Ieumwananonthachai A., Aizawa A.N., Schwartz S.R., Wah B.W., Yan J.C. <br/><b>Key words:</b><br/>"
566,paper_569,"Ling N., Bayoumi M.A.",,"This paper presents a concise report on a novel technique. The technique can be used to transform a class of algorithms to specific forms that can be mapped directly onto higher-dimensional systolic networks. The computation time, as well as its order of complexity, can be improved significantly through implementing these algorithms on higher-dimensional systolic networks, yet maintaining the same number of processing cells (the order of area complexity in many cases) as its 1-D counterpart. An example of applying the technique to the k-point Discrete Fourier Transform (DFT) algorithm is given. Applying the technique to other algorithms such as matrix-vector multiplication, 1-D convolution, and FIR filtering is mentioned briefly. Technological issues and implementation of high-dimensional systolic networks on 2-D or 3-D VLSI are also discussed. © 1989.","<b>Authors:</b><br/>Ling N., Bayoumi M.A. <br/><b>Key words:</b><br/>"
